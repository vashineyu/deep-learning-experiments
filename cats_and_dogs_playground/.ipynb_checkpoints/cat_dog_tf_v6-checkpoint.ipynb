{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter setting and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=64, do_augment=True, dq_size=6, epochs=50, gpu_id=5, image_dir='/home/seanyu/datasets/cat_dog/dataset/', image_size=(256, 256, 3), lr=0.0001, model_file_name='tmp_nb', n_batch=100, n_classes=2, n_threads=4, save_dir='./result', train_ratio=0.99, use_model_ckpt=None)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from time import sleep\n",
    "from tqdm import tqdm # if use notebook\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from threading import Thread, Event, Timer\n",
    "import queue\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "import random\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gpu_id', default=5)\n",
    "parser.add_argument('--image_dir', default=\"/home/seanyu/datasets/cat_dog/dataset/\")\n",
    "parser.add_argument('--save_dir', default='./result')\n",
    "parser.add_argument('--batch_size', default=64, type=int)\n",
    "parser.add_argument('--do_augment', default=True, type = bool)\n",
    "parser.add_argument('--epochs', default=50, type=int)\n",
    "parser.add_argument('--lr', default=0.0001, type=float)\n",
    "parser.add_argument('--image_size', default=(256,256,3), type = int)\n",
    "parser.add_argument('--n_classes', default=2, type = int)\n",
    "parser.add_argument('--n_batch', default=100, type = int)\n",
    "parser.add_argument('--train_ratio', default=0.99, type = float)\n",
    "parser.add_argument('--use_model_ckpt', default = None, type = str)\n",
    "parser.add_argument('--model_file_name', default = 'tmp_nb')\n",
    "parser.add_argument('--n_threads', default = 4, type = int)\n",
    "parser.add_argument('--dq_size', default = 6, type = int)\n",
    "\n",
    "FLAGS = parser.parse_args([])\n",
    "print(FLAGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check path and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(FLAGS.gpu_id)\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.slim.nets as slimNet\n",
    "\n",
    "if not os.path.exists(FLAGS.save_dir):\n",
    "    os.makedirs(FLAGS.save_dir)\n",
    "\n",
    "model_dir = FLAGS.save_dir + '/model'\n",
    "    \n",
    "graphs_dir = FLAGS.save_dir + '/graphs'\n",
    "if not os.path.exists(graphs_dir):\n",
    "    os.makedirs(graphs_dir)\n",
    "\n",
    "\"\"\"  Get data \"\"\"\n",
    "d_train = FLAGS.image_dir + '/train/'\n",
    "d_test = FLAGS.image_dir + '/test1/'\n",
    "\n",
    "image_train_list = glob.glob(d_train + '*.jpg')\n",
    "image_test_list = glob.glob(d_test + '*.jpg')\n",
    "\n",
    "df_train = pd.DataFrame({'img_path': image_train_list})\n",
    "df_test = pd.DataFrame({'img_path': image_test_list})\n",
    "\n",
    "df_train['cate'] = df_train.img_path.apply(os.path.basename)\n",
    "df_train['cate'] = [i.split(\".\")[0] for i in list(df_train.cate)]\n",
    "df_train.cate = df_train.cate.replace({'dog': 0, 'cat': 1})\n",
    "\n",
    "nb_epoch = FLAGS.epochs\n",
    "\n",
    "df_train_0, df_val_0 = train_test_split(df_train[df_train['cate'] == 0], test_size = 1-FLAGS.train_ratio)\n",
    "df_train_1, df_val_1 = train_test_split(df_train[df_train['cate'] == 1], test_size = 1-FLAGS.train_ratio)\n",
    "\n",
    "df_val = pd.concat((df_val_0, df_val_1)).reset_index(drop = True)\n",
    "\n",
    "del df_val_0, df_val_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def enqueue(queue, stop, gen_func):\n",
    "    gen = gen_func()\n",
    "    while True:\n",
    "        if stop.is_set():\n",
    "            return\n",
    "        queue.put(next(gen))\n",
    "\n",
    "class create_data_generator():\n",
    "    def __init__(self,\n",
    "                 df,\n",
    "                 open_image_handler,\n",
    "                 image_size,\n",
    "                 data_frame_handler = None,\n",
    "                 nd_inputs_preprocessing_handler = None,\n",
    "                 batch_size = 32,\n",
    "                 n_batch = 150,\n",
    "                 n_classes = 2,\n",
    "                 dual_ = False, # use epoch generator or not (defualt = False)\n",
    "                 do_augment = False,\n",
    "                 aug_params = None):\n",
    "        \n",
    "        self.f_readImg = open_image_handler  # how to open image\n",
    "        self.f_dataproc = data_frame_handler # how to proc original data\n",
    "        self.f_inputs_preproc = nd_inputs_preprocessing_handler # how to do image preprocessing\n",
    "        self.bz = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.n_batch = n_batch\n",
    "        self.n_classes = n_classes\n",
    "        self.aug = aug_params\n",
    "        self.do_augment = do_augment\n",
    "        self.img_per_epoch = batch_size * n_batch # make a epoch batch\n",
    "        self.dual_ = dual_\n",
    "        \n",
    "        # run functions at the begin\n",
    "        # self.df should become list of dataframe anyway\n",
    "        # if not, do the data_preproc. if yes, pass it\n",
    "        if data_frame_handler:\n",
    "            self.df = self.f_dataproc(df)\n",
    "        else:\n",
    "            self.df = df\n",
    "       \n",
    "    def get_train_data(self):\n",
    "        while True:\n",
    "            idxs = self.train_idx_queue.get()\n",
    "\n",
    "            select_list = []\n",
    "\n",
    "            for df, idx in zip(self.df, idxs):\n",
    "                select_list.append(df.iloc[idx])\n",
    "            select_list = pd.concat(select_list)\n",
    "            \n",
    "            x_ = np.array([self.f_readImg(iid, image_size = self.image_size, do_augment = self.do_augment, seq = self.aug) for iid in select_list.img_path], dtype=np.float32)\n",
    "            x_ = x_.astype(np.float32)\n",
    "            \"\"\" do preprocessing here\"\"\"\n",
    "            if self.f_inputs_preproc:\n",
    "                x_ = self.f_inputs_preproc(x_)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            \"\"\" Y out \"\"\"\n",
    "            y_ = np.array(select_list['cate'])\n",
    "            y_ = tf.keras.utils.to_categorical(y_, self.n_classes)\n",
    "\n",
    "            yield [x_], [y_]\n",
    "            \n",
    "    def get_train_epoch(self):\n",
    "        while True:\n",
    "            pre_x, pre_y = [], []\n",
    "            for cumulative_epoch in range(self.n_batch):\n",
    "                idxs = self.train_idx_queue.get()\n",
    "\n",
    "                select_list = []\n",
    "\n",
    "                for df, idx in zip(self.df, idxs):\n",
    "                    select_list.append(df.iloc[idx])\n",
    "                select_list = pd.concat(select_list)\n",
    "\n",
    "                x_ = np.array([self.f_readImg(iid, image_size = self.image_size, do_augment = self.do_augment, seq = self.aug) for iid in select_list.img_path], dtype=np.float32)\n",
    "                x_ = x_.astype(np.float32)\n",
    "                \"\"\" do preprocessing here\"\"\"\n",
    "                if self.f_inputs_preproc:\n",
    "                    x_ = self.f_inputs_preproc(x_)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                \"\"\" Y out \"\"\"\n",
    "                y_ = np.array(select_list['cate'])\n",
    "                y_ = tf.keras.utils.to_categorical(y_, self.n_classes)\n",
    "                \n",
    "                pre_x.append(x_)\n",
    "                pre_y.append(y_)\n",
    "                \n",
    "            yield pre_x, pre_y\n",
    "\n",
    "    def get_data(self):\n",
    "        self.mode = 'slow_mode'\n",
    "        while True:\n",
    "            if self.dual_:\n",
    "                # mode check\n",
    "                if self.train_epoch_queue.qsize() >= 4:\n",
    "                    self.mode = 'quick_mode'\n",
    "                    print(\"In quick mode\")\n",
    "                elif self.train_epoch_queue.qsize() <= 2:\n",
    "                    if self.mode == 'quick_mode':\n",
    "                        print('In slow mode')\n",
    "                    self.mode = 'slow_mode'\n",
    "            \n",
    "            if self.mode == \"quick_mode\":\n",
    "                # in quick mode\n",
    "                print(\"Get data from quick mode, qsize: %i\" % self.train_epoch_queue.qsize())\n",
    "                data = self.train_epoch_queue.get_nowait()\n",
    "            else:\n",
    "                # in slow mode\n",
    "                data = self.train_sample_queue.get()\n",
    "            \n",
    "            for ix in np.arange(len(data[0])):\n",
    "                yield data[0][ix], data[1][ix]\n",
    "    \n",
    "    def get_evaluate_data(self, target_df):\n",
    "        \n",
    "        x_ = np.array([self.f_readImg(i, image_size = self.image_size, is_training = False) for i in target_df.img_path], dtype=np.float32) # don't do augmentation!\n",
    "    \n",
    "        \"\"\" do preprocessing here\"\"\"\n",
    "        if self.f_inputs_preproc:\n",
    "            x_ = self.f_inputs_preproc(x_)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        \"\"\" Y out \"\"\"\n",
    "        y_ = np.array(target_df['cate'])\n",
    "        y_ = tf.keras.utils.to_categorical(y_, num_classes=self.n_classes)\n",
    "        \n",
    "        return x_, y_\n",
    "    \n",
    "    def _get_train_idx(self):\n",
    "        \"\"\" Description \n",
    "        Get training data index for each data frame in the data list\n",
    "        # note1: self.df should be list of data frame with different categories\n",
    "        # note2: if there is only 1 class (or for regression problem, should still be embraced [this_df] )\n",
    "        \"\"\"\n",
    "        len_list = [len(df) for df in self.df]\n",
    "        \n",
    "        bz_t = self.bz//len(len_list)\n",
    "        batch_num = [x//bz_t for x in len_list]\n",
    "\n",
    "        batch_nth = [0] * len(len_list)\n",
    "\n",
    "        select = [list(range(x)) for x in len_list]\n",
    "\n",
    "        for s in select:\n",
    "            random.shuffle(s)\n",
    "\n",
    "        while True:\n",
    "            idxs = []\n",
    "            for i in range(len(len_list)):\n",
    "                if batch_nth[i] >= batch_num[i]:\n",
    "                    batch_nth[i] = 0\n",
    "                    random.shuffle(select[i])\n",
    "                idx = select[i][batch_nth[i]*bz_t:(batch_nth[i]+1)*bz_t]\n",
    "                batch_nth[i] += 1\n",
    "                idxs.append(idx)\n",
    "\n",
    "            yield idxs\n",
    "    \n",
    "    def start_train_threads(self, jobs = 1, dq_size = 10):\n",
    "        \n",
    "        if self.dual_:\n",
    "            self.train_epoch_queue = queue.Queue(maxsize = dq_size)\n",
    "        self.train_sample_queue = queue.Queue(maxsize = dq_size * 5)\n",
    "        self.train_idx_queue =queue.Queue(maxsize = dq_size * 100)\n",
    "        self.jobs = jobs\n",
    "        ### for stop threads after training ###\n",
    "        self.events= list()\n",
    "        self.threading = list()\n",
    "\n",
    "        ### enqueue train index ###\n",
    "        event = Event()\n",
    "        thread = Thread(target = enqueue, \n",
    "                        args = (self.train_idx_queue, \n",
    "                                event, \n",
    "                                self._get_train_idx))\n",
    "        thread.daemon = True \n",
    "        thread.start()\n",
    "        self.events.append(event)\n",
    "        self.threading.append(thread)\n",
    "\n",
    "        ### enqueue train batch ###\n",
    "        if self.dual_:\n",
    "            for i in range(jobs):\n",
    "                event = Event()\n",
    "                thread = Thread(target = enqueue, args = (self.train_epoch_queue, \n",
    "                                                          event, \n",
    "                                                          self.get_train_epoch))\n",
    "                thread.daemon = True \n",
    "                thread.start()\n",
    "                self.events.append(event)\n",
    "                self.threading.append(thread)\n",
    "        ### enqueue train samples\n",
    "        for i in range(jobs // 2):\n",
    "            event = Event()\n",
    "            thread = Thread(target = enqueue, args = (self.train_sample_queue,\n",
    "                                                      event,\n",
    "                                                      self.get_train_data))\n",
    "            thread.daemon = True \n",
    "            thread.start()\n",
    "            self.events.append(event)\n",
    "            self.threading.append(thread)\n",
    "\n",
    "    def stop_train_threads(self):\n",
    "        \"\"\"\n",
    "        Stop the threading\n",
    "        \"\"\"\n",
    "        # block until all tasks are done\n",
    "        for t in self.events:\n",
    "            t.set()\n",
    "        \n",
    "        if self.dual_:\n",
    "            #self.train_epoch_queue.set()\n",
    "            self.train_epoch_queue.queue.clear()\n",
    "        \n",
    "        #self.train_sample_queue.set()\n",
    "        self.train_sample_queue.queue.clear()\n",
    "        #self.train_idx_queue.set()\n",
    "        self.train_idx_queue.queue.clear()\n",
    "        \n",
    "        for i, t in enumerate(self.threading):\n",
    "            t.join(timeout=1)\n",
    "            print(\"Stopping Thread %i\" % i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, patience, min_delta = 0.0001):\n",
    "        # validation loss should at least be less than current min_loss - min_delta\n",
    "        self.min_delta = min_delta \n",
    "        self.patience = patience\n",
    "        self.epoch_count = 0\n",
    "        self.min_loss = None\n",
    "        self.stop = False\n",
    "        \n",
    "    def on_epoch_end(self, val_loss, *args, **kwargs):\n",
    "        if self.min_loss is None or val_loss < self.min_loss - self.min_delta:\n",
    "            self.min_loss = val_loss\n",
    "            self.epoch_count = 0\n",
    "        else:\n",
    "            self.epoch_count += 1\n",
    "            \n",
    "        # if cumulative counts is larger than our patience, set the stop signal to True\n",
    "        if self.epoch_count >= self.patience:\n",
    "            self.stop = True\n",
    "        \n",
    "class Model_checkpoint():\n",
    "    def __init__(self, model_name, save_best_only = True):\n",
    "        self.min_loss = None\n",
    "        self.model_name = model_name\n",
    "        self.save_best_only = save_best_only\n",
    "        \n",
    "    def on_epoch_end(self, val_loss, nth_epoch, saver, sess, *args, **kwargs):\n",
    "        if self.min_loss is None or val_loss < self.min_loss:\n",
    "            print(\"== Validation loss has an improvement, save model ==\")\n",
    "            self.min_loss = val_loss\n",
    "            save_path = saver.save(sess, self.model_name + '.ckpt')\n",
    "            print(\"Model saved in path: %s\" % save_path)\n",
    "            \n",
    "        if not self.save_best_only:\n",
    "            saver.save(sess, self.model_name + '_' + str(nth_epoch) + '.ckpt',\n",
    "                       global_step=nth_epoch)\n",
    "        \n",
    "class ReduceLROnPlateau():\n",
    "    def __init__(self, lr, factor, patience, min_lr = 1e-10):\n",
    "        self.lr = lr\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.min_loss = None\n",
    "        self.epoch_count = 0\n",
    "    \n",
    "    def on_epoch_end(self, val_loss, *args, **kwargs):\n",
    "        if self.min_loss is None or val_loss < self.min_loss:\n",
    "            epoch_count = 0\n",
    "            self.min_loss = val_loss\n",
    "        else:\n",
    "            self.epoch_count += 1\n",
    "        \n",
    "        if self.epoch_count == self.patience:\n",
    "            self.lr *= self.factor\n",
    "            self.epoch_count = 0\n",
    "            \n",
    "            if self.lr <= self.min_lr:\n",
    "                self.lr = self.min_lr\n",
    "                \n",
    "class Run_collected_functions():\n",
    "    def __init__(self, callback_dicts):\n",
    "        self.on_session_begin = callback_dicts['on_session_begin']\n",
    "        self.on_session_end = callback_dicts['on_session_end']\n",
    "        self.on_batch_begin = callback_dicts['on_batch_begin']\n",
    "        self.on_batch_end = callback_dicts['on_batch_end']\n",
    "        self.on_epoch_begin = callback_dicts['on_epoch_begin']\n",
    "        self.on_epoch_end = callback_dicts['on_epoch_end']\n",
    "        \n",
    "    def run_on_epoch_end(self, val_loss, nth_epoch = None, sess = None, saver = None):\n",
    "        for func in self.on_epoch_end:\n",
    "            getattr(func, 'on_epoch_end')(val_loss = val_loss,\n",
    "                                          nth_epoch = nth_epoch,\n",
    "                                          sess = sess,\n",
    "                                          saver = saver)\n",
    "        \n",
    "    def run_on_session_end(self, *args, **kwargs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation setting and single image opener\n",
    "Also read them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5),\n",
    "    iaa.Flipud(0.5),\n",
    "    sometimes(iaa.Add((-10, 10), per_channel=0.5)),\n",
    "    sometimes(iaa.ContrastNormalization((0.4, 0.6)))\n",
    "])\n",
    "\"\"\"\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Crop(px=(0, 16)), # crop images from each side by 0 to 16px (randomly chosen)\n",
    "    iaa.Fliplr(0.5), # horizontally flip 50% of the images\n",
    "    sometimes(iaa.Affine(\n",
    "            scale = (0.8,1.2),\n",
    "            translate_percent = (-0.2, 0.2),\n",
    "            rotate = (-20, 20),\n",
    "            order = [0, 1],\n",
    "            #cval = (0,255),\n",
    "            mode = 'wrap'\n",
    "            ))\n",
    "])\n",
    "\"\"\"\n",
    "def cv_load_and_resize(x, image_size, is_training = True, do_augment = False, seq = None):\n",
    "    im_w, im_h, im_c = image_size\n",
    "    im = cv2.imread(x)\n",
    "    im = cv2.resize(im, (im_w, im_h))\n",
    "    if do_augment and is_training:\n",
    "        im = seq.augment_image(im)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = create_data_generator(df=[df_train_0, df_train_1], \n",
    "                                 n_classes = FLAGS.n_classes,\n",
    "                                 image_size = FLAGS.image_size,\n",
    "                                 do_augment = FLAGS.do_augment,\n",
    "                                 aug_params=seq,\n",
    "                                 batch_size=FLAGS.batch_size,\n",
    "                                 n_batch = FLAGS.n_batch,\n",
    "                                 open_image_handler=cv_load_and_resize)\n",
    "\n",
    "data_gen.start_train_threads(FLAGS.n_threads, dq_size = FLAGS.dq_size)\n",
    "train_gen = data_gen.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(252, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "x_val, y_val = data_gen.get_evaluate_data(df_val)\n",
    "print(x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(FLAGS):\n",
    "    # create a transfer learning model\n",
    "    tf.reset_default_graph()\n",
    "    im_w, im_h, im_c = FLAGS.image_size\n",
    "    \n",
    "    # placeholders\n",
    "    drp_holder = tf.placeholder(tf.float32)\n",
    "    #input1 = tf.placeholder(dtype=tf.float32, shape = (None, im_w, im_h, im_c), name = 'input1')\n",
    "    input1 = tf.layers.Input(dtype=tf.float32, shape = (im_w, im_h, im_c), name = 'input1')\n",
    "    y_true1 = tf.placeholder(dtype=tf.int8, shape = (None, FLAGS.n_classes), name='y_true1')\n",
    "    is_training = tf.placeholder(dtype=tf.bool, shape=[])\n",
    "    lr = tf.placeholder(tf.float32, shape = [])\n",
    "    is_training = tf.placeholder(tf.bool, shape = [])\n",
    "    \n",
    "    # model structs\n",
    "    with slim.arg_scope(slimNet.resnet_utils.resnet_arg_scope(batch_norm_decay=0.99)):\n",
    "        _, layers_dict = slimNet.resnet_v2.resnet_v2_50(input1, global_pool=False, \n",
    "                                                        is_training=is_training)\n",
    "        conv_output = layers_dict['resnet_v2_50/block4']\n",
    "    \n",
    "    with tf.variable_scope('output'):\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(conv_output)\n",
    "        pred = tf.layers.dense(inputs=x, units=FLAGS.n_classes)\n",
    "    \n",
    "    crossentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_true1, \n",
    "                                                           logits=pred)\n",
    "    global_loss = tf.reduce_mean(crossentropy)\n",
    "    \n",
    "    optimizer =  tf.train.AdamOptimizer(lr)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        update = optimizer.minimize(global_loss)\n",
    "    \n",
    "    # other\n",
    "    var_list = tf.trainable_variables()\n",
    "    all_vars = tf.global_variables() #tf.all_variables() # seems it will depricate after certain version of tensorflow\n",
    "    saver = tf.train.Saver()\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.variable_scope(\"metrics\"):\n",
    "        pred_output1 = tf.nn.softmax(pred)\n",
    "        correct_prediction = tf.equal(tf.argmax(pred_output1, 1), \n",
    "                                      tf.argmax(y_true1, 1))\n",
    "        accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # return model as a dictionary, make it easy to access when training or evaluation\n",
    "    model_key = {'input': [input1],\n",
    "                 'ground_truth': [y_true1],\n",
    "                 'output': {'prediction1':pred_output1},\n",
    "                 'metrics': {'accuracy': accuracy_op}, # add other metrics here (for example, f1, auc)\n",
    "                 'loss': [global_loss],\n",
    "                 'update': update,\n",
    "                 'learning_rate': lr,\n",
    "                 'is_training': is_training,\n",
    "                 'intializer': init,\n",
    "                 'saver': saver, # keep None if no saver\n",
    "                 'vars': {'partial_vars': var_list, # partial parameters for other usage (for instance, restore)\n",
    "                          'all_vars': all_vars},\n",
    "                 'optional': {'dropout': drp_holder}\n",
    "                 }\n",
    "    \n",
    "    metric_history = {k: {'train':[], 'valid':[]} for k in list(model_key['metrics'].keys())}\n",
    "    \n",
    "    return model_key, metric_history\n",
    "\n",
    "model_ops, metric_history = create_model(FLAGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TF_HandyTrainer():\n",
    "    def __init__(self, model_ops, \n",
    "                 data_gen,\n",
    "                 data_gen_get_data,\n",
    "                 FLAGS, \n",
    "                 metric_history = None, \n",
    "                 callback_manager = None,\n",
    "                 sess = None):\n",
    "        \"\"\" Description\n",
    "        - model_ops: model graph and its operation key, should be a dict from create_model\n",
    "        - data_gen: data generator\n",
    "        - FLAGS: hyper-parameters setting\n",
    "        - callback_mgr: callback manager, should be a dict\n",
    "        - callback_handler: give a handler that able to operate when training\n",
    "        - sess: usually we don't take sess from outside, we init it inside this class\n",
    "        \"\"\"\n",
    "        self.model_ops = model_ops\n",
    "        self.metric_history = metric_history\n",
    "        self.data_gen = data_gen\n",
    "        self.train_gen = data_gen_get_data\n",
    "        self.FLAGS = FLAGS\n",
    "        self.callback_mgr = callback_manager\n",
    "        self.sess = sess\n",
    "        self.loss_history = {'train': [],\n",
    "                             'valid': []}\n",
    "        ### Define and Set train / evaluation ops to list at once\n",
    "        # Increase certrain ops here\n",
    "        train_ops = [model_ops['update'], model_ops['loss'][0]]\n",
    "        valid_ops = [model_ops['loss'][0]]\n",
    "        if model_ops['metrics'] is not None:\n",
    "            # append ops if not none\n",
    "            for i in model_ops['metrics'].keys():\n",
    "                train_ops.append(model_ops['metrics'][i])\n",
    "                valid_ops.append(model_ops['metrics'][i])\n",
    "        # set\n",
    "        self.train_ops = train_ops\n",
    "        self.valid_ops = valid_ops\n",
    "\n",
    "    def initalize(self, graph_dir = None):\n",
    "        if self.model_ops['saver'] is not None:\n",
    "            # detect saver\n",
    "            self.saver = self.model_ops['saver']\n",
    "        else:\n",
    "            # no saver, create one\n",
    "            self.saver = tf.train.Saver()\n",
    "        \n",
    "        if self.sess is None:\n",
    "            self.sess = tf.Session()\n",
    "        else:\n",
    "            print(\"Warning! Use outside session, only do this unless you know it\")\n",
    "            \n",
    "        print(\"== INITIALIZE PARAMETERS ==\")\n",
    "        self.sess.run([tf.global_variables_initializer()])\n",
    "        if graph_dir is not None:\n",
    "            print(\"Save graph to \" + graph_dir)\n",
    "            tf.summary.FileWriter(graph_dir, self.sess.graph)\n",
    "        \n",
    "    def restore(self, model_to_restore, partial_restore = False):\n",
    "        \"\"\"\n",
    "        Restore weights of layers\n",
    "        - model_to_restore: should include full path of ckpt\n",
    "        e.g. tf_pretrain_model/resnet_v2_50.ckpt\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\" ============== \")\n",
    "        if partial_restore:\n",
    "            # used in take in pre-trained model\n",
    "            print(\"restore paratial parameters\")\n",
    "            # get list of layers to restore and set it into saver\n",
    "            saver_restore = tf.train.Saver([v for v in self.model_ops['vars']['partial_vars'] if 'resnet_v2_50' in v.name])\n",
    "            # restore it\n",
    "            saver_restore.restore(self.sess, model_to_restore)\n",
    "        else:\n",
    "            # used in inference\n",
    "            print(\"restore all parameters\")\n",
    "            self.saver.restore(self.sess, model_to_restore)\n",
    "        \n",
    "    def _train_on_epoch(self, cb_dict):\n",
    "        # Set learning rate of this epoch\n",
    "        if 'reduce_lr' in cb_dict.keys():\n",
    "            epoch_lr = cb_dict['reduce_lr'].lr\n",
    "        else:\n",
    "            epoch_lr = self.FLAGS.lr\n",
    "            \n",
    "        batch_bar = tqdm(range(self.FLAGS.n_batch), \n",
    "                         desc = \"Training batch\", \n",
    "                         unit = \"batch\", leave = True)\n",
    "        epoch_loss = []\n",
    "        \n",
    "        if self.metric_history is not None:\n",
    "            epoch_metric = {k: [] for k in list(self.metric_history.keys())}\n",
    "        \n",
    "        for i in batch_bar:\n",
    "            #x_, y_ = self.data_gen.train_queue.get()\n",
    "            x_, y_ = next(self.train_gen)\n",
    "            batch_result = self.sess.run(self.train_ops, \n",
    "                              feed_dict = {self.model_ops['input'][0]: x_,\n",
    "                                           self.model_ops['ground_truth'][0]: y_,\n",
    "                                           self.model_ops['is_training']: True,\n",
    "                                           self.model_ops['learning_rate']: epoch_lr,\n",
    "                                           self.model_ops['optional']['dropout']: 0.2})\n",
    "            batch_loss = batch_result[1]\n",
    "            batch_acc = batch_result[2]\n",
    "            \n",
    "            epoch_loss.append(batch_loss)\n",
    "            current_loss = np.mean(epoch_loss)\n",
    "            epoch_metric['accuracy'].append(batch_acc)\n",
    "            \n",
    "            ### Customized metric calculate over batches ###\n",
    "            current_acc = np.mean(epoch_metric['accuracy'])\n",
    "            \n",
    "            ### Display\n",
    "            batch_bar.set_description('Batch: %i, Training loss/acc: %.2f/%.2f' % (int(i+1), current_loss, current_acc))\n",
    "            \n",
    "        # return values\n",
    "        self.metric_history['accuracy']['train'].append(current_acc)\n",
    "        self.loss_history['train'].append(current_loss)\n",
    "\n",
    "    \n",
    "    def evaluate(self, x_, y_ = None):\n",
    "        \"\"\" Description \n",
    "        - x_: data to predict\n",
    "        - y_: data ground truth. if keep None, it is test mode\n",
    "        \"\"\"\n",
    "        bz = self.FLAGS.batch_size\n",
    "        total_len = range(len(x_) // bz + 1)\n",
    "        epoch_loss, epoch_predict = [], []\n",
    "        \n",
    "        if self.metric_history is not None:\n",
    "            epoch_metric = {k: [] for k in list(self.metric_history.keys())}\n",
    "        \n",
    "        for i in total_len:\n",
    "            # this is validation mode\n",
    "            batch_result = self.sess.run(self.valid_ops,\n",
    "                                      feed_dict = {self.model_ops['input'][0]: x_[i*bz : (i+1) * bz],\n",
    "                                                   self.model_ops['ground_truth'][0]: y_[i*bz : (i+1) * bz],\n",
    "                                                   self.model_ops['is_training']: False,\n",
    "                                                   self.model_ops['optional']['dropout']: 0.0} )\n",
    "            batch_loss = batch_result[0]\n",
    "            batch_acc = batch_result[1]\n",
    "\n",
    "            epoch_metric['accuracy'].append(batch_acc)\n",
    "            epoch_loss.append(batch_loss)\n",
    "\n",
    "            current_loss = np.mean([np.mean(i) for i in epoch_loss])\n",
    "            current_acc = np.mean([np.mean(i) for i in epoch_metric['accuracy']])\n",
    "        # End of for loop\n",
    "        # return values\n",
    "\n",
    "        # validation mode\n",
    "        self.loss_history['valid'].append(current_loss)\n",
    "        self.metric_history['accuracy']['valid'].append(current_acc)\n",
    "        return current_loss, current_acc\n",
    "        \n",
    "            \n",
    "    def predict(self, x_, model_to_restore = None, bz = None):\n",
    "        \"\"\"\n",
    "        Make prediction\n",
    "        - x_: Input images (np.array) (All images should be pre-processed)\n",
    "        \n",
    "        \"\"\"\n",
    "        if bz is None:\n",
    "            # Let it possible to change batch size when make prediction\n",
    "            bz = self.FLAGS.batch_size\n",
    "            \n",
    "        total_len = range(len(x_) // bz + 1)\n",
    "        epoch_predict = []\n",
    "        \n",
    "        self.saver.restore(self.sess, model_to_restore)\n",
    "        assert model_to_restore is not None, \"please pass the model file name (with full path)\"\n",
    "        \n",
    "        for i in total_len:\n",
    "            # this is testing mode\n",
    "            print('testing mode, progress: %i / %i' % (i, np.max(total_len) ))\n",
    "            batch_predict = self.sess.run([self.model_ops['output']['prediction1']], # prediction\n",
    "                                      feed_dict = {self.model_ops['input'][0]: x_[i*bz : (i+1) * bz],\n",
    "                                                   self.model_ops['is_training']: False,\n",
    "                                                   self.model_ops['optional']['dropout']: 0.0} )\n",
    "            epoch_predict.append(batch_predict)\n",
    "        # Reutrn it\n",
    "        return epoch_predict\n",
    "                \n",
    "    \n",
    "    def do_training(self, validation_set, cb_dict):\n",
    "        \"\"\" Description\n",
    "        - validation_set: should be a tuple (x, y)\n",
    "        - cb_dict: callbacks dictionary\n",
    "        \"\"\"\n",
    "        \n",
    "        #epoch_bar = tqdm(range(self.FLAGS.epochs), desc=\"epoch\", unit=\"epoch\")\n",
    "        epoch_bar = range(self.FLAGS.epochs)\n",
    "        for epoch in epoch_bar:\n",
    "            # train\n",
    "            #epo_start = time.time()\n",
    "            _ = self._train_on_epoch(cb_dict = cb_dict) # temporally define as 150\n",
    "            #epo_end = time.time()\n",
    "            #epo_ela_time = epo_end - epo_start\n",
    "            # validation\n",
    "            _ = self.evaluate(x_ = validation_set[0],\n",
    "                              y_ = validation_set[1])\n",
    "            \n",
    "            # single line report\n",
    "            print('Epoch: {}/{}'.format(int(epoch+1), self.FLAGS.epochs))\n",
    "            print('train loss: {} | val loss: {}'.format(self.loss_history['train'][-1], \n",
    "                                                        self.loss_history['valid'][-1]))\n",
    "            \n",
    "            # run callbacks\n",
    "            self.callback_mgr.run_on_epoch_end(val_loss = self.loss_history['valid'][-1],\n",
    "                                               sess = self.sess,\n",
    "                                               saver = self.saver,\n",
    "                                               nth_epoch = epoch)\n",
    "            if 'earlystop' in cb_dict.keys():\n",
    "                # check there is a earlystop key\n",
    "                if cb_dict['earlystop'].stop:\n",
    "                    print(\"Earlystop criteria met\")\n",
    "                    # met earlystop criteria\n",
    "                    self.data_gen.stop_train_threads()\n",
    "                    \n",
    "                    break\n",
    "        # Normal stop without earlystop\n",
    "        self.data_gen.stop_train_threads()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set callback list and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_dict = {\n",
    "    'reduce_lr' : ReduceLROnPlateau(lr=FLAGS.lr, factor=0.5, patience=3),\n",
    "    'earlystop' : EarlyStopping(min_delta = 1e-4, patience= 15),\n",
    "    'checkpoint' : Model_checkpoint(model_name=model_dir + '/' +  FLAGS.model_file_name, \n",
    "                                    save_best_only=True),\n",
    "}\n",
    "\n",
    "callback_dict = {\n",
    "    'on_session_begin':[], # start of a session\n",
    "    'on_batch_begin':[], # start of a training batch\n",
    "    'on_batch_end':[], # end of a training batch\n",
    "    'on_epoch_begin':[], # start of a epoch\n",
    "    'on_epoch_end':[cb_dict['earlystop'], \n",
    "                    cb_dict['reduce_lr'],\n",
    "                    cb_dict['checkpoint']], # end of a epoch\n",
    "    'on_session_end':[] # end of a session\n",
    "    }\n",
    "callback_manager = Run_collected_functions(callback_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== INITIALIZE PARAMETERS ==\n",
      " ============== \n",
      "restore paratial parameters\n",
      "INFO:tensorflow:Restoring parameters from resnet_v2_50.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100, Training loss/acc: 0.13/0.94: 100%|██████████| 100/100 [00:58<00:00,  1.71batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50\n",
      "train loss: 0.13176530599594116 | val loss: 0.9593197107315063\n",
      "== Validation loss has an improvement, save model ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training batch:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in path: ./result/model/tmp_nb.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100, Training loss/acc: 0.06/0.98: 100%|██████████| 100/100 [00:54<00:00,  1.82batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/50\n",
      "train loss: 0.05884788557887077 | val loss: 0.6784892678260803\n",
      "== Validation loss has an improvement, save model ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training batch:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in path: ./result/model/tmp_nb.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100, Training loss/acc: 0.07/0.98: 100%|██████████| 100/100 [00:55<00:00,  1.81batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/50\n",
      "train loss: 0.0656549483537674 | val loss: 0.37931764125823975\n",
      "== Validation loss has an improvement, save model ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training batch:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in path: ./result/model/tmp_nb.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100, Training loss/acc: 0.06/0.98: 100%|██████████| 100/100 [00:54<00:00,  1.84batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/50\n",
      "train loss: 0.05922131985425949 | val loss: 0.05524173378944397\n",
      "== Validation loss has an improvement, save model ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training batch:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in path: ./result/model/tmp_nb.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100, Training loss/acc: 0.04/0.99: 100%|██████████| 100/100 [00:54<00:00,  1.82batch/s]\n",
      "Training batch:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/50\n",
      "train loss: 0.0371864028275013 | val loss: 0.0760902687907219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100, Training loss/acc: 0.04/0.99: 100%|██████████| 100/100 [00:55<00:00,  1.79batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/50\n",
      "train loss: 0.03791878744959831 | val loss: 0.044185757637023926\n",
      "== Validation loss has an improvement, save model ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training batch:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in path: ./result/model/tmp_nb.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100, Training loss/acc: 0.05/0.98: 100%|██████████| 100/100 [00:55<00:00,  1.82batch/s]\n",
      "Training batch:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/50\n",
      "train loss: 0.045740265399217606 | val loss: 0.06249934434890747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100, Training loss/acc: 0.04/0.99: 100%|██████████| 100/100 [00:55<00:00,  1.81batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/50\n",
      "train loss: 0.04193701595067978 | val loss: 0.026843715459108353\n",
      "== Validation loss has an improvement, save model ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training batch:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in path: ./result/model/tmp_nb.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100, Training loss/acc: 0.03/0.99: 100%|██████████| 100/100 [00:55<00:00,  1.81batch/s]\n",
      "Training batch:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/50\n",
      "train loss: 0.03233915939927101 | val loss: 0.043029770255088806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100, Training loss/acc: 0.03/0.99: 100%|██████████| 100/100 [00:52<00:00,  1.91batch/s]\n",
      "Training batch:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/50\n",
      "train loss: 0.025983000174164772 | val loss: 0.034739166498184204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100, Training loss/acc: 0.03/0.99: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s]\n",
      "Training batch:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/50\n",
      "train loss: 0.026566408574581146 | val loss: 0.03138268366456032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100, Training loss/acc: 0.02/0.99: 100%|██████████| 100/100 [00:49<00:00,  2.03batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/50\n",
      "train loss: 0.018119001761078835 | val loss: 0.026439819484949112\n",
      "== Validation loss has an improvement, save model ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training batch:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in path: ./result/model/tmp_nb.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100, Training loss/acc: 0.01/1.00: 100%|██████████| 100/100 [00:49<00:00,  2.03batch/s]\n",
      "Training batch:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/50\n",
      "train loss: 0.01340425107628107 | val loss: 0.03642011806368828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100, Training loss/acc: 0.01/1.00: 100%|██████████| 100/100 [00:48<00:00,  2.05batch/s]\n",
      "Training batch:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/50\n",
      "train loss: 0.011438515037298203 | val loss: 0.045937228947877884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100, Training loss/acc: 0.01/1.00: 100%|██████████| 100/100 [00:48<00:00,  2.05batch/s]\n",
      "Training batch:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/50\n",
      "train loss: 0.009214753285050392 | val loss: 0.04872016981244087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100, Training loss/acc: 0.01/1.00: 100%|██████████| 100/100 [00:48<00:00,  2.04batch/s]\n",
      "Training batch:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/50\n",
      "train loss: 0.010343650355935097 | val loss: 0.04474490508437157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100, Training loss/acc: 0.00/1.00: 100%|██████████| 100/100 [00:49<00:00,  2.04batch/s]\n",
      "Training batch:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/50\n",
      "train loss: 0.004753867629915476 | val loss: 0.046687278896570206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100, Training loss/acc: 0.01/1.00: 100%|██████████| 100/100 [00:48<00:00,  2.07batch/s]\n",
      "Training batch:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/50\n",
      "train loss: 0.0080300597473979 | val loss: 0.04507558420300484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100, Training loss/acc: 0.01/1.00: 100%|██████████| 100/100 [00:48<00:00,  2.07batch/s]\n",
      "Training batch:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/50\n",
      "train loss: 0.0063314735889434814 | val loss: 0.04251771420240402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100, Training loss/acc: 0.01/1.00: 100%|██████████| 100/100 [00:48<00:00,  2.07batch/s]\n",
      "Training batch:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/50\n",
      "train loss: 0.0065522221848368645 | val loss: 0.03771534189581871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100, Training loss/acc: 0.00/1.00: 100%|██████████| 100/100 [00:48<00:00,  2.06batch/s]\n",
      "Training batch:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21/50\n",
      "train loss: 0.0032420423813164234 | val loss: 0.041836824268102646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100, Training loss/acc: 0.00/1.00: 100%|██████████| 100/100 [00:49<00:00,  2.02batch/s]\n",
      "Training batch:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22/50\n",
      "train loss: 0.004070478491485119 | val loss: 0.041555244475603104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100, Training loss/acc: 0.01/1.00: 100%|██████████| 100/100 [00:49<00:00,  2.01batch/s]\n",
      "Training batch:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23/50\n",
      "train loss: 0.0050077359192073345 | val loss: 0.042181145399808884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100, Training loss/acc: 0.00/1.00: 100%|██████████| 100/100 [00:49<00:00,  2.02batch/s]\n",
      "Training batch:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24/50\n",
      "train loss: 0.003029659390449524 | val loss: 0.04363696277141571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100, Training loss/acc: 0.01/1.00: 100%|██████████| 100/100 [00:49<00:00,  2.01batch/s]\n",
      "Training batch:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25/50\n",
      "train loss: 0.006571941543370485 | val loss: 0.042044397443532944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100, Training loss/acc: 0.00/1.00: 100%|██████████| 100/100 [00:49<00:00,  2.03batch/s]\n",
      "Training batch:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26/50\n",
      "train loss: 0.003858911106362939 | val loss: 0.0418291836977005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100, Training loss/acc: 0.00/1.00: 100%|██████████| 100/100 [00:48<00:00,  2.05batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27/50\n",
      "train loss: 0.003081203904002905 | val loss: 0.04133663326501846\n",
      "Earlystop criteria met\n",
      "Stopping Thread 0\n",
      "Stopping Thread 1\n",
      "Stopping Thread 2\n",
      "Stopping Thread 0\n",
      "Stopping Thread 1\n",
      "Stopping Thread 2\n"
     ]
    }
   ],
   "source": [
    "trainer = TF_HandyTrainer(FLAGS=FLAGS, # hyper-parameters\n",
    "                          data_gen=data_gen, # data generator\n",
    "                          data_gen_get_data = train_gen,\n",
    "                          model_ops=model_ops, # model\n",
    "                          metric_history=metric_history, # metric recording\n",
    "                          callback_manager=callback_manager# runable callbacks\n",
    "                         )\n",
    "\n",
    "trainer.initalize()\n",
    "trainer.restore(model_to_restore='resnet_v2_50.ckpt', partial_restore=True)\n",
    "trainer.do_training(cb_dict=cb_dict, validation_set=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./result/model/tmp_nb.ckpt\n",
      "testing mode, progress: 0 / 3\n",
      "testing mode, progress: 1 / 3\n",
      "testing mode, progress: 2 / 3\n",
      "testing mode, progress: 3 / 3\n"
     ]
    }
   ],
   "source": [
    "final_result = trainer.predict(x_val, model_to_restore=cb_dict['checkpoint'].model_name + '.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {
    "6b1438b3075f49289cfcf03a4fce2ccb": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "ab7848b490e5454e9a42f439a6ef6b31": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "e0e3b0d66b1e47c4ade6c276bacc5c55": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
