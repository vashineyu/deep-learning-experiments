{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter setting and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=4, do_augment=True, dq_size=6, epochs=100, gpu_id=5, image_dir='/data/seanyu/cat_dog/dataset/', image_size=(128, 128, 3), lr=0.0001, model_file_name='model.h5', n_batch=100, n_classes=2, n_threads=4, save_dir='./result', train_ratio=0.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from time import sleep\n",
    "from tqdm import tqdm # if use notebook\n",
    "\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Event\n",
    "import queue\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import random\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gpu_id', default=5)\n",
    "parser.add_argument('--image_dir', default=\"/data/seanyu/cat_dog/dataset/\")\n",
    "parser.add_argument('--save_dir', default='./result')\n",
    "parser.add_argument('--batch_size', default=4, type=int)\n",
    "parser.add_argument('--do_augment', default=True, type = bool)\n",
    "parser.add_argument('--epochs', default=100, type=int)\n",
    "parser.add_argument('--lr', default=1e-4, type=float)\n",
    "parser.add_argument('--image_size', default=(128,128,3), type = int)\n",
    "parser.add_argument('--n_classes', default=2, type = int)\n",
    "parser.add_argument('--n_batch', default=100, type = int)\n",
    "parser.add_argument('--train_ratio', default=0.9, type = float)\n",
    "parser.add_argument('--model_file_name', default = 'model.h5')\n",
    "parser.add_argument('--n_threads', default = 4, type = int)\n",
    "parser.add_argument('--dq_size', default = 6, type = int)\n",
    "\n",
    "FLAGS = parser.parse_args([])\n",
    "print(FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS.gpu_id = \"4\"\n",
    "FLAGS.image_dir = \"/data/seanyu/cat_dog/dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check path and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(FLAGS.gpu_id)\n",
    "import tensorflow as tf\n",
    "\n",
    "if not os.path.exists(FLAGS.save_dir):\n",
    "    os.makedirs(FLAGS.save_dir)\n",
    "\n",
    "model_dir = FLAGS.save_dir + '/model'\n",
    "\n",
    "\"\"\"  Get data \"\"\"\n",
    "d_train = FLAGS.image_dir + '/train/'\n",
    "d_test = FLAGS.image_dir + '/test1/'\n",
    "\n",
    "image_train_list = glob.glob(d_train + '*.jpg')\n",
    "image_test_list = glob.glob(d_test + '*.jpg')\n",
    "\n",
    "df_train = pd.DataFrame({'img_path': image_train_list})\n",
    "df_test = pd.DataFrame({'img_path': image_test_list})\n",
    "\n",
    "df_train['cate'] = df_train.img_path.apply(os.path.basename)\n",
    "df_train['cate'] = [i.split(\".\")[0] for i in list(df_train.cate)]\n",
    "df_train.cate = df_train.cate.replace({'dog': 0, 'cat': 1})\n",
    "\n",
    "nb_epoch = FLAGS.epochs\n",
    "\n",
    "df_train_0, df_val_0 = train_test_split(df_train[df_train['cate'] == 0], test_size = 1-FLAGS.train_ratio)\n",
    "df_train_1, df_val_1 = train_test_split(df_train[df_train['cate'] == 1], test_size = 1-FLAGS.train_ratio)\n",
    "\n",
    "df_val = pd.concat((df_val_0, df_val_1)).reset_index(drop = True)\n",
    "\n",
    "del df_val_0, df_val_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import imgaug as ia\n",
    "    from imgaug import augmenters as iaa\n",
    "except:\n",
    "    print(\"Import Error, Please make sure you have imgaug\")\n",
    "        \n",
    "try:\n",
    "    import sys\n",
    "    sys.path.append(\"/mnt/deep-learning/usr/seanyu/common_tools/\")\n",
    "    from customized_imgaug_func import keypoint_func, img_channelswap\n",
    "except:\n",
    "    print(\"Warning, if you used customized imgaug function\")\n",
    "    \n",
    "class Augmentation_Setup(object):  \n",
    "    sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "    lesstimes = lambda aug: iaa.Sometimes(0.2, aug)\n",
    "    \n",
    "    augmentation = iaa.Sequential([\n",
    "        iaa.Fliplr(0.5, name=\"FlipLR\"),\n",
    "        iaa.Flipud(0.5, name=\"FlipUD\"),\n",
    "        iaa.OneOf([iaa.Affine(rotate = 90),\n",
    "                   iaa.Affine(rotate = 180),\n",
    "                   iaa.Affine(rotate = 270)]),\n",
    "        sometimes(iaa.Affine(\n",
    "                    scale = (0.8,1.2),\n",
    "                    translate_percent = (-0.2, 0.2),\n",
    "                    rotate = (-15, 15),\n",
    "                    mode = 'wrap'\n",
    "                    ))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetDataset():\n",
    "    def __init__(self, df_list, class_id, n_classes, f_input_preproc, image_size=(256,256,3), onehot=True, augmentation=None):\n",
    "        \n",
    "        self.df_list = df_list\n",
    "        self.class_id = class_id\n",
    "        self.n_classes = n_classes\n",
    "        self.preproc = f_input_preproc\n",
    "        self.image_size = image_size\n",
    "        self.onehot = onehot\n",
    "        self.aug = augmentation\n",
    "        \n",
    "        ## Init ##\n",
    "        self.df_list = self.df_list.sample(frac=1.).reset_index(drop=True)\n",
    "        self.current_index = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img = self.load_image(img_path=self.df_list.iloc[self.current_index]['img_path'], image_size=self.image_size)\n",
    "        \n",
    "        if self.aug is not None:\n",
    "            img = self.aug.augment_image(img)\n",
    "            \n",
    "        img = img.astype(np.float32)\n",
    "        \n",
    "        if self.preproc is not None:\n",
    "            img = self.preproc(img)\n",
    "        \n",
    "        label = self.class_id\n",
    "        if self.onehot:\n",
    "             label = tf.keras.utils.to_categorical(label, num_classes=self.n_classes)\n",
    "        \n",
    "        self.current_index = (self.current_index + 1) % len(self.df_list)\n",
    "        return img, label\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.__getitem__(idx=self.current_index)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_image(img_path, image_size):\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (image_size[0], image_size[1]))\n",
    "        return img\n",
    "    \n",
    "class Customized_dataloader():\n",
    "    \"\"\"\n",
    "    1. Compose multiple generators together\n",
    "    2. Make this composed generator into multi-processing function\n",
    "    \"\"\"\n",
    "    def __init__(self, list_dataset, batch_size_per_dataset=16, queue_size=128, num_workers=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - list_dataset: put generator object as list [gen1, gen2, ...]\n",
    "            - batch_size_per_dataset: bz for each generator (total_batch_size/n_class)\n",
    "            - queue_size: queue size\n",
    "            - num_workers: start n workers to get data\n",
    "        \n",
    "        Action: Call with next\n",
    "        \"\"\"\n",
    "        self.list_dataset = list_dataset\n",
    "        self.batch_size_per_dataset = batch_size_per_dataset\n",
    "        self.sample_queue = mp.Queue(maxsize = queue_size)\n",
    "        \n",
    "        self.jobs = num_workers\n",
    "        self.events = list()\n",
    "        self.workers = list()\n",
    "        for i in range(num_workers):\n",
    "            event = Event()\n",
    "            work = mp.Process(target = enqueue, args = (self.sample_queue, event, self.compose_data))\n",
    "            work.daemon = True\n",
    "            work.start()\n",
    "            self.events.append(event)\n",
    "            self.workers.append(work)\n",
    "        print(\"workers ready\")\n",
    "        \n",
    "    def __next__(self):\n",
    "        return self.sample_queue.get()\n",
    "    \n",
    "    def compose_data(self):\n",
    "        while True:\n",
    "            imgs, labels = [], []\n",
    "            for z in range(self.batch_size_per_dataset):\n",
    "                data = [next(i) for i in self.list_dataset]\n",
    "                img, label = zip(*data)\n",
    "                imgs.append(np.array(img))\n",
    "                labels.append(np.array(label))\n",
    "            yield np.concatenate(imgs), np.concatenate(labels)\n",
    "    \n",
    "    def stop_worker(self):\n",
    "        for t in self.events:\n",
    "            t.set()\n",
    "        for i, t in enumerate(self.workers):\n",
    "            t.join(timeout = 1)\n",
    "        print(\"all_worker_stop\")\n",
    "\n",
    "# ----- #\n",
    "def enqueue(queue, stop, gen_func):\n",
    "    gen = gen_func()\n",
    "    while True:\n",
    "        if stop.is_set():\n",
    "            return\n",
    "        queue.put(next(gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(img):\n",
    "    #return (img - img.min()) / (img.max() - img.min())\n",
    "    return img / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_RESNET_PREPROC = False\n",
    "dog_train = GetDataset(df_list=df_train[df_train['cate'] == 0],\n",
    "                       class_id=0, n_classes=2,\n",
    "                       f_input_preproc=preproc if not USE_RESNET_PREPROC else tf.keras.applications.resnet50.preprocess_input,\n",
    "                       augmentation=Augmentation_Setup.augmentation, \n",
    "                       onehot= True, \n",
    "                       image_size=(256,256,3))\n",
    "\n",
    "cat_train = GetDataset(df_list=df_train[df_train['cate'] == 1], \n",
    "                       class_id=1, n_classes=2, \n",
    "                       f_input_preproc=preproc if not USE_RESNET_PREPROC else tf.keras.applications.resnet50.preprocess_input,\n",
    "                       augmentation=Augmentation_Setup.augmentation, \n",
    "                       onehot= True, \n",
    "                       image_size=(256,256,3))\n",
    "\n",
    "dog_valid = GetDataset(df_list=df_val[df_val['cate'] == 0], \n",
    "                       class_id=0, n_classes=2,\n",
    "                       f_input_preproc=preproc if not USE_RESNET_PREPROC else tf.keras.applications.resnet50.preprocess_input,\n",
    "                       augmentation=None, \n",
    "                       onehot= True, \n",
    "                       image_size=(256,256,3))\n",
    "\n",
    "cat_valid = GetDataset(df_list=df_val[df_val['cate'] == 1], \n",
    "                       class_id=1, n_classes=2, \n",
    "                       f_input_preproc=preproc if not USE_RESNET_PREPROC else tf.keras.applications.resnet50.preprocess_input,\n",
    "                       augmentation=None, \n",
    "                       onehot= True, \n",
    "                       image_size=(256,256,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 62.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workers ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_worker_stop\n",
      "(40, 256, 256, 3)\n",
      "(40, 2)\n",
      "[20. 20.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-2:\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/multiprocessing/process.py\", line 261, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/multiprocessing/process.py\", line 261, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/multiprocessing/util.py\", line 322, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/multiprocessing/util.py\", line 322, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/multiprocessing/util.py\", line 262, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/multiprocessing/util.py\", line 262, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/multiprocessing/util.py\", line 186, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/multiprocessing/util.py\", line 186, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/multiprocessing/queues.py\", line 191, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/multiprocessing/queues.py\", line 191, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/threading.py\", line 1056, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/threading.py\", line 1056, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "valid_gen = Customized_dataloader([dog_valid, cat_valid], batch_size_per_dataset=FLAGS.batch_size//2, num_workers=2, queue_size=10)\n",
    "x_val, y_val = [], []\n",
    "for _ in tqdm(range(10)):\n",
    "    a,b = next(valid_gen)\n",
    "    x_val.append(a)\n",
    "    y_val.append(b)\n",
    "x_val = np.concatenate(x_val)\n",
    "y_val = np.concatenate(y_val)\n",
    "valid_gen.stop_worker()\n",
    "\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "print(y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/deep-learning/usr/seanyu/lab_mldl_tools/models/\")\n",
    "sys.path.append(\"/mnt/deep-learning/usr/seanyu/lab_mldl_tools/\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tf_resnet.model import ResNet50, ResNet50V2\n",
    "from tfk_optimizer_wrapper import NormalizedOptimizer, ClippedOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/mnt/deep-learning/usr/seanyu/lab_mldl_tools/misc_tools/\")\n",
    "from tensorflow.keras import optimizers\n",
    "from keras_legacy import legacy_get_updates_support\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def max_norm(grad):\n",
    "    \"\"\"\n",
    "    Computes the L-infinity norm of the gradient.\n",
    "    # Arguments:\n",
    "        grad: gradient for a variable\n",
    "    # Returns:\n",
    "        The norm of the gradient\n",
    "    \"\"\"\n",
    "    grad_max = K.max(K.abs(grad))\n",
    "    norm = grad_max + K.epsilon()\n",
    "    return norm\n",
    "\n",
    "\n",
    "def min_max_norm(grad):\n",
    "    \"\"\"\n",
    "    Computes the average of the Max and Min of the absolute\n",
    "    values of the gradients.\n",
    "    # Arguments:\n",
    "        grad: gradient for a variable\n",
    "    # Returns:\n",
    "        The norm of the gradient\n",
    "    \"\"\"\n",
    "    grad_min = K.min(K.abs(grad))\n",
    "    grad_max = K.max(K.abs(grad))\n",
    "    norm = ((grad_max + grad_min) / 2.0) + K.epsilon()\n",
    "    return norm\n",
    "\n",
    "\n",
    "def std_norm(grad):\n",
    "    \"\"\"\n",
    "    Computes the standard deviation of the gradient.\n",
    "    # Arguments:\n",
    "        grad: gradient for a variable\n",
    "    # Returns:\n",
    "        The norm of the gradient\n",
    "    \"\"\"\n",
    "    norm = K.std(grad) + K.epsilon()\n",
    "    return norm\n",
    "\n",
    "\n",
    "def l1_norm(grad):\n",
    "    \"\"\"\n",
    "    Computes the L-1 norm of the gradient.\n",
    "    # Arguments:\n",
    "        grad: gradient for a variable\n",
    "    # Returns:\n",
    "        The norm of the gradient\n",
    "    \"\"\"\n",
    "    norm = K.sum(K.abs(grad)) + K.epsilon()\n",
    "    return norm\n",
    "\n",
    "\n",
    "def l2_norm(grad):\n",
    "    \"\"\"\n",
    "    Computes the L-2 norm of the gradient.\n",
    "    # Arguments:\n",
    "        grad: gradient for a variable\n",
    "    # Returns:\n",
    "        The norm of the gradient\n",
    "    \"\"\"\n",
    "    norm = K.sqrt(K.sum(K.square(grad))) + K.epsilon()\n",
    "    return norm\n",
    "\n",
    "\n",
    "def l1_l2_norm(grad):\n",
    "    \"\"\"\n",
    "    Computes the average of the L-1 and L-2 norms of the gradient.\n",
    "    # Arguments:\n",
    "        grad: gradient for a variable\n",
    "    # Returns:\n",
    "        The norm of the gradient\n",
    "    \"\"\"\n",
    "    l1 = l1_norm(grad)\n",
    "    l2 = l2_norm(grad)\n",
    "    norm = ((l1 + l2) / 2.) + K.epsilon()\n",
    "    return norm\n",
    "\n",
    "\n",
    "def average_l1_norm(grad):\n",
    "    \"\"\"\n",
    "    Computes the average of the L-1 norm (instead of sum) of the\n",
    "    gradient.\n",
    "    # Arguments:\n",
    "        grad: gradient for a variable\n",
    "    # Returns:\n",
    "        The norm of the gradient\n",
    "    \"\"\"\n",
    "    norm = K.mean(K.abs(grad)) + K.epsilon()\n",
    "    return norm\n",
    "\n",
    "\n",
    "def average_l2_norm(grad):\n",
    "    \"\"\"\n",
    "    Computes the average of the L-2 norm (instead of sum) of the\n",
    "    gradient.\n",
    "    # Arguments:\n",
    "        grad: gradient for a variable\n",
    "    # Returns:\n",
    "        The norm of the gradient\n",
    "    \"\"\"\n",
    "    norm = K.sqrt(K.mean(K.square(grad))) + K.epsilon()\n",
    "    return norm\n",
    "\n",
    "\n",
    "def average_l1_l2_norm(grad):\n",
    "    \"\"\"\n",
    "    Computes the average of the L-1 and L-2 norms (instead of the sum)\n",
    "    to compute the normalized gradient.\n",
    "    # Arguments:\n",
    "        grad: gradient for a variable\n",
    "    # Returns:\n",
    "        The norm of the gradient\n",
    "    \"\"\"\n",
    "    l1_norm = K.mean(K.abs(grad))\n",
    "    l2_norm = K.sqrt(K.mean(K.square(grad)))\n",
    "    norm = ((l1_norm + l2_norm) / 2.) + K.epsilon()\n",
    "    return norm\n",
    "\n",
    "\n",
    "class OptimizerWrapper(optimizers.Optimizer):\n",
    "\n",
    "    def __init__(self, optimizer):\n",
    "        \"\"\"\n",
    "        Base wrapper class for a Keras optimizer such that its gradients are\n",
    "        corrected prior to computing the update ops.\n",
    "        Since it is a wrapper optimizer, it must delegate all normal optimizer\n",
    "        calls to the optimizer that it wraps.\n",
    "        Note:\n",
    "            This wrapper optimizer monkey-patches the optimizer it wraps such that\n",
    "            the call to `get_gradients` will call the gradients of the\n",
    "            optimizer and then normalize the list of gradients.\n",
    "            This is required because Keras calls the optimizer's `get_gradients`\n",
    "            method inside `get_updates`, and without this patch, we cannot\n",
    "            normalize the gradients before computing the rest of the\n",
    "            `get_updates` code.\n",
    "        # Abstract Methods\n",
    "            get_gradients: Must be overridden to support differnt gradient\n",
    "                operations.\n",
    "            get_config: Config needs to be carefully built for serialization.\n",
    "            from_config: Config must be carefully used to build a Subclass.\n",
    "        # Arguments:\n",
    "            optimizer: Keras Optimizer or a string. All optimizers other\n",
    "                than TFOptimizer are supported. If string, instantiates a\n",
    "                default optimizer with that alias.\n",
    "        # Raises\n",
    "            NotImplementedError: If `optimizer` is of type `TFOptimizer`.\n",
    "        \"\"\"\n",
    "        if optimizer.__class__.__name__ == 'TFOptimizer':\n",
    "            raise NotImplementedError('Currently, TFOptimizer is not supported.')\n",
    "\n",
    "        self.optimizer = optimizers.get(optimizer)\n",
    "\n",
    "        # patch the `get_gradients` call\n",
    "        self._optimizer_get_gradients = self.optimizer.get_gradients\n",
    "\n",
    "    def get_gradients(self, loss, params):\n",
    "        \"\"\"\n",
    "        Compute the gradients of the wrapped Optimizer.\n",
    "        # Arguments:\n",
    "            loss: Keras tensor with a single value.\n",
    "            params: List of tensors to optimize\n",
    "        # Returns:\n",
    "            A list of normalized gradient tensors\n",
    "        \"\"\"\n",
    "        grads = self._optimizer_get_gradients(loss, params)\n",
    "        return grads\n",
    "\n",
    "    @legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        \"\"\"\n",
    "        Computes the update operations of the wrapped Optimizer using\n",
    "        normalized gradients and returns a list of operations.\n",
    "        # Arguments:\n",
    "            loss: Keras tensor with a single value\n",
    "            params: List of tensors to optimize\n",
    "        # Returns:\n",
    "            A list of parameter and optimizer update operations\n",
    "        \"\"\"\n",
    "        # monkey patch `get_gradients`\n",
    "        self.optimizer.get_gradients = self.get_gradients\n",
    "\n",
    "        # get the updates\n",
    "        self.optimizer.get_updates(loss, params)\n",
    "\n",
    "        # undo monkey patch\n",
    "        self.optimizer.get_gradients = self._optimizer_get_gradients\n",
    "\n",
    "        return self.updates\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        \"\"\"\n",
    "        Set the weights of the wrapped optimizer by delegation\n",
    "        # Arguments:\n",
    "            weights: List of weight matrices\n",
    "        \"\"\"\n",
    "        self.optimizer.set_weights(weights)\n",
    "\n",
    "    def get_weights(self):\n",
    "        \"\"\"\n",
    "        Get the weights of the wrapped optimizer by delegation\n",
    "        # Returns:\n",
    "            List of weight matrices\n",
    "        \"\"\"\n",
    "        return self.optimizer.get_weights()\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Updates the config of the wrapped optimizer with some meta\n",
    "        data about the normalization function as well as the optimizer\n",
    "        name so that model saving and loading can take place\n",
    "        # Returns:\n",
    "            dictionary of the config\n",
    "        \"\"\"\n",
    "        # properties of NormalizedOptimizer\n",
    "        config = {'optimizer_name': self.optimizer.__class__.__name__.lower()}\n",
    "\n",
    "        # optimizer config\n",
    "        optimizer_config = {'optimizer_config': self.optimizer.get_config()}\n",
    "        return dict(list(optimizer_config.items()) + list(config.items()))\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return self.optimizer.weights\n",
    "\n",
    "    @property\n",
    "    def updates(self):\n",
    "        return self.optimizer.updates\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @classmethod\n",
    "    def set_normalization_function(cls, name, func):\n",
    "        \"\"\"\n",
    "        Allows the addition of new normalization functions adaptively\n",
    "        # Arguments:\n",
    "            name: string name of the normalization function\n",
    "            func: callable function which takes in a single tensor and\n",
    "                returns a single tensor (input gradient tensor and output\n",
    "                normalized gradient tensor).\n",
    "        \"\"\"\n",
    "        global _NORMS\n",
    "        _NORMS[name] = func\n",
    "\n",
    "    @classmethod\n",
    "    def get_normalization_functions(cls):\n",
    "        \"\"\"\n",
    "        Get the list of all registered normalization functions that can be\n",
    "        used.\n",
    "        # Returns:\n",
    "            list of strings denoting the names of all of the normalization\n",
    "            functions.\n",
    "        \"\"\"\n",
    "        global _NORMS\n",
    "        return sorted(list(_NORMS.keys()))\n",
    "\n",
    "\n",
    "class NormalizedOptimizer(OptimizerWrapper):\n",
    "\n",
    "    def __init__(self, optimizer, normalization='l2'):\n",
    "        \"\"\"\n",
    "        Creates a wrapper for a Keras optimizer such that its gradients are\n",
    "        normalized prior to computing the update ops.\n",
    "        Since it is a wrapper optimizer, it must delegate all normal optimizer\n",
    "        calls to the optimizer that it wraps.\n",
    "        Note:\n",
    "            This wrapper optimizer monkey-patches the optimizer it wraps such that\n",
    "            the call to `get_gradients` will call the gradients of the\n",
    "            optimizer and then normalize the list of gradients.\n",
    "            This is required because Keras calls the optimizer's `get_gradients`\n",
    "            method inside `get_updates`, and without this patch, we cannot\n",
    "            normalize the gradients before computing the rest of the\n",
    "            `get_updates` code.\n",
    "        # Arguments:\n",
    "            optimizer: Keras Optimizer or a string. All optimizers other\n",
    "                than TFOptimizer are supported. If string, instantiates a\n",
    "                default optimizer with that alias.\n",
    "            normalization: string. Must refer to a normalization function\n",
    "                that is available in this modules list of normalization\n",
    "                functions. To get all possible normalization functions,\n",
    "                use `NormalizedOptimizer.get_normalization_functions()`.\n",
    "        # Raises\n",
    "            ValueError: If an incorrect name is supplied for `normalization`,\n",
    "                such that the normalization function is not available or not\n",
    "                set using `NormalizedOptimizer.set_normalization_functions()`.\n",
    "            NotImplementedError: If `optimizer` is of type `TFOptimizer`.\n",
    "        \"\"\"\n",
    "        super(NormalizedOptimizer, self).__init__(optimizer)\n",
    "\n",
    "        if normalization not in _NORMS:\n",
    "            raise ValueError('`normalization` must be one of %s.\\n' \n",
    "                             'Provided was \"%s\".' % (str(sorted(list(_NORMS.keys()))), normalization))\n",
    "\n",
    "        self.normalization = normalization\n",
    "        self.normalization_fn = _NORMS[normalization]\n",
    "\n",
    "    def get_gradients(self, loss, params):\n",
    "        \"\"\"\n",
    "        Compute the gradients of the wrapped Optimizer, then normalize\n",
    "        them with the supplied normalization function.\n",
    "        # Arguments:\n",
    "            loss: Keras tensor with a single value.\n",
    "            params: List of tensors to optimize\n",
    "        # Returns:\n",
    "            A list of normalized gradient tensors\n",
    "        \"\"\"\n",
    "        grads = super(NormalizedOptimizer, self).get_gradients(loss, params)\n",
    "        grads = [grad / self.normalization_fn(grad) for grad in grads]\n",
    "        return grads\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Updates the config of the wrapped optimizer with some meta\n",
    "        data about the normalization function as well as the optimizer\n",
    "        name so that model saving and loading can take place\n",
    "        # Returns:\n",
    "            dictionary of the config\n",
    "        \"\"\"\n",
    "        # properties of NormalizedOptimizer\n",
    "        config = {'normalization': self.normalization}\n",
    "\n",
    "        # optimizer config\n",
    "        base_config = super(NormalizedOptimizer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        \"\"\"\n",
    "        Utilizes the meta data from the config to create a new instance\n",
    "        of the optimizer which was wrapped previously, and creates a\n",
    "        new instance of this wrapper class.\n",
    "        # Arguments:\n",
    "            config: dictionary of the config\n",
    "        # Returns:\n",
    "            a new instance of NormalizedOptimizer\n",
    "        \"\"\"\n",
    "        optimizer_config = {'class_name': config['optimizer_name'],\n",
    "                            'config': config['optimizer_config']}\n",
    "\n",
    "        optimizer = optimizers.get(optimizer_config)\n",
    "        normalization = config['normalization']\n",
    "\n",
    "        return cls(optimizer, normalization=normalization)\n",
    "\n",
    "\n",
    "class ClippedOptimizer(OptimizerWrapper):\n",
    "\n",
    "    def __init__(self, optimizer, normalization='l2', clipnorm=1.0):\n",
    "        \"\"\"\n",
    "        Creates a wrapper for a Keras optimizer such that its gradients are\n",
    "        clipped by the norm prior to computing the update ops.\n",
    "        Since it is a wrapper optimizer, it must delegate all normal optimizer\n",
    "        calls to the optimizer that it wraps.\n",
    "        Note:\n",
    "            This wrapper optimizer monkey-patches the optimizer it wraps such that\n",
    "            the call to `get_gradients` will call the gradients of the\n",
    "            optimizer and then normalize the list of gradients.\n",
    "            This is required because Keras calls the optimizer's `get_gradients`\n",
    "            method inside `get_updates`, and without this patch, we cannot\n",
    "            normalize the gradients before computing the rest of the\n",
    "            `get_updates` code.\n",
    "        # Arguments:\n",
    "            optimizer: Keras Optimizer or a string. All optimizers other\n",
    "                than TFOptimizer are supported. If string, instantiates a\n",
    "                default optimizer with that alias.\n",
    "            normalization: string. Must refer to a normalization function\n",
    "                that is available in this modules list of normalization\n",
    "                functions. To get all possible normalization functions,\n",
    "                use `NormalizedOptimizer.get_normalization_functions()`.\n",
    "            clipnorm: float >= 0. Gradients will be clipped\n",
    "                when their norm exceeds this value.\n",
    "        # Raises\n",
    "            ValueError: If an incorrect name is supplied for `normalization`,\n",
    "                such that the normalization function is not available or not\n",
    "                set using `ClippedOptimizer.set_normalization_functions()`.\n",
    "            NotImplementedError: If `optimizer` is of type `TFOptimizer`.\n",
    "        \"\"\"\n",
    "        super(ClippedOptimizer, self).__init__(optimizer)\n",
    "\n",
    "        if normalization not in _NORMS:\n",
    "            raise ValueError('`normalization` must be one of %s.\\n' \n",
    "                             'Provided was \"%s\".' % (str(sorted(list(_NORMS.keys()))), normalization))\n",
    "\n",
    "        self.normalization = normalization\n",
    "        self.normalization_fn = _NORMS[normalization]\n",
    "\n",
    "        self.clipnorm = clipnorm\n",
    "\n",
    "    def get_gradients(self, loss, params):\n",
    "        \"\"\"\n",
    "        Compute the gradients of the wrapped Optimizer, then normalize\n",
    "        them with the supplied normalization function.\n",
    "        # Arguments:\n",
    "            loss: Keras tensor with a single value.\n",
    "            params: List of tensors to optimize\n",
    "        # Returns:\n",
    "            A list of normalized gradient tensors\n",
    "        \"\"\"\n",
    "        grads = super(ClippedOptimizer, self).get_gradients(loss, params)\n",
    "        grads = [self._clip_grad(grad) for grad in grads]\n",
    "        return grads\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Updates the config of the wrapped optimizer with some meta\n",
    "        data about the normalization function as well as the optimizer\n",
    "        name so that model saving and loading can take place\n",
    "        # Returns:\n",
    "            dictionary of the config\n",
    "        \"\"\"\n",
    "        # properties of NormalizedOptimizer\n",
    "        config = {'normalization': self.normalization,\n",
    "                  'clipnorm': self.clipnorm}\n",
    "\n",
    "        # optimizer config\n",
    "        base_config = super(ClippedOptimizer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def _clip_grad(self, grad):\n",
    "        \"\"\"\n",
    "        Helper method to compute the norm and then clip the gradients.\n",
    "        # Arguments:\n",
    "            grad: gradients of a single variable\n",
    "        # Returns:\n",
    "            clipped gradients\n",
    "        \"\"\"\n",
    "        norm = self.normalization_fn(grad)\n",
    "        grad = optimizers.clip_norm(grad, self.clipnorm, norm)\n",
    "        return grad\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        \"\"\"\n",
    "        Utilizes the meta data from the config to create a new instance\n",
    "        of the optimizer which was wrapped previously, and creates a\n",
    "        new instance of this wrapper class.\n",
    "        # Arguments:\n",
    "            config: dictionary of the config\n",
    "        # Returns:\n",
    "            a new instance of NormalizedOptimizer\n",
    "        \"\"\"\n",
    "        optimizer_config = {'class_name': config['optimizer_name'],\n",
    "                            'config': config['optimizer_config']}\n",
    "\n",
    "        optimizer = optimizers.get(optimizer_config)\n",
    "        normalization = config['normalization']\n",
    "        clipnorm = config['clipnorm']\n",
    "\n",
    "        return cls(optimizer, normalization=normalization, clipnorm=clipnorm)\n",
    "\n",
    "\n",
    "_NORMS = {\n",
    "    'max': max_norm,\n",
    "    'min_max': min_max_norm,\n",
    "    'l1': l1_norm,\n",
    "    'l2': l2_norm,\n",
    "    'linf': max_norm,\n",
    "    'l1_l2': l1_l2_norm,\n",
    "    'std': std_norm,\n",
    "    'avg_l1': average_l1_norm,\n",
    "    'avg_l2': average_l2_norm,\n",
    "    'avg_l1_l2': average_l1_l2_norm,\n",
    "}\n",
    "\n",
    "# register this optimizer to the global custom objects when it is imported\n",
    "get_custom_objects().update({'NormalizedOptimizer': NormalizedOptimizer,\n",
    "                             'ClippedOptimizer': ClippedOptimizer})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_graph(model_fn, norm_use, input_shape=(256,256,3), n_outputs=2):\n",
    "    pretrain_modules = model_fn(include_top=False, input_shape=input_shape, norm_use=norm_use, weights=None)\n",
    "    gap = tf.keras.layers.GlobalAveragePooling2D()(pretrain_modules.output)\n",
    "    out = tf.keras.layers.Dense(units=n_outputs, activation='softmax', name='output')(gap)\n",
    "    model = tf.keras.models.Model(inputs=[pretrain_modules.input], outputs=[out])\n",
    "    return model\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "class Logger(Callback):\n",
    "    def __init__(self, n, gpu_id = 0):\n",
    "        self.n = n   # print loss & acc every n epochs\n",
    "        self.gpu_id = gpu_id\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.n == 0:\n",
    "            # add what you need here\n",
    "            train_loss = logs.get('loss')\n",
    "            train_acc = logs.get('acc')\n",
    "            valid_loss = logs.get('val_loss')\n",
    "            valid_acc = logs.get('val_acc')\n",
    "            print(\"GPU_ID: %s, epoch: %4d, loss: %0.5f, acc: %0.3f, val_loss: %0.5f, val_acc: %0.3f\" \\\n",
    "                  % (self.gpu_id, epoch, \n",
    "                     train_loss, train_acc,\n",
    "                     valid_loss, valid_acc))\n",
    "\n",
    "import tensorflow.keras.backend as tfk\n",
    "def train(device, graph, model, generator):\n",
    "    print(\"Start training on %s\" % device)\n",
    "    logger = Logger(n=1, gpu_id=device)\n",
    "    \"\"\"\n",
    "    model.fit_generator(generator, epochs=100, steps_per_epoch=100,\n",
    "                        verbose=0, \n",
    "                        validation_data=(x_val, y_val), callbacks=[logger])\n",
    "    \"\"\"\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tfk.set_session(session=session)\n",
    "        model.fit_generator(generator, epochs=100, steps_per_epoch=100,\n",
    "                  verbose=0, \n",
    "                  validation_data=(x_val, y_val), callbacks=[logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workers ready\n"
     ]
    }
   ],
   "source": [
    "train_gen = Customized_dataloader([dog_train, cat_train], \n",
    "                                  batch_size_per_dataset=FLAGS.batch_size//2, \n",
    "                                  num_workers=4, queue_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single setting run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 262, 262, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 128, 128, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 130, 130, 64) 0           conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 64, 64, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_preact_gn (GroupNo (None, 64, 64, 64)   128         pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_preact_relu (Activ (None, 64, 64, 64)   0           conv2_block1_preact_gn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 64, 64, 64)   4096        conv2_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_gn (GroupNorm)   (None, 64, 64, 64)   128         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 64, 64, 64)   0           conv2_block1_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_pad (ZeroPadding (None, 66, 66, 64)   0           conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 64, 64, 64)   36864       conv2_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_gn (GroupNorm)   (None, 64, 64, 64)   128         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 64, 64, 64)   0           conv2_block1_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Add)          (None, 64, 64, 256)  0           conv2_block1_0_conv[0][0]        \n",
      "                                                                 conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_preact_gn (GroupNo (None, 64, 64, 256)  512         conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_preact_relu (Activ (None, 64, 64, 256)  0           conv2_block2_preact_gn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 64, 64, 64)   16384       conv2_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_gn (GroupNorm)   (None, 64, 64, 64)   128         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 64, 64, 64)   0           conv2_block2_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_pad (ZeroPadding (None, 66, 66, 64)   0           conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 64, 64, 64)   36864       conv2_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_gn (GroupNorm)   (None, 64, 64, 64)   128         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 64, 64, 64)   0           conv2_block2_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Add)          (None, 64, 64, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_preact_gn (GroupNo (None, 64, 64, 256)  512         conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_preact_relu (Activ (None, 64, 64, 256)  0           conv2_block3_preact_gn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 64, 64, 64)   16384       conv2_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_gn (GroupNorm)   (None, 64, 64, 64)   128         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 64, 64, 64)   0           conv2_block3_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_pad (ZeroPadding (None, 66, 66, 64)   0           conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 32, 32, 64)   36864       conv2_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_gn (GroupNorm)   (None, 32, 32, 64)   128         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 32, 32, 64)   0           conv2_block3_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 32, 32, 256)  0           conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Add)          (None, 32, 32, 256)  0           max_pooling2d[0][0]              \n",
      "                                                                 conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_preact_gn (GroupNo (None, 32, 32, 256)  512         conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_preact_relu (Activ (None, 32, 32, 256)  0           conv3_block1_preact_gn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 32, 32, 128)  32768       conv3_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_gn (GroupNorm)   (None, 32, 32, 128)  256         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 32, 32, 128)  0           conv3_block1_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_pad (ZeroPadding (None, 34, 34, 128)  0           conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 32, 32, 128)  147456      conv3_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_gn (GroupNorm)   (None, 32, 32, 128)  256         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 32, 32, 128)  0           conv3_block1_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 32, 32, 512)  131584      conv3_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Add)          (None, 32, 32, 512)  0           conv3_block1_0_conv[0][0]        \n",
      "                                                                 conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_preact_gn (GroupNo (None, 32, 32, 512)  1024        conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_preact_relu (Activ (None, 32, 32, 512)  0           conv3_block2_preact_gn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 32, 32, 128)  65536       conv3_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_gn (GroupNorm)   (None, 32, 32, 128)  256         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 32, 32, 128)  0           conv3_block2_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_pad (ZeroPadding (None, 34, 34, 128)  0           conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 32, 32, 128)  147456      conv3_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_gn (GroupNorm)   (None, 32, 32, 128)  256         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 32, 32, 128)  0           conv3_block2_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Add)          (None, 32, 32, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_preact_gn (GroupNo (None, 32, 32, 512)  1024        conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_preact_relu (Activ (None, 32, 32, 512)  0           conv3_block3_preact_gn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 32, 32, 128)  65536       conv3_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_gn (GroupNorm)   (None, 32, 32, 128)  256         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 32, 32, 128)  0           conv3_block3_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_pad (ZeroPadding (None, 34, 34, 128)  0           conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 32, 32, 128)  147456      conv3_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_gn (GroupNorm)   (None, 32, 32, 128)  256         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 32, 32, 128)  0           conv3_block3_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Add)          (None, 32, 32, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_preact_gn (GroupNo (None, 32, 32, 512)  1024        conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_preact_relu (Activ (None, 32, 32, 512)  0           conv3_block4_preact_gn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 32, 32, 128)  65536       conv3_block4_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_gn (GroupNorm)   (None, 32, 32, 128)  256         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 32, 32, 128)  0           conv3_block4_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_pad (ZeroPadding (None, 34, 34, 128)  0           conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 16, 16, 128)  147456      conv3_block4_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_gn (GroupNorm)   (None, 16, 16, 128)  256         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 16, 16, 128)  0           conv3_block4_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 512)  0           conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Add)          (None, 16, 16, 512)  0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_preact_gn (GroupNo (None, 16, 16, 512)  1024        conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_preact_relu (Activ (None, 16, 16, 512)  0           conv4_block1_preact_gn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 16, 16, 256)  131072      conv4_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_gn (GroupNorm)   (None, 16, 16, 256)  512         conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 16, 16, 256)  0           conv4_block1_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_pad (ZeroPadding (None, 18, 18, 256)  0           conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 16, 16, 256)  589824      conv4_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_gn (GroupNorm)   (None, 16, 16, 256)  512         conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 16, 16, 256)  0           conv4_block1_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 16, 16, 1024) 525312      conv4_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Add)          (None, 16, 16, 1024) 0           conv4_block1_0_conv[0][0]        \n",
      "                                                                 conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_preact_gn (GroupNo (None, 16, 16, 1024) 2048        conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_preact_relu (Activ (None, 16, 16, 1024) 0           conv4_block2_preact_gn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 16, 16, 256)  262144      conv4_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_gn (GroupNorm)   (None, 16, 16, 256)  512         conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 16, 16, 256)  0           conv4_block2_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_pad (ZeroPadding (None, 18, 18, 256)  0           conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 16, 16, 256)  589824      conv4_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_gn (GroupNorm)   (None, 16, 16, 256)  512         conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 16, 16, 256)  0           conv4_block2_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Add)          (None, 16, 16, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_preact_gn (GroupNo (None, 16, 16, 1024) 2048        conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_preact_relu (Activ (None, 16, 16, 1024) 0           conv4_block3_preact_gn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 16, 16, 256)  262144      conv4_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_gn (GroupNorm)   (None, 16, 16, 256)  512         conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 16, 16, 256)  0           conv4_block3_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_pad (ZeroPadding (None, 18, 18, 256)  0           conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 16, 16, 256)  589824      conv4_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_gn (GroupNorm)   (None, 16, 16, 256)  512         conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 16, 16, 256)  0           conv4_block3_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Add)          (None, 16, 16, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_preact_gn (GroupNo (None, 16, 16, 1024) 2048        conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_preact_relu (Activ (None, 16, 16, 1024) 0           conv4_block4_preact_gn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 16, 16, 256)  262144      conv4_block4_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_gn (GroupNorm)   (None, 16, 16, 256)  512         conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 16, 16, 256)  0           conv4_block4_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_pad (ZeroPadding (None, 18, 18, 256)  0           conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 16, 16, 256)  589824      conv4_block4_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_gn (GroupNorm)   (None, 16, 16, 256)  512         conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 16, 16, 256)  0           conv4_block4_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Add)          (None, 16, 16, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_preact_gn (GroupNo (None, 16, 16, 1024) 2048        conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_preact_relu (Activ (None, 16, 16, 1024) 0           conv4_block5_preact_gn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 16, 16, 256)  262144      conv4_block5_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_gn (GroupNorm)   (None, 16, 16, 256)  512         conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 16, 16, 256)  0           conv4_block5_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_pad (ZeroPadding (None, 18, 18, 256)  0           conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 16, 16, 256)  589824      conv4_block5_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_gn (GroupNorm)   (None, 16, 16, 256)  512         conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 16, 16, 256)  0           conv4_block5_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Add)          (None, 16, 16, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_preact_gn (GroupNo (None, 16, 16, 1024) 2048        conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_preact_relu (Activ (None, 16, 16, 1024) 0           conv4_block6_preact_gn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 16, 16, 256)  262144      conv4_block6_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_gn (GroupNorm)   (None, 16, 16, 256)  512         conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 16, 16, 256)  0           conv4_block6_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_pad (ZeroPadding (None, 18, 18, 256)  0           conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 8, 8, 256)    589824      conv4_block6_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_gn (GroupNorm)   (None, 8, 8, 256)    512         conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 8, 8, 256)    0           conv4_block6_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 1024)   0           conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Add)          (None, 8, 8, 1024)   0           max_pooling2d_2[0][0]            \n",
      "                                                                 conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_preact_gn (GroupNo (None, 8, 8, 1024)   2048        conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_preact_relu (Activ (None, 8, 8, 1024)   0           conv5_block1_preact_gn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 8, 8, 512)    524288      conv5_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_gn (GroupNorm)   (None, 8, 8, 512)    1024        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 8, 8, 512)    0           conv5_block1_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_pad (ZeroPadding (None, 10, 10, 512)  0           conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 8, 8, 512)    2359296     conv5_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_gn (GroupNorm)   (None, 8, 8, 512)    1024        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 8, 8, 512)    0           conv5_block1_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 8, 8, 2048)   2099200     conv5_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Add)          (None, 8, 8, 2048)   0           conv5_block1_0_conv[0][0]        \n",
      "                                                                 conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_preact_gn (GroupNo (None, 8, 8, 2048)   4096        conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_preact_relu (Activ (None, 8, 8, 2048)   0           conv5_block2_preact_gn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 8, 8, 512)    1048576     conv5_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_gn (GroupNorm)   (None, 8, 8, 512)    1024        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 8, 8, 512)    0           conv5_block2_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_pad (ZeroPadding (None, 10, 10, 512)  0           conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 8, 8, 512)    2359296     conv5_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_gn (GroupNorm)   (None, 8, 8, 512)    1024        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 8, 8, 512)    0           conv5_block2_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Add)          (None, 8, 8, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_preact_gn (GroupNo (None, 8, 8, 2048)   4096        conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_preact_relu (Activ (None, 8, 8, 2048)   0           conv5_block3_preact_gn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 8, 8, 512)    1048576     conv5_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_gn (GroupNorm)   (None, 8, 8, 512)    1024        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 8, 8, 512)    0           conv5_block3_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_pad (ZeroPadding (None, 10, 10, 512)  0           conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 8, 8, 512)    2359296     conv5_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_gn (GroupNorm)   (None, 8, 8, 512)    1024        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 8, 8, 512)    0           conv5_block3_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Add)          (None, 8, 8, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "post_gn (GroupNorm)             (None, 8, 8, 2048)   4096        conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "post_relu (Activation)          (None, 8, 8, 2048)   0           post_gn[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 2048)         0           post_relu[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            4098        global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 23,523,458\n",
      "Trainable params: 23,523,458\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = build_model_graph(ResNet50V2, \"bn\")\n",
    "#optim = tf.keras.optimizers.Adam(lr=1e-5, amsgrad=True)\n",
    "#optim = NormalizedOptimizer(tf.keras.optimizers.Adam(lr=1e-5), normalization='l2')\n",
    "#optim = ClippedOptimizer(tf.keras.optimizers.Adam(lr=1e-5, clipnorm = 5.), normalization='l2')\n",
    "optim = NormalizedOptimizer(tf.keras.optimizers.SGD(lr = 1e-4, momentum=0.9, nesterov=True), normalization='avg_l2')\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              metrics=[\"accuracy\"], \n",
    "              optimizer=optim)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 38s 381ms/step - loss: 0.8526 - acc: 0.5000 - val_loss: 0.7317 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 15s 151ms/step - loss: 0.7059 - acc: 0.5150 - val_loss: 0.6729 - val_acc: 0.7000\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 15s 148ms/step - loss: 0.6866 - acc: 0.6075 - val_loss: 0.6722 - val_acc: 0.7500\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 15s 146ms/step - loss: 0.6747 - acc: 0.6375 - val_loss: 0.6705 - val_acc: 0.6500\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 14s 140ms/step - loss: 0.6765 - acc: 0.6200 - val_loss: 0.6695 - val_acc: 0.6500\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 12s 124ms/step - loss: 0.6934 - acc: 0.5250 - val_loss: 0.6703 - val_acc: 0.7000\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 0.6945 - acc: 0.5375 - val_loss: 0.6708 - val_acc: 0.6000\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 11s 115ms/step - loss: 0.6977 - acc: 0.5325 - val_loss: 0.6723 - val_acc: 0.7000\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 12s 123ms/step - loss: 0.6821 - acc: 0.5625 - val_loss: 0.6723 - val_acc: 0.7500\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.6928 - acc: 0.5600 - val_loss: 0.6725 - val_acc: 0.6500\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.6895 - acc: 0.4950 - val_loss: 0.6737 - val_acc: 0.7000\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 14s 139ms/step - loss: 0.6874 - acc: 0.5500 - val_loss: 0.6743 - val_acc: 0.6500\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.6878 - acc: 0.5075 - val_loss: 0.6754 - val_acc: 0.6000\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 14s 136ms/step - loss: 0.6859 - acc: 0.5500 - val_loss: 0.6752 - val_acc: 0.6000\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 14s 139ms/step - loss: 0.6863 - acc: 0.5750 - val_loss: 0.6760 - val_acc: 0.7000\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 13s 132ms/step - loss: 0.6873 - acc: 0.6025 - val_loss: 0.6758 - val_acc: 0.6500\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 12s 120ms/step - loss: 0.6795 - acc: 0.6200 - val_loss: 0.6752 - val_acc: 0.6500\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 11s 115ms/step - loss: 0.6805 - acc: 0.5950 - val_loss: 0.6745 - val_acc: 0.7000\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 0.6916 - acc: 0.5025 - val_loss: 0.6750 - val_acc: 0.6000\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 12s 120ms/step - loss: 0.6925 - acc: 0.4700 - val_loss: 0.6762 - val_acc: 0.6500\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 14s 136ms/step - loss: 0.6867 - acc: 0.5375 - val_loss: 0.6766 - val_acc: 0.6000\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 14s 141ms/step - loss: 0.6860 - acc: 0.5650 - val_loss: 0.6754 - val_acc: 0.6500\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 14s 139ms/step - loss: 0.6785 - acc: 0.6575 - val_loss: 0.6755 - val_acc: 0.6500\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 14s 139ms/step - loss: 0.6841 - acc: 0.6100 - val_loss: 0.6761 - val_acc: 0.5500\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.6821 - acc: 0.5875 - val_loss: 0.6771 - val_acc: 0.5500\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 14s 137ms/step - loss: 0.6852 - acc: 0.5375 - val_loss: 0.6787 - val_acc: 0.6000\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 14s 142ms/step - loss: 0.6746 - acc: 0.6450 - val_loss: 0.6787 - val_acc: 0.6000\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 11s 115ms/step - loss: 0.6818 - acc: 0.5700 - val_loss: 0.6780 - val_acc: 0.6500\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 0.6861 - acc: 0.5350 - val_loss: 0.6789 - val_acc: 0.6000\n",
      "Epoch 30/100\n",
      " 34/100 [=========>....................] - ETA: 7s - loss: 0.6863 - acc: 0.4853"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-dc2efb022c58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                     \u001b[0;31m#callbacks=cb_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                     )\n",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         outs = model.train_on_batch(\n\u001b[0;32m--> 176\u001b[0;31m             x, y, sample_weight=sample_weight, class_weight=class_weight)\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cb_list = [tf.keras.callbacks.ReduceLROnPlateau(factor=0.5,\n",
    "                                                patience=4,\n",
    "                                                min_lr=1e-12),\n",
    "           tf.keras.callbacks.EarlyStopping(min_delta = 1e-4, \n",
    "                                            patience= 50)\n",
    "          ]\n",
    "\n",
    "model.fit_generator(train_gen,\n",
    "                    epochs=FLAGS.epochs,\n",
    "                    steps_per_epoch=FLAGS.n_batch, \n",
    "                    validation_data=(x_val, y_val),\n",
    "                    #callbacks=cb_list\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = model.history.history['loss']\n",
    "valid_loss = model.history.history['val_loss']\n",
    "train_acc = model.history.history['acc']\n",
    "valid_acc = model.history.history['val_acc']\n",
    "\n",
    "plt.plot(range(len(train_loss)), train_loss, label='train_loss')\n",
    "plt.plot(range(len(valid_loss)), valid_loss, label='valid_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(train_acc)), train_acc, label='train_accuracy')\n",
    "plt.plot(range(len(valid_acc)), valid_acc, label='valid_accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple setting comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[<tensorflow.python.framework.ops.Graph object at 0x7f1d70a98eb8>, <tensorflow.python.keras.engine.training.Model object at 0x7f1b184772e8>], [<tensorflow.python.framework.ops.Graph object at 0x7f1b1847e080>, <tensorflow.python.keras.engine.training.Model object at 0x7f1d164828d0>]]\n"
     ]
    }
   ],
   "source": [
    "experiment_set = [ResNet50, ResNet50V2]\n",
    "jobs = []\n",
    "devices = ['/device:GPU:0', '/device:GPU:1']\n",
    "\n",
    "for exp, device in zip(experiment_set, devices):\n",
    "    tfk.clear_session()\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        with tf.device(device):\n",
    "            model = build_model_graph(exp, \"gn\")\n",
    "        optim = tf.keras.optimizers.SGD(lr=1e-4, momentum=0.9, nesterov=True, decay=0.999)\n",
    "        model.compile(loss='categorical_crossentropy', \n",
    "                      metrics=[\"accuracy\"], \n",
    "                      optimizer=optim)\n",
    "        jobs.append([graph, model])\n",
    "print(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:1'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = jobs[1]\n",
    "m.input.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-66c8194b76dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m m.fit_generator(train_gen, epochs=100, steps_per_epoch=100,\n\u001b[1;32m      2\u001b[0m                   \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                   validation_data=(x_val, y_val))\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         outs = model.train_on_batch(\n\u001b[0;32m--> 176\u001b[0;31m             x, y, sample_weight=sample_weight, class_weight=class_weight)\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "m.fit_generator(train_gen, epochs=100, steps_per_epoch=100,\n",
    "                  verbose=1, \n",
    "                  validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training on /device:GPU:0\n",
      "Start training on /device:GPU:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-14-09ef6b28a51a>\", line 38, in train\n",
      "    validation_data=(x_val, y_val), callbacks=[logger])\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 2177, in fit_generator\n",
      "    initial_epoch=initial_epoch)\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\", line 176, in fit_generator\n",
      "    x, y, sample_weight=sample_weight, class_weight=class_weight)\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1940, in train_on_batch\n",
      "    outputs = self.train_function(ins)\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 2986, in __call__\n",
      "    run_metadata=self.run_metadata)\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1439, in __call__\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 528, in __exit__\n",
      "    c_api.TF_GetCode(self.status.status))\n",
      "tensorflow.python.framework.errors_impl.FailedPreconditionError: Error while reading resource variable conv2_block2_2_gn/gamma from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/conv2_block2_2_gn/gamma/N10tensorflow3VarE does not exist.\n",
      "\t [[{{node training/SGD/ReadVariableOp_106}} = ReadVariableOp[dtype=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"](conv2_block2_2_gn/gamma)]]\n",
      "\t [[{{node loss/output_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/ExpandDims_1/_1283}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_8928_...pandDims_1\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "\n",
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-14-09ef6b28a51a>\", line 38, in train\n",
      "    validation_data=(x_val, y_val), callbacks=[logger])\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 2177, in fit_generator\n",
      "    initial_epoch=initial_epoch)\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\", line 176, in fit_generator\n",
      "    x, y, sample_weight=sample_weight, class_weight=class_weight)\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1940, in train_on_batch\n",
      "    outputs = self.train_function(ins)\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 2986, in __call__\n",
      "    run_metadata=self.run_metadata)\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1439, in __call__\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 528, in __exit__\n",
      "    c_api.TF_GetCode(self.status.status))\n",
      "tensorflow.python.framework.errors_impl.FailedPreconditionError: Error while reading resource variable conv5_block2_2_conv/bias from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/conv5_block2_2_conv/bias/N10tensorflow3VarE does not exist.\n",
      "\t [[{{node training/SGD/ReadVariableOp_971}} = ReadVariableOp[dtype=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv5_block2_2_conv/bias)]]\n",
      "\t [[{{node loss/output_loss/broadcast_weights/assert_broadcastable/AssertGuard/Assert/Switch_2/_1437}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_9733_...t/Switch_2\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start threads in parallel\n",
    "import threading\n",
    "\n",
    "train_threads = []\n",
    "for i, item in enumerate(jobs):\n",
    "    \n",
    "    this_graph = item[0]\n",
    "    this_model = item[1]\n",
    "    train_threads.append(threading.Thread(target=train, args=(devices[i], this_graph, this_model, train_gen)))\n",
    "    \n",
    "    #train_threads.append(threading.Thread(target=train, args=(devices[i], this_model, train_gen)))\n",
    "for t in train_threads:\n",
    "    t.start()\n",
    "for t in train_threads:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_worker_stop\n"
     ]
    }
   ],
   "source": [
    "train_gen.stop_worker()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {
    "6b1438b3075f49289cfcf03a4fce2ccb": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "ab7848b490e5454e9a42f439a6ef6b31": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "e0e3b0d66b1e47c4ade6c276bacc5c55": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
