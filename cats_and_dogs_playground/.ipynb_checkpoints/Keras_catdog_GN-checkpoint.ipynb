{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter setting and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=4, do_augment=True, dq_size=6, epochs=100, gpu_id=5, image_dir='/data/seanyu/cat_dog/dataset/', image_size=(128, 128, 3), lr=0.0001, model_file_name='model.h5', n_batch=100, n_classes=2, n_threads=4, save_dir='./result', train_ratio=0.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import threading\n",
    "import time\n",
    "from time import sleep\n",
    "from tqdm import tqdm # if use notebook\n",
    "\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Event\n",
    "import queue\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import random\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gpu_id', default=5)\n",
    "parser.add_argument('--image_dir', default=\"/data/seanyu/cat_dog/dataset/\")\n",
    "parser.add_argument('--save_dir', default='./result')\n",
    "parser.add_argument('--batch_size', default=4, type=int)\n",
    "parser.add_argument('--do_augment', default=True, type = bool)\n",
    "parser.add_argument('--epochs', default=100, type=int)\n",
    "parser.add_argument('--lr', default=1e-4, type=float)\n",
    "parser.add_argument('--image_size', default=(128,128,3), type = int)\n",
    "parser.add_argument('--n_classes', default=2, type = int)\n",
    "parser.add_argument('--n_batch', default=100, type = int)\n",
    "parser.add_argument('--train_ratio', default=0.9, type = float)\n",
    "parser.add_argument('--model_file_name', default = 'model.h5')\n",
    "parser.add_argument('--n_threads', default = 4, type = int)\n",
    "parser.add_argument('--dq_size', default = 6, type = int)\n",
    "\n",
    "FLAGS = parser.parse_args([])\n",
    "print(FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS.gpu_id = \"4\"\n",
    "FLAGS.image_dir = \"/data/seanyu/cat_dog/dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check path and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(FLAGS.gpu_id)\n",
    "import tensorflow as tf\n",
    "\n",
    "if not os.path.exists(FLAGS.save_dir):\n",
    "    os.makedirs(FLAGS.save_dir)\n",
    "\n",
    "model_dir = FLAGS.save_dir + '/model'\n",
    "\n",
    "\"\"\"  Get data \"\"\"\n",
    "d_train = FLAGS.image_dir + '/train/'\n",
    "d_test = FLAGS.image_dir + '/test1/'\n",
    "\n",
    "image_train_list = glob.glob(d_train + '*.jpg')\n",
    "image_test_list = glob.glob(d_test + '*.jpg')\n",
    "\n",
    "df_train = pd.DataFrame({'img_path': image_train_list})\n",
    "df_test = pd.DataFrame({'img_path': image_test_list})\n",
    "\n",
    "df_train['cate'] = df_train.img_path.apply(os.path.basename)\n",
    "df_train['cate'] = [i.split(\".\")[0] for i in list(df_train.cate)]\n",
    "df_train.cate = df_train.cate.replace({'dog': 0, 'cat': 1})\n",
    "\n",
    "nb_epoch = FLAGS.epochs\n",
    "\n",
    "df_train_0, df_val_0 = train_test_split(df_train[df_train['cate'] == 0], test_size = 1-FLAGS.train_ratio)\n",
    "df_train_1, df_val_1 = train_test_split(df_train[df_train['cate'] == 1], test_size = 1-FLAGS.train_ratio)\n",
    "\n",
    "df_val = pd.concat((df_val_0, df_val_1)).reset_index(drop = True)\n",
    "\n",
    "del df_val_0, df_val_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import imgaug as ia\n",
    "    from imgaug import augmenters as iaa\n",
    "except:\n",
    "    print(\"Import Error, Please make sure you have imgaug\")\n",
    "        \n",
    "try:\n",
    "    import sys\n",
    "    sys.path.append(\"/mnt/deep-learning/usr/seanyu/common_tools/\")\n",
    "    from customized_imgaug_func import keypoint_func, img_channelswap\n",
    "except:\n",
    "    print(\"Warning, if you used customized imgaug function\")\n",
    "    \n",
    "class Augmentation_Setup(object):  \n",
    "    sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "    lesstimes = lambda aug: iaa.Sometimes(0.2, aug)\n",
    "    \n",
    "    augmentation = iaa.Sequential([\n",
    "        iaa.Fliplr(0.5, name=\"FlipLR\"),\n",
    "        iaa.Flipud(0.5, name=\"FlipUD\"),\n",
    "        iaa.OneOf([iaa.Affine(rotate = 90),\n",
    "                   iaa.Affine(rotate = 180),\n",
    "                   iaa.Affine(rotate = 270)]),\n",
    "        sometimes(iaa.Affine(\n",
    "                    scale = (0.8,1.2),\n",
    "                    translate_percent = (-0.2, 0.2),\n",
    "                    rotate = (-15, 15),\n",
    "                    mode = 'wrap'\n",
    "                    ))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetDataset():\n",
    "    def __init__(self, df_list, class_id, n_classes, f_input_preproc, image_size=(256,256,3), onehot=True, augmentation=None):\n",
    "        \n",
    "        self.df_list = df_list\n",
    "        self.class_id = class_id\n",
    "        self.n_classes = n_classes\n",
    "        self.preproc = f_input_preproc\n",
    "        self.image_size = image_size\n",
    "        self.onehot = onehot\n",
    "        self.aug = augmentation\n",
    "        \n",
    "        ## Init ##\n",
    "        self.df_list = self.df_list.sample(frac=1.).reset_index(drop=True)\n",
    "        self.current_index = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img = self.load_image(img_path=self.df_list.iloc[self.current_index]['img_path'], image_size=self.image_size)\n",
    "        \n",
    "        if self.aug is not None:\n",
    "            img = self.aug.augment_image(img)\n",
    "            \n",
    "        img = img.astype(np.float32)\n",
    "        \n",
    "        if self.preproc is not None:\n",
    "            img = self.preproc(img)\n",
    "        \n",
    "        label = self.class_id\n",
    "        if self.onehot:\n",
    "             label = tf.keras.utils.to_categorical(label, num_classes=self.n_classes)\n",
    "        \n",
    "        self.current_index = (self.current_index + 1) % len(self.df_list)\n",
    "        return img, label\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.__getitem__(idx=self.current_index)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_image(img_path, image_size):\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (image_size[0], image_size[1]))\n",
    "        return img\n",
    "    \n",
    "class Customized_dataloader():\n",
    "    \"\"\"\n",
    "    1. Compose multiple generators together\n",
    "    2. Make this composed generator into multi-processing function\n",
    "    \"\"\"\n",
    "    def __init__(self, list_dataset, batch_size_per_dataset=16, queue_size=128, num_workers=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - list_dataset: put generator object as list [gen1, gen2, ...]\n",
    "            - batch_size_per_dataset: bz for each generator (total_batch_size/n_class)\n",
    "            - queue_size: queue size\n",
    "            - num_workers: start n workers to get data\n",
    "        \n",
    "        Action: Call with next\n",
    "        \"\"\"\n",
    "        self.list_dataset = list_dataset\n",
    "        self.batch_size_per_dataset = batch_size_per_dataset\n",
    "        self.sample_queue = mp.Queue(maxsize = queue_size)\n",
    "        \n",
    "        self.jobs = num_workers\n",
    "        self.events = list()\n",
    "        self.workers = list()\n",
    "        for i in range(num_workers):\n",
    "            event = Event()\n",
    "            work = mp.Process(target = enqueue, args = (self.sample_queue, event, self.compose_data))\n",
    "            work.daemon = True\n",
    "            work.start()\n",
    "            self.events.append(event)\n",
    "            self.workers.append(work)\n",
    "        print(\"workers ready\")\n",
    "        \n",
    "    def __next__(self):\n",
    "        return self.sample_queue.get()\n",
    "    \n",
    "    def compose_data(self):\n",
    "        while True:\n",
    "            imgs, labels = [], []\n",
    "            for z in range(self.batch_size_per_dataset):\n",
    "                data = [next(i) for i in self.list_dataset]\n",
    "                img, label = zip(*data)\n",
    "                imgs.append(np.array(img))\n",
    "                labels.append(np.array(label))\n",
    "            yield np.concatenate(imgs), np.concatenate(labels)\n",
    "    \n",
    "    def stop_worker(self):\n",
    "        for t in self.events:\n",
    "            t.set()\n",
    "        for i, t in enumerate(self.workers):\n",
    "            t.join(timeout = 1)\n",
    "        print(\"all_worker_stop\")\n",
    "\n",
    "# ----- #\n",
    "def enqueue(queue, stop, gen_func):\n",
    "    gen = gen_func()\n",
    "    while True:\n",
    "        if stop.is_set():\n",
    "            return\n",
    "        queue.put(next(gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(img):\n",
    "    #return (img - img.min()) / (img.max() - img.min())\n",
    "    return img / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_RESNET_PREPROC = False\n",
    "dog_train = GetDataset(df_list=df_train[df_train['cate'] == 0],\n",
    "                       class_id=0, n_classes=2,\n",
    "                       f_input_preproc=preproc if not USE_RESNET_PREPROC else tf.keras.applications.resnet50.preprocess_input,\n",
    "                       augmentation=Augmentation_Setup.augmentation, \n",
    "                       onehot= True, \n",
    "                       image_size=(256,256,3))\n",
    "\n",
    "cat_train = GetDataset(df_list=df_train[df_train['cate'] == 1], \n",
    "                       class_id=1, n_classes=2, \n",
    "                       f_input_preproc=preproc if not USE_RESNET_PREPROC else tf.keras.applications.resnet50.preprocess_input,\n",
    "                       augmentation=Augmentation_Setup.augmentation, \n",
    "                       onehot= True, \n",
    "                       image_size=(256,256,3))\n",
    "\n",
    "dog_valid = GetDataset(df_list=df_val[df_val['cate'] == 0], \n",
    "                       class_id=0, n_classes=2,\n",
    "                       f_input_preproc=preproc if not USE_RESNET_PREPROC else tf.keras.applications.resnet50.preprocess_input,\n",
    "                       augmentation=None, \n",
    "                       onehot= True, \n",
    "                       image_size=(256,256,3))\n",
    "\n",
    "cat_valid = GetDataset(df_list=df_val[df_val['cate'] == 1], \n",
    "                       class_id=1, n_classes=2, \n",
    "                       f_input_preproc=preproc if not USE_RESNET_PREPROC else tf.keras.applications.resnet50.preprocess_input,\n",
    "                       augmentation=None, \n",
    "                       onehot= True, \n",
    "                       image_size=(256,256,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:01,  8.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workers ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 23.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_worker_stop\n",
      "(40, 256, 256, 3)\n",
      "(40, 2)\n",
      "[20. 20.]\n"
     ]
    }
   ],
   "source": [
    "valid_gen = Customized_dataloader([dog_valid, cat_valid], batch_size_per_dataset=FLAGS.batch_size//2, num_workers=2, queue_size=10)\n",
    "x_val, y_val = [], []\n",
    "for _ in tqdm(range(10)):\n",
    "    a,b = next(valid_gen)\n",
    "    x_val.append(a)\n",
    "    y_val.append(b)\n",
    "x_val = np.concatenate(x_val)\n",
    "y_val = np.concatenate(y_val)\n",
    "valid_gen.stop_worker()\n",
    "\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "print(y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/deep-learning/usr/seanyu/lab_mldl_tools/models/\")\n",
    "\n",
    "import keras\n",
    "from keras_resnet_rebuilt.model import ResNet50, ResNet50V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_graph(model_fn, norm_use, input_shape=(256,256,3), n_outputs=2):\n",
    "    pretrain_modules = model_fn(include_top=False, input_shape=input_shape, norm_use=norm_use, weights='imagenet')\n",
    "    gap = keras.layers.GlobalAveragePooling2D()(pretrain_modules.output)\n",
    "    out = keras.layers.Dense(units=n_outputs, activation='softmax', name='output')(gap)\n",
    "    model = keras.models.Model(inputs=[pretrain_modules.input], outputs=[out])\n",
    "    return model\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "class Logger(Callback):\n",
    "    def __init__(self, n, gpu_id = 0):\n",
    "        self.n = n   # print loss & acc every n epochs\n",
    "        self.gpu_id = gpu_id\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.n == 0:\n",
    "            # add what you need here\n",
    "            train_loss = logs.get('loss')\n",
    "            train_acc = logs.get('acc')\n",
    "            valid_loss = logs.get('val_loss')\n",
    "            valid_acc = logs.get('val_acc')\n",
    "            print(\"GPU_ID: %s, epoch: %4d, loss: %0.5f, acc: %0.3f, val_loss: %0.5f, val_acc: %0.3f\" \\\n",
    "                  % (self.gpu_id, epoch, \n",
    "                     train_loss, train_acc,\n",
    "                     valid_loss, valid_acc))\n",
    "\n",
    "import keras.backend as K\n",
    "def train(device, model, generator):\n",
    "    print(\"Start training on %s\" % device)\n",
    "    logger = Logger(n=1, gpu_id=device)\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tfk.set_session(session=session)\n",
    "        model.fit_generator(generator, epochs=100, steps_per_epoch=100,\n",
    "                  verbose=0, \n",
    "                  validation_data=(x_val, y_val), callbacks=[logger])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder:0\", shape=(7, 7, 3, 64), dtype=float32) is not an element of this graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1091\u001b[0m             subfeed_t = self.graph.as_graph_element(\n\u001b[0;32m-> 1092\u001b[0;31m                 subfeed, allow_tensor=True, allow_operation=False)\n\u001b[0m\u001b[1;32m   1093\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3489\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3490\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3568\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3569\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3570\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor Tensor(\"Placeholder:0\", shape=(7, 7, 3, 64), dtype=float32) is not an element of this graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d49a0307d127>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         model.compile(loss='categorical_crossentropy', \n",
      "\u001b[0;32m<ipython-input-10-1ed086b00861>\u001b[0m in \u001b[0;36mbuild_model_graph\u001b[0;34m(model_fn, norm_use, input_shape, n_outputs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_model_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_use\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpretrain_modules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_use\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm_use\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mgap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGlobalAveragePooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrain_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpretrain_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/deep-learning/usr/seanyu/lab_mldl_tools/models/keras_resnet_rebuilt/model.py\u001b[0m in \u001b[0;36mResNet50\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, norm_use, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m                   \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                   \u001b[0mpooling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_use\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm_use\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                   **kwargs)\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/deep-learning/usr/seanyu/lab_mldl_tools/models/keras_resnet_rebuilt/model.py\u001b[0m in \u001b[0;36mResNet\u001b[0;34m(stack_fn, preact, use_bias, model_name, include_top, weights, input_tensor, input_shape, pooling, classes, norm_use, **kwargs)\u001b[0m\n\u001b[1;32m    375\u001b[0m                                             \u001b[0mcache_subdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                                             file_hash=file_hash)\n\u001b[0;32m--> 377\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 saving.load_weights_from_hdf5_group_by_name(\n\u001b[1;32m   1162\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_mismatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1163\u001b[0;31m                     reshape=reshape)\n\u001b[0m\u001b[1;32m   1164\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m                 saving.load_weights_from_hdf5_group(\n",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group_by_name\u001b[0;34m(f, layers, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1152\u001b[0m                                                 weight_values[i]))\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2468\u001b[0m             \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2469\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2470\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1093\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m             raise TypeError(\n\u001b[0;32m-> 1095\u001b[0;31m                 'Cannot interpret feed_dict key as Tensor: ' + e.args[0])\n\u001b[0m\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder:0\", shape=(7, 7, 3, 64), dtype=float32) is not an element of this graph."
     ]
    }
   ],
   "source": [
    "experiment_set = [ResNet50, ResNet50V2]\n",
    "jobs = []\n",
    "devices = ['/device:GPU:0', '/device:GPU:1']\n",
    "\n",
    "for exp, device in zip(experiment_set, devices):\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        with tf.device(device):\n",
    "            model = build_model_graph(exp, \"gn\")\n",
    "        optim = keras.optimizers.SGD(lr=1e-4, momentum=0.9, nesterov=True, decay=0.999)\n",
    "        model.compile(loss='categorical_crossentropy', \n",
    "                      metrics=[\"accuracy\"], \n",
    "                      optimizer=optim)\n",
    "        jobs.append([graph, model])\n",
    "print(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.backend.clear_session()\n",
    "pretrain_modules = ResNeXt50(include_top=False, input_shape=x_val[0].shape, norm_use=\"gn\", weights='imagenet')\n",
    "gap = keras.layers.GlobalAveragePooling2D()(pretrain_modules.output)\n",
    "out = keras.layers.Dense(units=2, activation='softmax', name='output')(gap)\n",
    "model = keras.models.Model(inputs=[pretrain_modules.input], outputs=[out])\n",
    "\n",
    "optim = keras.optimizers.Adam(lr=1e-5, amsgrad=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              metrics=[\"accuracy\"], \n",
    "              optimizer=optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 262, 262, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 128, 128, 64) 9408        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_gn (GroupNorm)            (None, 128, 128, 64) 128         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 128, 128, 64) 0           conv1_gn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 130, 130, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 64, 64, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 64, 64, 128)  8192        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_gn (GroupNorm)   (None, 64, 64, 128)  256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 64, 64, 128)  0           conv2_block1_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_pad (ZeroPadding (None, 66, 66, 128)  0           conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (DepthwiseC (None, 64, 64, 512)  4608        conv2_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 64, 64, 32, 4 0           conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_reduce (Lambda)  (None, 64, 64, 32, 4 0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 64, 64, 128)  0           conv2_block1_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_gn (GroupNorm)   (None, 64, 64, 128)  256         reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 64, 64, 128)  0           conv2_block1_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 64, 64, 256)  16384       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 64, 64, 256)  32768       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_gn (GroupNorm)   (None, 64, 64, 256)  512         conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_gn (GroupNorm)   (None, 64, 64, 256)  512         conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 64, 64, 256)  0           conv2_block1_0_gn[0][0]          \n",
      "                                                                 conv2_block1_3_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 64, 64, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 64, 64, 128)  32768       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_gn (GroupNorm)   (None, 64, 64, 128)  256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 64, 64, 128)  0           conv2_block2_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_pad (ZeroPadding (None, 66, 66, 128)  0           conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (DepthwiseC (None, 64, 64, 512)  4608        conv2_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 64, 64, 32, 4 0           conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_reduce (Lambda)  (None, 64, 64, 32, 4 0           reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 64, 64, 128)  0           conv2_block2_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_gn (GroupNorm)   (None, 64, 64, 128)  256         reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 64, 64, 128)  0           conv2_block2_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 64, 64, 256)  32768       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_gn (GroupNorm)   (None, 64, 64, 256)  512         conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 64, 64, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 64, 64, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 64, 64, 128)  32768       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_gn (GroupNorm)   (None, 64, 64, 128)  256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 64, 64, 128)  0           conv2_block3_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_pad (ZeroPadding (None, 66, 66, 128)  0           conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (DepthwiseC (None, 64, 64, 512)  4608        conv2_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 64, 64, 32, 4 0           conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_reduce (Lambda)  (None, 64, 64, 32, 4 0           reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 64, 64, 128)  0           conv2_block3_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_gn (GroupNorm)   (None, 64, 64, 128)  256         reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 64, 64, 128)  0           conv2_block3_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 64, 64, 256)  32768       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_gn (GroupNorm)   (None, 64, 64, 256)  512         conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 64, 64, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 64, 64, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 64, 64, 256)  65536       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_gn (GroupNorm)   (None, 64, 64, 256)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 64, 64, 256)  0           conv3_block1_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_pad (ZeroPadding (None, 66, 66, 256)  0           conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (DepthwiseC (None, 32, 32, 2048) 18432       conv3_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 32, 32, 32, 8 0           conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_reduce (Lambda)  (None, 32, 32, 32, 8 0           reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 32, 32, 256)  0           conv3_block1_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_gn (GroupNorm)   (None, 32, 32, 256)  512         reshape_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 32, 32, 256)  0           conv3_block1_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 32, 32, 512)  131072      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 32, 32, 512)  131072      conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_gn (GroupNorm)   (None, 32, 32, 512)  1024        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_gn (GroupNorm)   (None, 32, 32, 512)  1024        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 32, 32, 512)  0           conv3_block1_0_gn[0][0]          \n",
      "                                                                 conv3_block1_3_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 32, 32, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 32, 32, 256)  131072      conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_gn (GroupNorm)   (None, 32, 32, 256)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 32, 32, 256)  0           conv3_block2_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_pad (ZeroPadding (None, 34, 34, 256)  0           conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (DepthwiseC (None, 32, 32, 2048) 18432       conv3_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 32, 32, 32, 8 0           conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_reduce (Lambda)  (None, 32, 32, 32, 8 0           reshape_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)            (None, 32, 32, 256)  0           conv3_block2_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_gn (GroupNorm)   (None, 32, 32, 256)  512         reshape_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 32, 32, 256)  0           conv3_block2_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 32, 32, 512)  131072      conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_gn (GroupNorm)   (None, 32, 32, 512)  1024        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 32, 32, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 32, 32, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 32, 32, 256)  131072      conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_gn (GroupNorm)   (None, 32, 32, 256)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 32, 32, 256)  0           conv3_block3_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_pad (ZeroPadding (None, 34, 34, 256)  0           conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (DepthwiseC (None, 32, 32, 2048) 18432       conv3_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_11 (Reshape)            (None, 32, 32, 32, 8 0           conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_reduce (Lambda)  (None, 32, 32, 32, 8 0           reshape_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_12 (Reshape)            (None, 32, 32, 256)  0           conv3_block3_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_gn (GroupNorm)   (None, 32, 32, 256)  512         reshape_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 32, 32, 256)  0           conv3_block3_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 32, 32, 512)  131072      conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_gn (GroupNorm)   (None, 32, 32, 512)  1024        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 32, 32, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 32, 32, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 32, 32, 256)  131072      conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_gn (GroupNorm)   (None, 32, 32, 256)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 32, 32, 256)  0           conv3_block4_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_pad (ZeroPadding (None, 34, 34, 256)  0           conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (DepthwiseC (None, 32, 32, 2048) 18432       conv3_block4_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)            (None, 32, 32, 32, 8 0           conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_reduce (Lambda)  (None, 32, 32, 32, 8 0           reshape_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_14 (Reshape)            (None, 32, 32, 256)  0           conv3_block4_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_gn (GroupNorm)   (None, 32, 32, 256)  512         reshape_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 32, 32, 256)  0           conv3_block4_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 32, 32, 512)  131072      conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_gn (GroupNorm)   (None, 32, 32, 512)  1024        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 32, 32, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 32, 32, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 32, 32, 512)  262144      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_gn (GroupNorm)   (None, 32, 32, 512)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 32, 32, 512)  0           conv4_block1_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_pad (ZeroPadding (None, 34, 34, 512)  0           conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (DepthwiseC (None, 16, 16, 8192) 73728       conv4_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_15 (Reshape)            (None, 16, 16, 32, 1 0           conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_reduce (Lambda)  (None, 16, 16, 32, 1 0           reshape_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_16 (Reshape)            (None, 16, 16, 512)  0           conv4_block1_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_gn (GroupNorm)   (None, 16, 16, 512)  1024        reshape_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 16, 16, 512)  0           conv4_block1_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 16, 16, 1024) 524288      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 16, 16, 1024) 524288      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_gn (GroupNorm)   (None, 16, 16, 1024) 2048        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_gn (GroupNorm)   (None, 16, 16, 1024) 2048        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_0_gn[0][0]          \n",
      "                                                                 conv4_block1_3_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 16, 16, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 16, 16, 512)  524288      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_gn (GroupNorm)   (None, 16, 16, 512)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 16, 16, 512)  0           conv4_block2_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_pad (ZeroPadding (None, 18, 18, 512)  0           conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (DepthwiseC (None, 16, 16, 8192) 73728       conv4_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_17 (Reshape)            (None, 16, 16, 32, 1 0           conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_reduce (Lambda)  (None, 16, 16, 32, 1 0           reshape_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_18 (Reshape)            (None, 16, 16, 512)  0           conv4_block2_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_gn (GroupNorm)   (None, 16, 16, 512)  1024        reshape_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 16, 16, 512)  0           conv4_block2_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 16, 16, 1024) 524288      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_gn (GroupNorm)   (None, 16, 16, 1024) 2048        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 16, 16, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 16, 16, 512)  524288      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_gn (GroupNorm)   (None, 16, 16, 512)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 16, 16, 512)  0           conv4_block3_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_pad (ZeroPadding (None, 18, 18, 512)  0           conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (DepthwiseC (None, 16, 16, 8192) 73728       conv4_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_19 (Reshape)            (None, 16, 16, 32, 1 0           conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_reduce (Lambda)  (None, 16, 16, 32, 1 0           reshape_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_20 (Reshape)            (None, 16, 16, 512)  0           conv4_block3_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_gn (GroupNorm)   (None, 16, 16, 512)  1024        reshape_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 16, 16, 512)  0           conv4_block3_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 16, 16, 1024) 524288      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_gn (GroupNorm)   (None, 16, 16, 1024) 2048        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 16, 16, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 16, 16, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 16, 16, 512)  524288      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_gn (GroupNorm)   (None, 16, 16, 512)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 16, 16, 512)  0           conv4_block4_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_pad (ZeroPadding (None, 18, 18, 512)  0           conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (DepthwiseC (None, 16, 16, 8192) 73728       conv4_block4_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_21 (Reshape)            (None, 16, 16, 32, 1 0           conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_reduce (Lambda)  (None, 16, 16, 32, 1 0           reshape_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_22 (Reshape)            (None, 16, 16, 512)  0           conv4_block4_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_gn (GroupNorm)   (None, 16, 16, 512)  1024        reshape_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 16, 16, 512)  0           conv4_block4_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 16, 16, 1024) 524288      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_gn (GroupNorm)   (None, 16, 16, 1024) 2048        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 16, 16, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 16, 16, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 16, 16, 512)  524288      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_gn (GroupNorm)   (None, 16, 16, 512)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 16, 16, 512)  0           conv4_block5_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_pad (ZeroPadding (None, 18, 18, 512)  0           conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (DepthwiseC (None, 16, 16, 8192) 73728       conv4_block5_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_23 (Reshape)            (None, 16, 16, 32, 1 0           conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_reduce (Lambda)  (None, 16, 16, 32, 1 0           reshape_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_24 (Reshape)            (None, 16, 16, 512)  0           conv4_block5_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_gn (GroupNorm)   (None, 16, 16, 512)  1024        reshape_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 16, 16, 512)  0           conv4_block5_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 16, 16, 1024) 524288      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_gn (GroupNorm)   (None, 16, 16, 1024) 2048        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 16, 16, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 16, 16, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 16, 16, 512)  524288      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_gn (GroupNorm)   (None, 16, 16, 512)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 16, 16, 512)  0           conv4_block6_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_pad (ZeroPadding (None, 18, 18, 512)  0           conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (DepthwiseC (None, 16, 16, 8192) 73728       conv4_block6_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_25 (Reshape)            (None, 16, 16, 32, 1 0           conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_reduce (Lambda)  (None, 16, 16, 32, 1 0           reshape_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_26 (Reshape)            (None, 16, 16, 512)  0           conv4_block6_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_gn (GroupNorm)   (None, 16, 16, 512)  1024        reshape_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 16, 16, 512)  0           conv4_block6_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 16, 16, 1024) 524288      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_gn (GroupNorm)   (None, 16, 16, 1024) 2048        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 16, 16, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 16, 16, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 16, 16, 1024) 1048576     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_gn (GroupNorm)   (None, 16, 16, 1024) 2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 16, 16, 1024) 0           conv5_block1_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_pad (ZeroPadding (None, 18, 18, 1024) 0           conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (DepthwiseC (None, 8, 8, 32768)  294912      conv5_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_27 (Reshape)            (None, 8, 8, 32, 32, 0           conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_reduce (Lambda)  (None, 8, 8, 32, 32) 0           reshape_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_28 (Reshape)            (None, 8, 8, 1024)   0           conv5_block1_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_gn (GroupNorm)   (None, 8, 8, 1024)   2048        reshape_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 8, 8, 1024)   0           conv5_block1_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 8, 8, 2048)   2097152     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 8, 8, 2048)   2097152     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_gn (GroupNorm)   (None, 8, 8, 2048)   4096        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_gn (GroupNorm)   (None, 8, 8, 2048)   4096        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_0_gn[0][0]          \n",
      "                                                                 conv5_block1_3_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 8, 8, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 8, 8, 1024)   2097152     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_gn (GroupNorm)   (None, 8, 8, 1024)   2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 8, 8, 1024)   0           conv5_block2_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_pad (ZeroPadding (None, 10, 10, 1024) 0           conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (DepthwiseC (None, 8, 8, 32768)  294912      conv5_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_29 (Reshape)            (None, 8, 8, 32, 32, 0           conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_reduce (Lambda)  (None, 8, 8, 32, 32) 0           reshape_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_30 (Reshape)            (None, 8, 8, 1024)   0           conv5_block2_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_gn (GroupNorm)   (None, 8, 8, 1024)   2048        reshape_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 8, 8, 1024)   0           conv5_block2_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 8, 8, 2048)   2097152     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_gn (GroupNorm)   (None, 8, 8, 2048)   4096        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 8, 8, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 8, 8, 1024)   2097152     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_gn (GroupNorm)   (None, 8, 8, 1024)   2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 8, 8, 1024)   0           conv5_block3_1_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_pad (ZeroPadding (None, 10, 10, 1024) 0           conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (DepthwiseC (None, 8, 8, 32768)  294912      conv5_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_31 (Reshape)            (None, 8, 8, 32, 32, 0           conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_reduce (Lambda)  (None, 8, 8, 32, 32) 0           reshape_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_32 (Reshape)            (None, 8, 8, 1024)   0           conv5_block3_2_reduce[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_gn (GroupNorm)   (None, 8, 8, 1024)   2048        reshape_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 8, 8, 1024)   0           conv5_block3_2_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 8, 8, 2048)   2097152     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_gn (GroupNorm)   (None, 8, 8, 2048)   4096        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 8, 8, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_gn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 8, 8, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            4098        global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 22,984,002\n",
      "Trainable params: 22,984,002\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workers ready\n"
     ]
    }
   ],
   "source": [
    "train_gen = Customized_dataloader([dog_train, cat_train], \n",
    "                                  batch_size_per_dataset=FLAGS.batch_size//2, \n",
    "                                  num_workers=4, queue_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.6998 - acc: 0.5900 - val_loss: 0.7408 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 30s 299ms/step - loss: 0.6134 - acc: 0.6600 - val_loss: 0.6610 - val_acc: 0.6000\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 30s 299ms/step - loss: 0.6728 - acc: 0.5875 - val_loss: 0.6392 - val_acc: 0.6500\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 30s 301ms/step - loss: 0.6408 - acc: 0.6575 - val_loss: 0.6522 - val_acc: 0.5000\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 28s 284ms/step - loss: 0.6489 - acc: 0.6300 - val_loss: 0.5888 - val_acc: 0.6500\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 30s 300ms/step - loss: 0.6070 - acc: 0.6625 - val_loss: 0.6086 - val_acc: 0.7500\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 31s 309ms/step - loss: 0.5927 - acc: 0.6800 - val_loss: 0.6673 - val_acc: 0.7000\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 29s 295ms/step - loss: 0.6280 - acc: 0.6650 - val_loss: 0.7351 - val_acc: 0.7500\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 29s 290ms/step - loss: 0.5652 - acc: 0.6950 - val_loss: 0.6393 - val_acc: 0.6000\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 29s 292ms/step - loss: 0.5664 - acc: 0.7100 - val_loss: 0.6516 - val_acc: 0.6500\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 31s 305ms/step - loss: 0.4910 - acc: 0.7725 - val_loss: 0.7687 - val_acc: 0.5500\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 30s 301ms/step - loss: 0.4746 - acc: 0.7750 - val_loss: 0.6109 - val_acc: 0.7500\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 29s 295ms/step - loss: 0.4632 - acc: 0.7650 - val_loss: 0.6519 - val_acc: 0.6500\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 0.5638 - acc: 0.7200 - val_loss: 0.5099 - val_acc: 0.8000\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 30s 302ms/step - loss: 0.4500 - acc: 0.7900 - val_loss: 0.5329 - val_acc: 0.7500\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 30s 304ms/step - loss: 0.5097 - acc: 0.7700 - val_loss: 0.6758 - val_acc: 0.7000\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 30s 299ms/step - loss: 0.4031 - acc: 0.8150 - val_loss: 0.9855 - val_acc: 0.4000\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 28s 284ms/step - loss: 0.3931 - acc: 0.8500 - val_loss: 0.6044 - val_acc: 0.6500\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 30s 299ms/step - loss: 0.4736 - acc: 0.7850 - val_loss: 0.7077 - val_acc: 0.5500\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 30s 299ms/step - loss: 0.4216 - acc: 0.8000 - val_loss: 0.5505 - val_acc: 0.6000\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 30s 302ms/step - loss: 0.3853 - acc: 0.8375 - val_loss: 0.9872 - val_acc: 0.7000\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 29s 291ms/step - loss: 0.3687 - acc: 0.8600 - val_loss: 0.6201 - val_acc: 0.6500\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 29s 290ms/step - loss: 0.4685 - acc: 0.7850 - val_loss: 0.5536 - val_acc: 0.6500\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 30s 300ms/step - loss: 0.3988 - acc: 0.8175 - val_loss: 0.5531 - val_acc: 0.6500\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 30s 299ms/step - loss: 0.3630 - acc: 0.8650 - val_loss: 0.6215 - val_acc: 0.7000\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 29s 292ms/step - loss: 0.3445 - acc: 0.8675 - val_loss: 0.6793 - val_acc: 0.7000\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 29s 286ms/step - loss: 0.3475 - acc: 0.8600 - val_loss: 0.5204 - val_acc: 0.7000\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 30s 300ms/step - loss: 0.3325 - acc: 0.8800 - val_loss: 0.6695 - val_acc: 0.8000\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 30s 302ms/step - loss: 0.2851 - acc: 0.8950 - val_loss: 0.5306 - val_acc: 0.7500\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 31s 306ms/step - loss: 0.3616 - acc: 0.8575 - val_loss: 0.6177 - val_acc: 0.7500\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.3667 - acc: 0.8550 - val_loss: 0.5368 - val_acc: 0.8000\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 30s 297ms/step - loss: 0.3005 - acc: 0.8700 - val_loss: 0.5925 - val_acc: 0.7000\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 30s 297ms/step - loss: 0.4099 - acc: 0.8225 - val_loss: 0.6638 - val_acc: 0.7500\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 30s 299ms/step - loss: 0.3560 - acc: 0.8500 - val_loss: 0.6592 - val_acc: 0.7500\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 29s 289ms/step - loss: 0.2886 - acc: 0.8825 - val_loss: 1.1827 - val_acc: 0.6500\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 29s 293ms/step - loss: 0.3109 - acc: 0.8775 - val_loss: 0.7633 - val_acc: 0.7000\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 30s 300ms/step - loss: 0.3986 - acc: 0.8100 - val_loss: 1.7110 - val_acc: 0.6500\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 31s 308ms/step - loss: 0.4627 - acc: 0.7950 - val_loss: 0.7708 - val_acc: 0.6500\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 29s 291ms/step - loss: 0.3754 - acc: 0.8350 - val_loss: 0.7014 - val_acc: 0.7000\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 29s 286ms/step - loss: 0.2552 - acc: 0.9125 - val_loss: 0.9857 - val_acc: 0.6500\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 30s 305ms/step - loss: 0.3156 - acc: 0.8700 - val_loss: 0.5092 - val_acc: 0.8000\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 30s 302ms/step - loss: 0.3586 - acc: 0.8675 - val_loss: 0.4931 - val_acc: 0.6500\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 30s 305ms/step - loss: 0.3593 - acc: 0.8500 - val_loss: 0.5774 - val_acc: 0.7500\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.3837 - acc: 0.8325 - val_loss: 0.9149 - val_acc: 0.6500\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 31s 305ms/step - loss: 0.3284 - acc: 0.8675 - val_loss: 0.4604 - val_acc: 0.7000\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 29s 292ms/step - loss: 0.2278 - acc: 0.8975 - val_loss: 0.6215 - val_acc: 0.8000\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 28s 284ms/step - loss: 0.3983 - acc: 0.8150 - val_loss: 0.4255 - val_acc: 0.7500\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.2746 - acc: 0.8925 - val_loss: 0.3991 - val_acc: 0.8000\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.3056 - acc: 0.8950 - val_loss: 0.4664 - val_acc: 0.8000\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 28s 284ms/step - loss: 0.2248 - acc: 0.9100 - val_loss: 0.4866 - val_acc: 0.7000\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.2606 - acc: 0.9100 - val_loss: 0.8141 - val_acc: 0.8000\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 28s 284ms/step - loss: 0.1995 - acc: 0.9200 - val_loss: 0.5720 - val_acc: 0.8000\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.2599 - acc: 0.9075 - val_loss: 0.4429 - val_acc: 0.7500\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 29s 289ms/step - loss: 0.1618 - acc: 0.9475 - val_loss: 0.5360 - val_acc: 0.8000\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 29s 288ms/step - loss: 0.1364 - acc: 0.9450 - val_loss: 0.5950 - val_acc: 0.7000\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 29s 289ms/step - loss: 0.2060 - acc: 0.9025 - val_loss: 0.6687 - val_acc: 0.8000\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 29s 291ms/step - loss: 0.3005 - acc: 0.8975 - val_loss: 0.4102 - val_acc: 0.8500\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.2397 - acc: 0.9225 - val_loss: 0.3917 - val_acc: 0.8500\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.2985 - acc: 0.8725 - val_loss: 0.4741 - val_acc: 0.7500\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 29s 292ms/step - loss: 0.2675 - acc: 0.8950 - val_loss: 0.4147 - val_acc: 0.8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "100/100 [==============================] - 29s 290ms/step - loss: 0.1785 - acc: 0.9250 - val_loss: 0.8432 - val_acc: 0.8000\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 29s 290ms/step - loss: 0.1886 - acc: 0.9325 - val_loss: 0.3885 - val_acc: 0.8500\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 0.1887 - acc: 0.9275 - val_loss: 0.4925 - val_acc: 0.8000\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 28s 284ms/step - loss: 0.2452 - acc: 0.8975 - val_loss: 1.0024 - val_acc: 0.8000\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 29s 292ms/step - loss: 0.1548 - acc: 0.9375 - val_loss: 0.7574 - val_acc: 0.8000\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 29s 291ms/step - loss: 0.1687 - acc: 0.9450 - val_loss: 0.5208 - val_acc: 0.8000\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 29s 290ms/step - loss: 0.1048 - acc: 0.9625 - val_loss: 0.6517 - val_acc: 0.8000\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 0.1624 - acc: 0.9425 - val_loss: 0.4430 - val_acc: 0.8000\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 28s 285ms/step - loss: 0.1195 - acc: 0.9575 - val_loss: 0.7374 - val_acc: 0.8000\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 29s 292ms/step - loss: 0.1066 - acc: 0.9600 - val_loss: 0.5985 - val_acc: 0.8000\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 29s 291ms/step - loss: 0.2348 - acc: 0.9075 - val_loss: 0.3245 - val_acc: 0.8500\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 29s 289ms/step - loss: 0.1465 - acc: 0.9400 - val_loss: 0.6268 - val_acc: 0.8000\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 28s 284ms/step - loss: 0.2759 - acc: 0.9000 - val_loss: 0.4474 - val_acc: 0.8500\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 29s 289ms/step - loss: 0.2409 - acc: 0.9050 - val_loss: 0.6939 - val_acc: 0.7500\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 29s 290ms/step - loss: 0.1774 - acc: 0.9225 - val_loss: 0.3320 - val_acc: 0.9000\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 29s 292ms/step - loss: 0.1271 - acc: 0.9575 - val_loss: 0.2630 - val_acc: 0.9000\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 29s 294ms/step - loss: 0.1232 - acc: 0.9550 - val_loss: 0.5654 - val_acc: 0.9000\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 28s 285ms/step - loss: 0.1940 - acc: 0.9275 - val_loss: 0.2524 - val_acc: 0.8500\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 29s 292ms/step - loss: 0.1800 - acc: 0.9400 - val_loss: 0.3055 - val_acc: 0.9000\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 30s 298ms/step - loss: 0.1784 - acc: 0.9250 - val_loss: 0.4122 - val_acc: 0.9000\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 30s 296ms/step - loss: 0.2616 - acc: 0.9250 - val_loss: 0.2601 - val_acc: 0.9500\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 29s 289ms/step - loss: 0.1581 - acc: 0.9525 - val_loss: 0.1903 - val_acc: 0.9000\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 29s 294ms/step - loss: 0.1470 - acc: 0.9500 - val_loss: 0.1947 - val_acc: 0.9000\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 30s 299ms/step - loss: 0.0999 - acc: 0.9550 - val_loss: 0.1611 - val_acc: 0.9000\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 30s 297ms/step - loss: 0.1255 - acc: 0.9525 - val_loss: 0.1251 - val_acc: 0.9000\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 30s 301ms/step - loss: 0.1524 - acc: 0.9450 - val_loss: 0.2925 - val_acc: 0.8500\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.1517 - acc: 0.9375 - val_loss: 0.8705 - val_acc: 0.8000\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 30s 304ms/step - loss: 0.1812 - acc: 0.9250 - val_loss: 0.5122 - val_acc: 0.9000\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 31s 309ms/step - loss: 0.1786 - acc: 0.9400 - val_loss: 0.1558 - val_acc: 0.9000\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 30s 304ms/step - loss: 0.1297 - acc: 0.9600 - val_loss: 0.1982 - val_acc: 0.8500\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 30s 297ms/step - loss: 0.1540 - acc: 0.9300 - val_loss: 0.2118 - val_acc: 0.8500\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 29s 290ms/step - loss: 0.1298 - acc: 0.9600 - val_loss: 0.1124 - val_acc: 0.9000\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 31s 306ms/step - loss: 0.1156 - acc: 0.9600 - val_loss: 0.1964 - val_acc: 0.9000\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 30s 305ms/step - loss: 0.1123 - acc: 0.9575 - val_loss: 0.0761 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 30s 304ms/step - loss: 0.1552 - acc: 0.9350 - val_loss: 0.0973 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - 29s 295ms/step - loss: 0.1524 - acc: 0.9250 - val_loss: 0.2135 - val_acc: 0.9000\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 29s 288ms/step - loss: 0.1657 - acc: 0.9500 - val_loss: 0.2252 - val_acc: 0.9000\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - 31s 309ms/step - loss: 0.1667 - acc: 0.9525 - val_loss: 0.0748 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 30s 304ms/step - loss: 0.1335 - acc: 0.9500 - val_loss: 0.0949 - val_acc: 0.9500\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 30s 304ms/step - loss: 0.0993 - acc: 0.9625 - val_loss: 0.0918 - val_acc: 0.9500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3cd3ccf9b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb_list = [tf.keras.callbacks.ReduceLROnPlateau(factor=0.5,\n",
    "                                                patience=4,\n",
    "                                                min_lr=1e-12),\n",
    "           tf.keras.callbacks.EarlyStopping(min_delta = 1e-4, \n",
    "                                            patience= 50)\n",
    "          ]\n",
    "\n",
    "model.fit_generator(train_gen,\n",
    "                    epochs=FLAGS.epochs,\n",
    "                    steps_per_epoch=FLAGS.n_batch, \n",
    "                    validation_data=(x_val, y_val),\n",
    "                    #callbacks=cb_list\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_worker_stop\n"
     ]
    }
   ],
   "source": [
    "train_gen.stop_worker()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {
    "6b1438b3075f49289cfcf03a4fce2ccb": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "ab7848b490e5454e9a42f439a6ef6b31": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "e0e3b0d66b1e47c4ade6c276bacc5c55": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
