{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do experiments with cats/dogs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description=\"Cats/Dogs playground\")\n",
    "parser.add_argument(\n",
    "    \"--config-file\",\n",
    "    default=\"\",\n",
    "    metavar=\"FILE\",\n",
    "    help=\"path to config file\",\n",
    "    type=str,\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"opts\",\n",
    "        help=\"Modify config options using the command-line\",\n",
    "        default=None,\n",
    "        nargs=argparse.REMAINDER,\n",
    "    )\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "import cv2\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.config_file = \"configs/R50v1_BN_BZ1_Pretrain.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augmentation_Setup(object):  \n",
    "    sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "    lesstimes = lambda aug: iaa.Sometimes(0.2, aug)\n",
    "    \n",
    "    augmentation = iaa.Sequential([\n",
    "        iaa.Fliplr(0.5, name=\"FlipLR\"),\n",
    "        iaa.Flipud(0.5, name=\"FlipUD\"),\n",
    "        iaa.OneOf([iaa.Affine(rotate = 90),\n",
    "                   iaa.Affine(rotate = 180),\n",
    "                   iaa.Affine(rotate = 270)]),\n",
    "        sometimes(iaa.Affine(\n",
    "                    scale = (0.8,1.2),\n",
    "                    translate_percent = (-0.2, 0.2),\n",
    "                    rotate = (-15, 15),\n",
    "                    mode = 'wrap'\n",
    "                    ))\n",
    "    ])\n",
    "\n",
    "def preproc(img):\n",
    "    #return (img - img.min()) / (img.max() - img.min())\n",
    "    return img / 255.\n",
    "    \n",
    "### DONT CHANGE IF NOT TESTING FOR DATASET LOADER ###\n",
    "class GetDataset():\n",
    "    def __init__(self, df_list, class_id, n_classes, f_input_preproc, image_size=(256,256,3), onehot=True, augmentation=None):\n",
    "        \n",
    "        self.df_list = df_list\n",
    "        self.class_id = class_id\n",
    "        self.n_classes = n_classes\n",
    "        self.preproc = f_input_preproc\n",
    "        self.image_size = image_size\n",
    "        self.onehot = onehot\n",
    "        self.aug = augmentation\n",
    "        \n",
    "        ## Init ##\n",
    "        self.df_list = self.df_list.sample(frac=1.).reset_index(drop=True)\n",
    "        self.current_index = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img = self.load_image(img_path=self.df_list.iloc[self.current_index]['img_path'], image_size=self.image_size)\n",
    "        \n",
    "        if self.aug is not None:\n",
    "            img = self.aug.augment_image(img)\n",
    "            \n",
    "        img = img.astype(np.float32)\n",
    "        \n",
    "        if self.preproc is not None:\n",
    "            img = self.preproc(img)\n",
    "        \n",
    "        label = self.df_list.iloc[self.current_index]['cate']\n",
    "        if self.onehot:\n",
    "             label = tf.keras.utils.to_categorical(label, num_classes=self.n_classes)\n",
    "        \n",
    "        self.current_index = (self.current_index + 1) % len(self.df_list)\n",
    "        return img, label\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.__getitem__(idx=self.current_index)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_image(img_path, image_size):\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (image_size[0], image_size[1]))\n",
    "        return img\n",
    "    \n",
    "class Customized_dataloader():\n",
    "    \"\"\"\n",
    "    1. Compose multiple generators together\n",
    "    2. Make this composed generator into multi-processing function\n",
    "    \"\"\"\n",
    "    def __init__(self, list_dataset, batch_size_per_dataset=16, queue_size=128, num_workers=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - list_dataset: put generator object as list [gen1, gen2, ...]\n",
    "            - batch_size_per_dataset: bz for each generator (total_batch_size/n_class)\n",
    "            - queue_size: queue size\n",
    "            - num_workers: start n workers to get data\n",
    "        \n",
    "        Action: Call with next\n",
    "        \"\"\"\n",
    "        self.list_dataset = list_dataset\n",
    "        self.batch_size_per_dataset = batch_size_per_dataset\n",
    "        self.sample_queue = mp.Queue(maxsize = queue_size)\n",
    "        \n",
    "        self.jobs = num_workers\n",
    "        self.events = list()\n",
    "        self.workers = list()\n",
    "        for i in range(num_workers):\n",
    "            event = Event()\n",
    "            work = mp.Process(target = enqueue, args = (self.sample_queue, event, self.compose_data))\n",
    "            work.daemon = True\n",
    "            work.start()\n",
    "            self.events.append(event)\n",
    "            self.workers.append(work)\n",
    "        print(\"workers ready\")\n",
    "        \n",
    "    def __next__(self):\n",
    "        return self.sample_queue.get()\n",
    "    \n",
    "    def compose_data(self):\n",
    "        while True:\n",
    "            imgs, labels = [], []\n",
    "            for z in range(self.batch_size_per_dataset):\n",
    "                data = [next(i) for i in self.list_dataset]\n",
    "                img, label = zip(*data)\n",
    "                imgs.append(np.array(img))\n",
    "                labels.append(np.array(label))\n",
    "            yield np.concatenate(imgs), np.concatenate(labels)\n",
    "    \n",
    "    def stop_worker(self):\n",
    "        for t in self.events:\n",
    "            t.set()\n",
    "        for i, t in enumerate(self.workers):\n",
    "            t.join(timeout = 1)\n",
    "        print(\"all_worker_stop\")\n",
    "\n",
    "# ----- #\n",
    "def enqueue(queue, stop, gen_func):\n",
    "    gen = gen_func()\n",
    "    while True:\n",
    "        if stop.is_set():\n",
    "            return\n",
    "        queue.put(next(gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET:\n",
      "  TEST: /data/seanyu/cat_dog/dataset/test1/\n",
      "  TRAIN: /data/seanyu/cat_dog/dataset/train/\n",
      "MODEL:\n",
      "  BACKBONE: R-50-v1\n",
      "  NORM_USE: bn\n",
      "  OPTIMIZER: Adam\n",
      "  USE_PRETRAIN: True\n",
      "SYSTEM:\n",
      "  BACKBONE_PATH: /home/seanyu/research/deep-learning-experiments/cats_and_dogs_playground/experimental_stuff\n",
      "  GPU_ID: 1\n",
      "  NAME_FLAG: R50v1_BN_BZ1_Pretrain\n",
      "  NUM_WORKERS: 4\n",
      "  QUEUE_SIZE: 50\n",
      "  RESULT_DIR: ./results/\n",
      "TRAIN:\n",
      "  BATCH_SIZE: 1\n",
      "  EPOCHS: 50\n",
      "  IMAGE_SIZE: (256, 256, 3)\n",
      "  LR: 0.0001\n",
      "  NUM_CLASSES: 2\n",
      "  NUM_UPDATES: 2000\n",
      "  TRAIN_RATIO: 0.9\n",
      "  USE_RESNET_PREPROC: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workers ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:20<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_worker_stop\n",
      "(1600, 256, 256, 3)\n",
      "(1600, 2)\n",
      "[812. 788.]\n"
     ]
    }
   ],
   "source": [
    "from default import get_cfg_defaults\n",
    "cfg = get_cfg_defaults()\n",
    "cfg.merge_from_file(args.config_file)\n",
    "cfg.merge_from_list(args.opts)\n",
    "cfg.freeze()\n",
    "sys.path.append(cfg.SYSTEM.BACKBONE_PATH)\n",
    "from model import build_model, parse_model_fn, make_optimizer, preproc\n",
    "from data_generator import GetDataset, Customized_dataloader\n",
    "print(cfg)\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(cfg.SYSTEM.GPU_ID)\n",
    "\n",
    "\"\"\"  Get data \"\"\"\n",
    "image_train_list = glob.glob(cfg.DATASET.TRAIN + '*.jpg')\n",
    "image_test_list = glob.glob(cfg.DATASET.TEST + '*.jpg')\n",
    "\n",
    "df_train = pd.DataFrame({'img_path': image_train_list})\n",
    "df_test = pd.DataFrame({'img_path': image_test_list})\n",
    "\n",
    "df_train['cate'] = df_train.img_path.apply(os.path.basename)\n",
    "df_train['cate'] = [i.split(\".\")[0] for i in list(df_train.cate)]\n",
    "df_train.cate = df_train.cate.replace({'dog': 0, 'cat': 1})\n",
    "\n",
    "df_train_0, df_val_0 = train_test_split(df_train[df_train['cate'] == 0], test_size = 1-cfg.TRAIN.TRAIN_RATIO)\n",
    "df_train_1, df_val_1 = train_test_split(df_train[df_train['cate'] == 1], test_size = 1-cfg.TRAIN.TRAIN_RATIO)\n",
    "df_val = pd.concat((df_val_0, df_val_1)).reset_index(drop = True)\n",
    "\n",
    "del df_val_0, df_val_1\n",
    "\n",
    "USE_RESNET_PREPROC = cfg.TRAIN.USE_RESNET_PREPROC\n",
    "dtrain = GetDataset(df_list=df_train,\n",
    "                    class_id=0, n_classes=2,\n",
    "                    f_input_preproc=preproc if not USE_RESNET_PREPROC else tf.keras.applications.resnet50.preprocess_input,\n",
    "                    augmentation=Augmentation_Setup.augmentation, \n",
    "                    onehot= True, \n",
    "                    image_size=cfg.TRAIN.IMAGE_SIZE)\n",
    "\n",
    "dvalid = GetDataset(df_list=df_val, \n",
    "                    class_id=0, n_classes=2,\n",
    "                    f_input_preproc=preproc if not USE_RESNET_PREPROC else tf.keras.applications.resnet50.preprocess_input,\n",
    "                    augmentation=None, \n",
    "                    onehot= True, \n",
    "                    image_size=cfg.TRAIN.IMAGE_SIZE)\n",
    "\n",
    "valid_gen = Customized_dataloader([dvalid], batch_size_per_dataset=16, num_workers=1)\n",
    "x_val, y_val = [], []\n",
    "for _ in tqdm(range(100)):\n",
    "    a,b = next(valid_gen)\n",
    "    x_val.append(a)\n",
    "    y_val.append(b)\n",
    "x_val = np.concatenate(x_val)\n",
    "y_val = np.concatenate(y_val)\n",
    "valid_gen.stop_worker()\n",
    "\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "print(y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "Process Process-3:\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workers ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/seanyu/research/deep-learning-experiments/cats_and_dogs_playground/data_generator.py\", line 116, in enqueue\n",
      "    queue.put(next(gen))\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/seanyu/research/deep-learning-experiments/cats_and_dogs_playground/data_generator.py\", line 101, in compose_data\n",
      "    yield np.concatenate(imgs), np.concatenate(labels)\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "ValueError: need at least one array to concatenate\n",
      "  File \"/home/seanyu/research/deep-learning-experiments/cats_and_dogs_playground/data_generator.py\", line 116, in enqueue\n",
      "    queue.put(next(gen))\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-5:\n",
      "  File \"/home/seanyu/research/deep-learning-experiments/cats_and_dogs_playground/data_generator.py\", line 101, in compose_data\n",
      "    yield np.concatenate(imgs), np.concatenate(labels)\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "ValueError: need at least one array to concatenate\n",
      "  File \"/home/seanyu/research/deep-learning-experiments/cats_and_dogs_playground/data_generator.py\", line 116, in enqueue\n",
      "    queue.put(next(gen))\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seanyu/research/deep-learning-experiments/cats_and_dogs_playground/data_generator.py\", line 101, in compose_data\n",
      "    yield np.concatenate(imgs), np.concatenate(labels)\n",
      "ValueError: need at least one array to concatenate\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/seanyu/research/deep-learning-experiments/cats_and_dogs_playground/data_generator.py\", line 116, in enqueue\n",
      "    queue.put(next(gen))\n",
      "  File \"/home/seanyu/research/deep-learning-experiments/cats_and_dogs_playground/data_generator.py\", line 101, in compose_data\n",
      "    yield np.concatenate(imgs), np.concatenate(labels)\n",
      "ValueError: need at least one array to concatenate\n"
     ]
    }
   ],
   "source": [
    "train_gen = Customized_dataloader([dtrain], \n",
    "                                  batch_size_per_dataset=cfg.TRAIN.BATCH_SIZE // cfg.TRAIN.NUM_CLASSES, \n",
    "                                  num_workers=cfg.SYSTEM.NUM_WORKERS, \n",
    "                                  queue_size=cfg.SYSTEM.QUEUE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "model = build_model(model_fn=parse_model_fn(cfg.MODEL.BACKBONE), norm_use=cfg.MODEL.NORM_USE)\n",
    "optim = make_optimizer(cfg)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              metrics=[\"accuracy\"], \n",
    "              optimizer=optim)\n",
    "model.summary()\n",
    "cb_list = [tf.keras.callbacks.ReduceLROnPlateau(factor=0.5,\n",
    "                                            patience=4,\n",
    "                                            min_lr=1e-12),\n",
    "      ]\n",
    "model.fit_generator(train_gen,\n",
    "                    epochs=cfg.TRAIN.EPOCHS,\n",
    "                    steps_per_epoch=cfg.TRAIN.NUM_UPDATES, \n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=cb_list\n",
    "                    )\n",
    "\n",
    "train_loss = model.history.history['loss']\n",
    "valid_loss = model.history.history['val_loss']\n",
    "train_acc = model.history.history['acc']\n",
    "valid_acc = model.history.history['val_acc']\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(range(len(train_loss)), train_loss, label='train_loss')\n",
    "plt.plot(range(len(valid_loss)), valid_loss, label='valid_loss')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(\"results\", \"exp_\" + cfg.SYSTEM.NAME_FLAG + \"_loss.png\"))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(range(len(train_acc)), train_acc, label='train_accuracy')\n",
    "plt.plot(range(len(valid_acc)), valid_acc, label='valid_accuracy')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(\"results\", \"exp_\" + cfg.SYSTEM.NAME_FLAG + \"_acc.png\"))\n",
    "\n",
    "result_df = pd.DataFrame({\"train_loss\":train_loss,\n",
    "                          \"valid_loss\":valid_loss,\n",
    "                          \"train_acc\":train_acc,\n",
    "                          \"valid_acc\":valid_acc\n",
    "                         })\n",
    "result_df.to_csv(os.path.join(\"results\", \"exp_\" + cfg.SYSTEM.NAME_FLAG + \"_result.csv\"), index=False)\n",
    "print(\"All Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import get_cfg_defaults\n",
    "import yaml\n",
    "\n",
    "def quoted_presenter(dumper, data):\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='\"')\n",
    "yaml.add_representer(str, quoted_presenter)\n",
    "\n",
    "cfg = get_cfg_defaults()\n",
    "cfg.TRAIN.EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SYSTEM': {'NUM_WORKERS': 2,\n",
       "  'QUEUE_SIZE': 50,\n",
       "  'GPU_ID': 0,\n",
       "  'RESULT_DIR': './results/',\n",
       "  'NAME_FLAG': 'default',\n",
       "  'BACKBONE_PATH': ''},\n",
       " 'DATASET': CfgNode({'TRAIN': '/data/seanyu/cat_dog/dataset/train/', 'TEST': '/data/seanyu/cat_dog/dataset/test1/'}),\n",
       " 'TRAIN': CfgNode({'TRAIN_RATIO': 0.9, 'IMAGE_SIZE': (256, 256, 3), 'NUM_CLASSES': 2, 'BATCH_SIZE': 32, 'NUM_UPDATES': 2000, 'EPOCHS': 50, 'LR': 0.0001, 'USE_RESNET_PREPROC': True}),\n",
       " 'MODEL': CfgNode({'BACKBONE': 'R-50-v1', 'USE_PRETRAIN': True, 'NORM_USE': 'bn', 'OPTIMIZER': 'SGD', 'EPOCHS': 100}),\n",
       " 'EXPERIMENT': CfgNode({'ABC': 1, 'DEF': CfgNode({'RGB': 'test'})})}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = dict(cfg)\n",
    "tmp['SYSTEM'] = dict(tmp['SYSTEM'])\n",
    "tmp\n",
    "\n",
    "#isinstance(tmp['SYSTEM'], dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.yaml\", \"w\") as f:\n",
    "    f.write(cfg.dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfg_to_yaml(cfg, output_name):\n",
    "    \"\"\"Write yacs config to file\n",
    "    \n",
    "    Args:\n",
    "      cfg (object): yacs configuration\n",
    "      output_name (str): full path to save the yaml, ex: ~/path_to_result/train_conidtion.yaml\n",
    "    Returns:\n",
    "      no return\n",
    "    \"\"\"\n",
    "    with open(output_name, \"w\") as f:\n",
    "        f.write(cfg.dump())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET:\n",
      "  TEST: /data/seanyu/cat_dog/dataset/test1/\n",
      "  TRAIN: /data/seanyu/cat_dog/dataset/train/\n",
      "EXPERIMENT:\n",
      "  ABC: 1\n",
      "  DEF:\n",
      "    RGB: test\n",
      "MODEL:\n",
      "  BACKBONE: R-50-v1\n",
      "  NORM_USE: bn\n",
      "  OPTIMIZER: SGD\n",
      "  USE_PRETRAIN: True\n",
      "SYSTEM:\n",
      "  BACKBONE_PATH: \n",
      "  GPU_ID: 0\n",
      "  NAME_FLAG: default\n",
      "  NUM_WORKERS: 2\n",
      "  QUEUE_SIZE: 50\n",
      "  RESULT_DIR: ./results/\n",
      "TRAIN:\n",
      "  BATCH_SIZE: 32\n",
      "  EPOCHS: 100\n",
      "  IMAGE_SIZE: (256, 256, 3)\n",
      "  LR: 0.0001\n",
      "  NUM_CLASSES: 2\n",
      "  NUM_UPDATES: 2000\n",
      "  TRAIN_RATIO: 0.9\n",
      "  USE_RESNET_PREPROC: True\n"
     ]
    }
   ],
   "source": [
    "from default import get_cfg_defaults\n",
    "cfg = get_cfg_defaults()\n",
    "cfg.merge_from_file(\"test.yaml\")\n",
    "print(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
