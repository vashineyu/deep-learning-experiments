{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do experiments with cats/dogs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Cats/Dogs playground\")\n",
    "parser.add_argument(\n",
    "    \"--config-file\",\n",
    "    default=None,\n",
    "    metavar=\"FILE\",\n",
    "    help=\"path to config file\",\n",
    "    type=str,\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"opts\",\n",
    "        help=\"Modify config options using the command-line\",\n",
    "        default=None,\n",
    "        nargs=argparse.REMAINDER,\n",
    "    )\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET:\n",
      "  TEST: /data/seanyu/cat_dog/dataset/test1/\n",
      "  TRAIN: /data/seanyu/cat_dog/dataset/train/\n",
      "EXPERIMENT:\n",
      "  ABC: 1\n",
      "  DEF:\n",
      "    RGB: test\n",
      "MODEL:\n",
      "  BACKBONE: R-50-v2\n",
      "  NORM_USE: bn\n",
      "  OPTIMIZER: Adam\n",
      "  USE_PRETRAIN: True\n",
      "SYSTEM:\n",
      "  BACKBONE_PATH: /home/seanyu/research/deep-learning-experiments/cats_and_dogs_playground/experimental_stuff\n",
      "  DEVICES: [0]\n",
      "  NAME_FLAG: default\n",
      "  NUM_WORKERS: 2\n",
      "  QUEUE_SIZE: 50\n",
      "  RESULT_DIR: ./results/\n",
      "TRAIN:\n",
      "  BATCH_SIZE: 32\n",
      "  EPOCHS: 50\n",
      "  IMAGE_SIZE: (256, 256, 3)\n",
      "  LR: 0.0001\n",
      "  NUM_CLASSES: 2\n",
      "  NUM_UPDATES: 500\n",
      "  TRAIN_RATIO: 0.9\n",
      "  USE_RESNET_PREPROC: True\n"
     ]
    }
   ],
   "source": [
    "from config import get_cfg_defaults\n",
    "cfg = get_cfg_defaults()\n",
    "if args.config_file is not None:\n",
    "    cfg.merge_from_file(args.config_file)\n",
    "if args.opts is not None:\n",
    "    cfg.merge_from_list(args.opts)\n",
    "cfg.freeze()\n",
    "sys.path.append(cfg.SYSTEM.BACKBONE_PATH)\n",
    "print(cfg)\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "device = ','.join(str(i) for i in cfg.SYSTEM.DEVICES)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Customized functions\n",
    "\"\"\"\n",
    "# model.py\n",
    "import tensorflow as tf\n",
    "from backbone import *\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.models as models\n",
    "import tensorflow.nn as F\n",
    "\n",
    "graph_mapping = {\n",
    "    \"R-50-v1\":ResNet50,\n",
    "    \"R-101-v1\":ResNet101,\n",
    "    \"R-152-v1\":ResNet152,\n",
    "    \"R-50-v2\":ResNet50V2,\n",
    "    \"R-101-v2\":ResNet101V2,\n",
    "    \"R-152-v2\":ResNet152V2,\n",
    "    \"R-50-xt\":ResNeXt50,\n",
    "    \"R-101-xt\":ResNeXt101}\n",
    "\n",
    "class Main_model(models.Model):\n",
    "    def __init__(self, num_class=2, **kwargs):\n",
    "        super(Main_model, self).__init__(**kwargs)\n",
    "        \n",
    "        self.gap_layer = layers.GlobalAveragePooling2D(name=\"GAP_layer\")\n",
    "        self.output_layer = layers.Dense(units=num_class)\n",
    "        \n",
    "        \n",
    "    def call(self, x, training=True):\n",
    "        x = self.gap_layer(x)\n",
    "        output = self.output_layer(x)       \n",
    "        return F.softmax(output)\n",
    "\n",
    "def entropy_loss(y_true, y_pred):\n",
    "    return tf.keras.losses.categorical_crossentropy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "module = graph_mapping[cfg.MODEL.BACKBONE](input_shape=(256,256,3), include_top=False, weights=\"imagenet\", norm_use=\"bn\")\n",
    "\"\"\"\n",
    "xx = layers.GlobalAveragePooling2D()(module.output)\n",
    "xx = layers.Dense(units=2, activation=\"softmax\")(xx)\n",
    "model = models.Model(inputs=[module.input], outputs=[xx])\n",
    "\"\"\"\n",
    "m_head = Main_model(num_class=2)(module.output)\n",
    "model = models.Model(inputs=[module.input], outputs=[m_head])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub-functions\n",
    "from model import build_model, make_optimizer, preproc\n",
    "from augment import Augmentation_Setup\n",
    "from data_generator import GetDataset, Customized_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:00<00:17,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workers ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  8.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_worker_stop\n",
      "(1600, 256, 256, 3)\n",
      "(1600, 2)\n",
      "[815. 785.]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"  Get data \"\"\"\n",
    "image_train_list = glob.glob(cfg.DATASET.TRAIN + '*.jpg')\n",
    "image_test_list = glob.glob(cfg.DATASET.TEST + '*.jpg')\n",
    "\n",
    "df_train = pd.DataFrame({'img_path': image_train_list})\n",
    "df_test = pd.DataFrame({'img_path': image_test_list})\n",
    "\n",
    "df_train['cate'] = df_train.img_path.apply(os.path.basename)\n",
    "df_train['cate'] = [i.split(\".\")[0] for i in list(df_train.cate)]\n",
    "df_train.cate = df_train.cate.replace({'dog': 0, 'cat': 1})\n",
    "\n",
    "df_train_0, df_val_0 = train_test_split(df_train[df_train['cate'] == 0], test_size = 1-cfg.TRAIN.TRAIN_RATIO)\n",
    "df_train_1, df_val_1 = train_test_split(df_train[df_train['cate'] == 1], test_size = 1-cfg.TRAIN.TRAIN_RATIO)\n",
    "df_val = pd.concat((df_val_0, df_val_1)).reset_index(drop = True)\n",
    "\n",
    "del df_val_0, df_val_1\n",
    "\n",
    "USE_RESNET_PREPROC = cfg.TRAIN.USE_RESNET_PREPROC\n",
    "dtrain = GetDataset(df_list=df_train,\n",
    "                    class_id=0, n_classes=2,\n",
    "                    f_input_preproc=preproc if not USE_RESNET_PREPROC else tf.keras.applications.resnet50.preprocess_input,\n",
    "                    augmentation=Augmentation_Setup.augmentation, \n",
    "                    onehot= True, \n",
    "                    image_size=cfg.TRAIN.IMAGE_SIZE)\n",
    "\n",
    "dvalid = GetDataset(df_list=df_val, \n",
    "                    class_id=0, n_classes=2,\n",
    "                    f_input_preproc=preproc if not USE_RESNET_PREPROC else tf.keras.applications.resnet50.preprocess_input,\n",
    "                    augmentation=None, \n",
    "                    onehot= True, \n",
    "                    image_size=cfg.TRAIN.IMAGE_SIZE)\n",
    "\n",
    "valid_gen = Customized_dataloader([dvalid], batch_size_per_dataset=16, num_workers=1)\n",
    "x_val, y_val = [], []\n",
    "for _ in tqdm(range(100)):\n",
    "    a,b = next(valid_gen)\n",
    "    x_val.append(a)\n",
    "    y_val.append(b)\n",
    "x_val = np.concatenate(x_val)\n",
    "y_val = np.concatenate(y_val)\n",
    "valid_gen.stop_worker()\n",
    "\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "print(y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workers ready\n"
     ]
    }
   ],
   "source": [
    "train_gen = Customized_dataloader([dtrain], \n",
    "                                  batch_size_per_dataset=cfg.TRAIN.BATCH_SIZE // cfg.TRAIN.NUM_CLASSES, \n",
    "                                  num_workers=cfg.SYSTEM.NUM_WORKERS, \n",
    "                                  queue_size=cfg.SYSTEM.QUEUE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 262, 262, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 128, 128, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 130, 130, 64) 0           conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 64, 64, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_preact_bn (BatchNo (None, 64, 64, 64)   256         pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_preact_relu (Activ (None, 64, 64, 64)   0           conv2_block1_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 64, 64, 64)   4096        conv2_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 64, 64, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_pad (ZeroPadding (None, 66, 66, 64)   0           conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 64, 64, 64)   36864       conv2_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 64, 64, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Add)          (None, 64, 64, 256)  0           conv2_block1_0_conv[0][0]        \n",
      "                                                                 conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_preact_bn (BatchNo (None, 64, 64, 256)  1024        conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_preact_relu (Activ (None, 64, 64, 256)  0           conv2_block2_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 64, 64, 64)   16384       conv2_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 64, 64, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_pad (ZeroPadding (None, 66, 66, 64)   0           conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 64, 64, 64)   36864       conv2_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 64, 64, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Add)          (None, 64, 64, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_preact_bn (BatchNo (None, 64, 64, 256)  1024        conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_preact_relu (Activ (None, 64, 64, 256)  0           conv2_block3_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 64, 64, 64)   16384       conv2_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 64, 64, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_pad (ZeroPadding (None, 66, 66, 64)   0           conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 32, 32, 64)   36864       conv2_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 32, 32, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 32, 32, 256)  0           conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Add)          (None, 32, 32, 256)  0           max_pooling2d[0][0]              \n",
      "                                                                 conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_preact_bn (BatchNo (None, 32, 32, 256)  1024        conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_preact_relu (Activ (None, 32, 32, 256)  0           conv3_block1_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 32, 32, 128)  32768       conv3_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 32, 32, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_pad (ZeroPadding (None, 34, 34, 128)  0           conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 32, 32, 128)  147456      conv3_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 32, 32, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 32, 32, 512)  131584      conv3_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Add)          (None, 32, 32, 512)  0           conv3_block1_0_conv[0][0]        \n",
      "                                                                 conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_preact_bn (BatchNo (None, 32, 32, 512)  2048        conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_preact_relu (Activ (None, 32, 32, 512)  0           conv3_block2_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 32, 32, 128)  65536       conv3_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 32, 32, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_pad (ZeroPadding (None, 34, 34, 128)  0           conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 32, 32, 128)  147456      conv3_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 32, 32, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Add)          (None, 32, 32, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_preact_bn (BatchNo (None, 32, 32, 512)  2048        conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_preact_relu (Activ (None, 32, 32, 512)  0           conv3_block3_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 32, 32, 128)  65536       conv3_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 32, 32, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_pad (ZeroPadding (None, 34, 34, 128)  0           conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 32, 32, 128)  147456      conv3_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 32, 32, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Add)          (None, 32, 32, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_preact_bn (BatchNo (None, 32, 32, 512)  2048        conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_preact_relu (Activ (None, 32, 32, 512)  0           conv3_block4_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 32, 32, 128)  65536       conv3_block4_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 32, 32, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_pad (ZeroPadding (None, 34, 34, 128)  0           conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 16, 16, 128)  147456      conv3_block4_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 16, 16, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 512)  0           conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Add)          (None, 16, 16, 512)  0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_preact_bn (BatchNo (None, 16, 16, 512)  2048        conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_preact_relu (Activ (None, 16, 16, 512)  0           conv4_block1_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 16, 16, 256)  131072      conv4_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 16, 16, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_pad (ZeroPadding (None, 18, 18, 256)  0           conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 16, 16, 256)  589824      conv4_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 16, 16, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 16, 16, 1024) 525312      conv4_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Add)          (None, 16, 16, 1024) 0           conv4_block1_0_conv[0][0]        \n",
      "                                                                 conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_preact_bn (BatchNo (None, 16, 16, 1024) 4096        conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_preact_relu (Activ (None, 16, 16, 1024) 0           conv4_block2_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 16, 16, 256)  262144      conv4_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 16, 16, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_pad (ZeroPadding (None, 18, 18, 256)  0           conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 16, 16, 256)  589824      conv4_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 16, 16, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Add)          (None, 16, 16, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_preact_bn (BatchNo (None, 16, 16, 1024) 4096        conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_preact_relu (Activ (None, 16, 16, 1024) 0           conv4_block3_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 16, 16, 256)  262144      conv4_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 16, 16, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_pad (ZeroPadding (None, 18, 18, 256)  0           conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 16, 16, 256)  589824      conv4_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 16, 16, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Add)          (None, 16, 16, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_preact_bn (BatchNo (None, 16, 16, 1024) 4096        conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_preact_relu (Activ (None, 16, 16, 1024) 0           conv4_block4_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 16, 16, 256)  262144      conv4_block4_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 16, 16, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_pad (ZeroPadding (None, 18, 18, 256)  0           conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 16, 16, 256)  589824      conv4_block4_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 16, 16, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Add)          (None, 16, 16, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_preact_bn (BatchNo (None, 16, 16, 1024) 4096        conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_preact_relu (Activ (None, 16, 16, 1024) 0           conv4_block5_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 16, 16, 256)  262144      conv4_block5_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 16, 16, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_pad (ZeroPadding (None, 18, 18, 256)  0           conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 16, 16, 256)  589824      conv4_block5_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 16, 16, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Add)          (None, 16, 16, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_preact_bn (BatchNo (None, 16, 16, 1024) 4096        conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_preact_relu (Activ (None, 16, 16, 1024) 0           conv4_block6_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 16, 16, 256)  262144      conv4_block6_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 16, 16, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_pad (ZeroPadding (None, 18, 18, 256)  0           conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 8, 8, 256)    589824      conv4_block6_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 8, 8, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 1024)   0           conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Add)          (None, 8, 8, 1024)   0           max_pooling2d_2[0][0]            \n",
      "                                                                 conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_preact_bn (BatchNo (None, 8, 8, 1024)   4096        conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_preact_relu (Activ (None, 8, 8, 1024)   0           conv5_block1_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 8, 8, 512)    524288      conv5_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 8, 8, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_pad (ZeroPadding (None, 10, 10, 512)  0           conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 8, 8, 512)    2359296     conv5_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 8, 8, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 8, 8, 2048)   2099200     conv5_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Add)          (None, 8, 8, 2048)   0           conv5_block1_0_conv[0][0]        \n",
      "                                                                 conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_preact_bn (BatchNo (None, 8, 8, 2048)   8192        conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_preact_relu (Activ (None, 8, 8, 2048)   0           conv5_block2_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 8, 8, 512)    1048576     conv5_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 8, 8, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_pad (ZeroPadding (None, 10, 10, 512)  0           conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 8, 8, 512)    2359296     conv5_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 8, 8, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Add)          (None, 8, 8, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_preact_bn (BatchNo (None, 8, 8, 2048)   8192        conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_preact_relu (Activ (None, 8, 8, 2048)   0           conv5_block3_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 8, 8, 512)    1048576     conv5_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 8, 8, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_pad (ZeroPadding (None, 10, 10, 512)  0           conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 8, 8, 512)    2359296     conv5_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 8, 8, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Add)          (None, 8, 8, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "post_bn (BatchNormalization)    (None, 8, 8, 2048)   8192        conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "post_relu (Activation)          (None, 8, 8, 2048)   0           post_bn[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "main_model (Main_model)         (None, 2)            4098        post_relu[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 23,568,898\n",
      "Trainable params: 23,523,458\n",
      "Non-trainable params: 45,440\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "500/500 [==============================] - 112s 225ms/step - loss: 0.1424 - acc: 0.9470 - val_loss: 0.2330 - val_acc: 0.9244\n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 124s 248ms/step - loss: 0.1379 - acc: 0.9499 - val_loss: 0.1390 - val_acc: 0.9450\n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 114s 227ms/step - loss: 0.1216 - acc: 0.9544 - val_loss: 0.1356 - val_acc: 0.9575\n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 103s 205ms/step - loss: 0.0916 - acc: 0.9675 - val_loss: 0.0945 - val_acc: 0.9600\n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 124s 248ms/step - loss: 0.0894 - acc: 0.9677 - val_loss: 0.0975 - val_acc: 0.9644\n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 115s 230ms/step - loss: 0.1166 - acc: 0.9629 - val_loss: 0.1568 - val_acc: 0.9400\n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 103s 206ms/step - loss: 0.0928 - acc: 0.9701 - val_loss: 0.0984 - val_acc: 0.9656\n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 122s 243ms/step - loss: 0.0666 - acc: 0.9758 - val_loss: 0.1033 - val_acc: 0.9625\n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 114s 228ms/step - loss: 0.0648 - acc: 0.9785 - val_loss: 0.0834 - val_acc: 0.9644\n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 102s 205ms/step - loss: 0.0452 - acc: 0.9832 - val_loss: 0.0694 - val_acc: 0.9756\n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 118s 236ms/step - loss: 0.0531 - acc: 0.9811 - val_loss: 0.0580 - val_acc: 0.9800\n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 120s 240ms/step - loss: 0.0532 - acc: 0.9834 - val_loss: 0.0630 - val_acc: 0.9775\n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 101s 201ms/step - loss: 0.0463 - acc: 0.9839 - val_loss: 0.0747 - val_acc: 0.9719\n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 115s 230ms/step - loss: 0.0428 - acc: 0.9846 - val_loss: 0.0900 - val_acc: 0.9725\n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 121s 243ms/step - loss: 0.0422 - acc: 0.9848 - val_loss: 0.0709 - val_acc: 0.9688\n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 102s 204ms/step - loss: 0.0371 - acc: 0.9860 - val_loss: 0.0487 - val_acc: 0.9769\n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 111s 222ms/step - loss: 0.0333 - acc: 0.9869 - val_loss: 0.0442 - val_acc: 0.9819\n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 127s 253ms/step - loss: 0.0383 - acc: 0.9864 - val_loss: 0.0474 - val_acc: 0.9806\n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 102s 204ms/step - loss: 0.0289 - acc: 0.9889 - val_loss: 0.0604 - val_acc: 0.9762\n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 106s 211ms/step - loss: 0.0267 - acc: 0.9906 - val_loss: 0.0414 - val_acc: 0.9838\n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 129s 257ms/step - loss: 0.0263 - acc: 0.9908 - val_loss: 0.0413 - val_acc: 0.9825\n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 103s 206ms/step - loss: 0.0293 - acc: 0.9891 - val_loss: 0.0418 - val_acc: 0.9838\n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 107s 213ms/step - loss: 0.0270 - acc: 0.9902 - val_loss: 0.0426 - val_acc: 0.9825\n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 132s 264ms/step - loss: 0.0230 - acc: 0.9923 - val_loss: 0.0693 - val_acc: 0.9838\n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 102s 204ms/step - loss: 0.0302 - acc: 0.9893 - val_loss: 0.0345 - val_acc: 0.9869\n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 104s 207ms/step - loss: 0.0211 - acc: 0.9932 - val_loss: 0.0390 - val_acc: 0.9850\n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 134s 269ms/step - loss: 0.0171 - acc: 0.9936 - val_loss: 0.0341 - val_acc: 0.9881\n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 104s 207ms/step - loss: 0.0214 - acc: 0.9929 - val_loss: 0.0319 - val_acc: 0.9900\n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 104s 208ms/step - loss: 0.0199 - acc: 0.9939 - val_loss: 0.0364 - val_acc: 0.9862\n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 135s 270ms/step - loss: 0.0198 - acc: 0.9940 - val_loss: 0.0405 - val_acc: 0.9881\n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 104s 208ms/step - loss: 0.0231 - acc: 0.9923 - val_loss: 0.0325 - val_acc: 0.9888\n",
      "Epoch 32/50\n",
      "500/500 [==============================] - 102s 203ms/step - loss: 0.0166 - acc: 0.9943 - val_loss: 0.0368 - val_acc: 0.9850\n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 136s 272ms/step - loss: 0.0183 - acc: 0.9934 - val_loss: 0.0280 - val_acc: 0.9881\n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 101s 203ms/step - loss: 0.0199 - acc: 0.9936 - val_loss: 0.0276 - val_acc: 0.9894\n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 102s 204ms/step - loss: 0.0154 - acc: 0.9950 - val_loss: 0.0269 - val_acc: 0.9875\n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 132s 265ms/step - loss: 0.0237 - acc: 0.9908 - val_loss: 0.0301 - val_acc: 0.9888\n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 103s 205ms/step - loss: 0.0344 - acc: 0.9899 - val_loss: 0.0299 - val_acc: 0.9875\n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 104s 207ms/step - loss: 0.0171 - acc: 0.9952 - val_loss: 0.0292 - val_acc: 0.9881\n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 135s 270ms/step - loss: 0.0186 - acc: 0.9941 - val_loss: 0.0265 - val_acc: 0.9875\n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 100s 200ms/step - loss: 0.0132 - acc: 0.9958 - val_loss: 0.0268 - val_acc: 0.9900\n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 102s 205ms/step - loss: 0.0123 - acc: 0.9959 - val_loss: 0.0261 - val_acc: 0.9906\n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 131s 263ms/step - loss: 0.0103 - acc: 0.9962 - val_loss: 0.0269 - val_acc: 0.9906\n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 105s 210ms/step - loss: 0.0128 - acc: 0.9958 - val_loss: 0.0258 - val_acc: 0.9906\n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 103s 206ms/step - loss: 0.0124 - acc: 0.9958 - val_loss: 0.0249 - val_acc: 0.9925\n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 130s 260ms/step - loss: 0.0123 - acc: 0.9958 - val_loss: 0.0231 - val_acc: 0.9912\n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 105s 211ms/step - loss: 0.0138 - acc: 0.9951 - val_loss: 0.0278 - val_acc: 0.9894\n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 102s 203ms/step - loss: 0.0137 - acc: 0.9956 - val_loss: 0.0262 - val_acc: 0.9888\n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 129s 257ms/step - loss: 0.0112 - acc: 0.9956 - val_loss: 0.0289 - val_acc: 0.9900\n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 107s 215ms/step - loss: 0.0107 - acc: 0.9960 - val_loss: 0.0279 - val_acc: 0.9894\n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 104s 208ms/step - loss: 0.0181 - acc: 0.9940 - val_loss: 0.0300 - val_acc: 0.9900\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7c9f67bf7d9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "model = build_model(backbone=cfg.MODEL.BACKBONE, \n",
    "                    norm_use=cfg.MODEL.NORM_USE, \n",
    "                    weights=\"imagenet\" if cfg.MODEL.USE_PRETRAIN else None)\n",
    "\"\"\"\n",
    "optim = make_optimizer(cfg)\n",
    "model.compile(loss=entropy_loss,#'categorical_crossentropy', \n",
    "              metrics=[\"accuracy\"], \n",
    "              optimizer=optim)\n",
    "model.summary()\n",
    "\n",
    "cb_list = [tf.keras.callbacks.ReduceLROnPlateau(factor=0.5,\n",
    "                                            patience=4,\n",
    "                                            min_lr=1e-12),\n",
    "      ]\n",
    "model.fit_generator(train_gen,\n",
    "                    epochs=cfg.TRAIN.EPOCHS,\n",
    "                    steps_per_epoch=cfg.TRAIN.NUM_UPDATES, \n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=cb_list\n",
    "                    )\n",
    "\n",
    "train_loss = model.history.history['loss']\n",
    "valid_loss = model.history.history['val_loss']\n",
    "train_acc = model.history.history['acc']\n",
    "valid_acc = model.history.history['val_acc']\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(range(len(train_loss)), train_loss, label='train_loss')\n",
    "plt.plot(range(len(valid_loss)), valid_loss, label='valid_loss')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(\"results\", \"exp_\" + cfg.SYSTEM.NAME_FLAG + \"_loss.png\"))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(range(len(train_acc)), train_acc, label='train_accuracy')\n",
    "plt.plot(range(len(valid_acc)), valid_acc, label='valid_accuracy')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(\"results\", \"exp_\" + cfg.SYSTEM.NAME_FLAG + \"_acc.png\"))\n",
    "\n",
    "result_df = pd.DataFrame({\"train_loss\":train_loss,\n",
    "                          \"valid_loss\":valid_loss,\n",
    "                          \"train_acc\":train_acc,\n",
    "                          \"valid_acc\":valid_acc\n",
    "                         })\n",
    "result_df.to_csv(os.path.join(\"results\", \"exp_\" + cfg.SYSTEM.NAME_FLAG + \"_result.csv\"), index=False)\n",
    "print(\"All Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
