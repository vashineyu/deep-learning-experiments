{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use tf.keras subclass model to build your own custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "from config import get_cfg_defaults\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--config-file\",\n",
    "    default=None,\n",
    "    metavar=\"FILE\",\n",
    "    help=\"path to config file\",\n",
    "    type=str,\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"opts\",\n",
    "        help=\"Modify config options using the command-line\",\n",
    "        default=None,\n",
    "        nargs=argparse.REMAINDER,\n",
    "    )\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg_defaults()\n",
    "if args.config_file is not None:\n",
    "    cfg.merge_from_file(args.config_file)\n",
    "if args.opts is not None:\n",
    "    cfg.merge_from_list(args.opts)\n",
    "cfg.freeze()\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(cfg.SYSTEM.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# model.py\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.models as models\n",
    "import tensorflow.nn as F\n",
    "\n",
    "class Conv_bn_relu(models.Model):\n",
    "    \"\"\"Stack blocks of Conv2D->BN->relu.\n",
    "    \n",
    "    Args:\n",
    "      filters (int): numbers of filters of conv layer\n",
    "      kernel_size (int): filter size\n",
    "      strides (int): stride step\n",
    "      data_format (str): channels_first or channels_last\n",
    "      use_bias (bool): add bias to layer?\n",
    "    Returns:\n",
    "      tf.keras.model object\n",
    "    \"\"\"\n",
    "    def __init__(self, filters, kernel_size=3, strides=1, data_format=\"channels_last\",\n",
    "                 use_bias=True, **kwargs):\n",
    "        super(Conv_bn_relu, self).__init__(**kwargs)\n",
    "        \n",
    "        axis = -1 if data_format is \"channels_last\" else 1\n",
    "        \n",
    "        self.conv = Conv2DFixedPadding(filters=filters, kernel_size=kernel_size, strides=strides, use_bias=use_bias)\n",
    "        self.normalize = layers.BatchNormalization(axis=axis)\n",
    "        \n",
    "    def call(self, x, training=True):\n",
    "        x = self.conv(x)\n",
    "        x = self.normalize(x, training=training)\n",
    "        return F.relu(x)\n",
    "\n",
    "class StackCNN(models.Model):\n",
    "    \"\"\"Stack all required layers together.\n",
    "    \n",
    "    Args:\n",
    "      neurons_of_layers (list): list of filter size of convolution layers\n",
    "      output_units (int): units of output node\n",
    "    Returns:\n",
    "      tf.keras.model object\n",
    "    \"\"\"\n",
    "    def __init__(self, neurons_of_layers, output_units, **kwargs):\n",
    "        super(StackCNN, self).__init__(**kwargs)\n",
    "        \n",
    "        self.layers_list = []\n",
    "        for i, l in enumerate(neurons_of_layers):\n",
    "            if (i+1) != len(neurons_of_layers):\n",
    "                self.layers_list.append(Conv_bn_relu(filters=l, kernel_size=3, strides=1))\n",
    "                self.layers_list.append(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "            else:\n",
    "                self.layers_list.append(Conv_bn_relu(filters=l, kernel_size=3, strides=1))\n",
    "                \n",
    "        self.layers_list.append(layers.Flatten())\n",
    "        self.layers_list.append(layers.Dense(units=output_units))\n",
    "    \n",
    "    def call(self, x, training=True):\n",
    "        for l in self.layers_list:\n",
    "            try:\n",
    "                # some customized layer should give training flags\n",
    "                x = l(x, training=training)\n",
    "            except:\n",
    "                # some original layers may not have training flags\n",
    "                x = l(x)\n",
    "        return F.softmax(x)\n",
    "\n",
    "## Fixed Functions ##\n",
    "def fixed_padding(inputs, kernel_size, data_format):\n",
    "    \"\"\"Pads the input along the spatial dimensions independently of input size.\n",
    "\n",
    "    This function is copied from:\n",
    "      https://github.com/tensorflow/models/blob/master/official/resnet/resnet_model.py\n",
    "\n",
    "    Args:\n",
    "      inputs: A tensor of size [batch, channels, height_in, width_in] or\n",
    "        [batch, height_in, width_in, channels] depending on data_format.\n",
    "      kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n",
    "        Should be a positive integer.\n",
    "      data_format: The input format ('channels_last' or 'channels_first').\n",
    "\n",
    "    Returns:\n",
    "      A tensor with the same format as the input with the data either intact\n",
    "    (if kernel_size == 1) or padded (if kernel_size > 1).\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    pad_total = kernel_size - 1\n",
    "    pad_beg = pad_total // 2\n",
    "    pad_end = pad_total - pad_beg\n",
    "\n",
    "    if data_format == 'channels_first':\n",
    "        padded_inputs = tf.pad(tensor=inputs,\n",
    "                               paddings=[[0, 0], [0, 0], [pad_beg, pad_end],\n",
    "                                         [pad_beg, pad_end]])\n",
    "    else:\n",
    "        padded_inputs = tf.pad(tensor=inputs,\n",
    "                               paddings=[[0, 0], [pad_beg, pad_end],\n",
    "                                         [pad_beg, pad_end], [0, 0]])\n",
    "    return padded_inputs\n",
    "\n",
    "\n",
    "class Conv2DFixedPadding(models.Model):\n",
    "    \"\"\"Class for Strided 2-D convolution with explicit padding.\n",
    "\n",
    "    The padding is consistent and is based only on `kernel_size`, not on the\n",
    "    dimensions of `inputs` (as opposed to using `tf.layers.conv2d` alone).\n",
    "\n",
    "    This class is based on:\n",
    "      https://github.com/tensorflow/models/blob/master/official/resnet/resnet_model.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filters, kernel_size=3, strides=1, data_format=\"channels_last\",\n",
    "                 use_bias=True, **kwargs):\n",
    "        super(Conv2DFixedPadding, self).__init__(**kwargs)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.data_format = data_format\n",
    "        self.strides = strides\n",
    "\n",
    "        self.conv = layers.Conv2D(filters=filters, kernel_size=kernel_size,\n",
    "                                  strides=strides, padding=('SAME' if strides == 1 else 'VALID'),\n",
    "                                  use_bias=use_bias, data_format=data_format)\n",
    "    \n",
    "    def call(self, x):\n",
    "        if self.strides > 1:\n",
    "            x = fixed_padding(x, self.kernel_size, self.data_format)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets.cifar10 import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = load_data()\n",
    "x_train, y_train = train\n",
    "x_valid, y_valid = valid\n",
    "\n",
    "x_train = x_train / 255.\n",
    "x_valid = x_valid / 255.\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_valid = tf.keras.utils.to_categorical(y_valid, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = layers.Input(shape=[32,32,3])\n",
    "module = StackCNN(neurons_of_layers=[32,32,64], output_units=10)(input_layer)\n",
    "model = models.Model(inputs=input_layer, outputs=module)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=tf.train.AdamOptimizer(), metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "stack_cnn (StackCNN)         (None, 10)                70122     \n",
      "=================================================================\n",
      "Total params: 70,122\n",
      "Trainable params: 69,866\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_bn_relu (Conv_bn_relu)  (None, 32, 32, 32)        1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_bn_relu_1 (Conv_bn_relu (None, 16, 16, 32)        9376      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv_bn_relu_2 (Conv_bn_relu (None, 8, 8, 64)          18752     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 70,122\n",
      "Trainable params: 69,866\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "model.get_layer('stack_cnn').summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 79s 2ms/step - loss: 1.3408 - acc: 0.5252 - val_loss: 1.0816 - val_acc: 0.6162\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 78s 2ms/step - loss: 0.9606 - acc: 0.6661 - val_loss: 0.9525 - val_acc: 0.6659\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 78s 2ms/step - loss: 0.8224 - acc: 0.7165 - val_loss: 0.8573 - val_acc: 0.7042\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 78s 2ms/step - loss: 0.7320 - acc: 0.7482 - val_loss: 0.8198 - val_acc: 0.7202\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 78s 2ms/step - loss: 0.6647 - acc: 0.7695 - val_loss: 0.8026 - val_acc: 0.7257\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 78s 2ms/step - loss: 0.6159 - acc: 0.7877 - val_loss: 0.8689 - val_acc: 0.7070\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 78s 2ms/step - loss: 0.5735 - acc: 0.8032 - val_loss: 0.7794 - val_acc: 0.7339\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 78s 2ms/step - loss: 0.5316 - acc: 0.8191 - val_loss: 0.8192 - val_acc: 0.7301\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 78s 2ms/step - loss: 0.4949 - acc: 0.8312 - val_loss: 0.7737 - val_acc: 0.7417\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 78s 2ms/step - loss: 0.4661 - acc: 0.8402 - val_loss: 0.8003 - val_acc: 0.7359\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 78s 2ms/step - loss: 0.4286 - acc: 0.8531 - val_loss: 0.7933 - val_acc: 0.7441\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 78s 2ms/step - loss: 0.4080 - acc: 0.8593 - val_loss: 0.8591 - val_acc: 0.7274\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 78s 2ms/step - loss: 0.3825 - acc: 0.8690 - val_loss: 0.8164 - val_acc: 0.7387\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 77s 2ms/step - loss: 0.3600 - acc: 0.8785 - val_loss: 0.8157 - val_acc: 0.7413\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 76s 2ms/step - loss: 0.3470 - acc: 0.8822 - val_loss: 0.8419 - val_acc: 0.7401\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 76s 2ms/step - loss: 0.3200 - acc: 0.8908 - val_loss: 0.8595 - val_acc: 0.7409\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 77s 2ms/step - loss: 0.2955 - acc: 0.8993 - val_loss: 0.8421 - val_acc: 0.7459\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 76s 2ms/step - loss: 0.2792 - acc: 0.9078 - val_loss: 0.8847 - val_acc: 0.7369\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 78s 2ms/step - loss: 0.2630 - acc: 0.9115 - val_loss: 0.8886 - val_acc: 0.7414\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 79s 2ms/step - loss: 0.2478 - acc: 0.9182 - val_loss: 0.8979 - val_acc: 0.7421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2d38045780>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, \n",
    "          epochs=cfg.TRAIN.EPOCHS, \n",
    "          batch_size=cfg.TRAIN.BATCH_SIZE, \n",
    "          shuffle=True, validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 13, 13, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 13, 13, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 29,802\n",
      "Trainable params: 29,546\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_example_model():\n",
    "    input_layer = layers.Input(shape=(32,32,3))\n",
    "    x = layers.Conv2D(filters=32, kernel_size=3, strides=1)(input_layer)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2,2))(x)\n",
    "    \n",
    "    x = layers.Conv2D(filters=32, kernel_size=3, strides=1)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2,2))(x)\n",
    "    \n",
    "    x = layers.Conv2D(filters=64, kernel_size=3, strides=1)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(units=10, activation=\"softmax\")(x)\n",
    "    return models.Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "model = build_example_model()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=tf.train.AdamOptimizer(), metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 1.5022 - acc: 0.4690 - val_loss: 3.2010 - val_acc: 0.1022\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1.1741 - acc: 0.5884 - val_loss: 2.0205 - val_acc: 0.2815\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1.0543 - acc: 0.6335 - val_loss: 1.6761 - val_acc: 0.4040\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 54s 1ms/step - loss: 0.9790 - acc: 0.6564 - val_loss: 1.4327 - val_acc: 0.5217\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 0.9251 - acc: 0.6794 - val_loss: 1.3064 - val_acc: 0.5332\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 0.8811 - acc: 0.6943 - val_loss: 1.2173 - val_acc: 0.5693\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 0.8458 - acc: 0.7073 - val_loss: 1.1131 - val_acc: 0.6234\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 0.8114 - acc: 0.7180 - val_loss: 1.2092 - val_acc: 0.5860\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 0.7881 - acc: 0.7262 - val_loss: 0.9582 - val_acc: 0.6634\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 0.7628 - acc: 0.7362 - val_loss: 1.2601 - val_acc: 0.5915\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 0.7365 - acc: 0.7453 - val_loss: 1.3690 - val_acc: 0.5534\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 0.7212 - acc: 0.7494 - val_loss: 1.0717 - val_acc: 0.6304\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 54s 1ms/step - loss: 0.7053 - acc: 0.7567 - val_loss: 1.2069 - val_acc: 0.6045\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 54s 1ms/step - loss: 0.6872 - acc: 0.7641 - val_loss: 0.9669 - val_acc: 0.6690\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 52s 1ms/step - loss: 0.6685 - acc: 0.7698 - val_loss: 1.0706 - val_acc: 0.6494\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 51s 1ms/step - loss: 0.6557 - acc: 0.7744 - val_loss: 1.1344 - val_acc: 0.6310\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 52s 1ms/step - loss: 0.6428 - acc: 0.7784 - val_loss: 0.9569 - val_acc: 0.6818\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 52s 1ms/step - loss: 0.6305 - acc: 0.7833 - val_loss: 0.8512 - val_acc: 0.7002\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 52s 1ms/step - loss: 0.6183 - acc: 0.7875 - val_loss: 0.8886 - val_acc: 0.6919\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 52s 1ms/step - loss: 0.6104 - acc: 0.7906 - val_loss: 1.0358 - val_acc: 0.6548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2bc06aeeb8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, \n",
    "          epochs=cfg.TRAIN.EPOCHS, \n",
    "          batch_size=cfg.TRAIN.BATCH_SIZE, \n",
    "          shuffle=True, validation_data=(x_valid, y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
