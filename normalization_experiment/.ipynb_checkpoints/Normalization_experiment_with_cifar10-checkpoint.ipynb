{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from time import sleep\n",
    "from tqdm import tqdm # if use notebook\n",
    "\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Event\n",
    "import queue\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check path and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train, d_valid= cifar10.load_data()\n",
    "x_train, y_train = d_train\n",
    "x_valid, y_valid = d_valid\n",
    "\n",
    "x_train = x_train / 255.\n",
    "x_valid = x_valid / 255.\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_valid = keras.utils.to_categorical(y_valid, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers as KL\n",
    "from keras.layers import Layer, InputSpec\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras import backend as K\n",
    "\n",
    "class GroupNorm(Layer):\n",
    "    '''Group normalization layer\n",
    "    Group Normalization divides the channels into groups and computes within each group\n",
    "    the mean and variance for normalization. GN's computation is independent of batch sizes,\n",
    "    and its accuracy is stable in a wide range of batch sizes\n",
    "    # Arguments\n",
    "        groups: Integer, the number of groups for Group Normalization.\n",
    "        axis: Integer, the axis that should be normalized\n",
    "            (typically the features axis).\n",
    "            For instance, after a `Conv2D` layer with\n",
    "            `data_format=\"channels_first\"`,\n",
    "            set `axis=1` in `BatchNormalization`.\n",
    "        epsilon: Small float added to variance to avoid dividing by zero.\n",
    "        center: If True, add offset of `beta` to normalized tensor.\n",
    "            If False, `beta` is ignored.\n",
    "        scale: If True, multiply by `gamma`.\n",
    "            If False, `gamma` is not used.\n",
    "            When the next layer is linear (also e.g. `nn.relu`),\n",
    "            this can be disabled since the scaling\n",
    "            will be done by the next layer.\n",
    "        beta_initializer: Initializer for the beta weight.\n",
    "        gamma_initializer: Initializer for the gamma weight.\n",
    "        beta_regularizer: Optional regularizer for the beta weight.\n",
    "        gamma_regularizer: Optional regularizer for the gamma weight.\n",
    "        beta_constraint: Optional constraint for the beta weight.\n",
    "        gamma_constraint: Optional constraint for the gamma weight.\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "    # Output shape\n",
    "        Same shape as input.\n",
    "    # References\n",
    "        - [Group Normalization](https://arxiv.org/abs/1803.08494)\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 groups=32,\n",
    "                 axis=-1,\n",
    "                 epsilon=1e-8,\n",
    "                 center=True,\n",
    "                 scale=True,\n",
    "                 beta_initializer='zeros',\n",
    "                 gamma_initializer='ones',\n",
    "                 beta_regularizer=None,\n",
    "                 gamma_regularizer=None,\n",
    "                 beta_constraint=None,\n",
    "                 gamma_constraint=None,\n",
    "                 **kwargs):\n",
    "        super(GroupNorm, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.groups = groups\n",
    "        self.axis = axis\n",
    "        self.epsilon = epsilon\n",
    "        self.center = center\n",
    "        self.scale = scale\n",
    "        self.beta_initializer = initializers.get(beta_initializer)\n",
    "        self.gamma_initializer = initializers.get(gamma_initializer)\n",
    "        self.beta_regularizer = regularizers.get(beta_regularizer)\n",
    "        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
    "        self.beta_constraint = constraints.get(beta_constraint)\n",
    "        self.gamma_constraint = constraints.get(gamma_constraint)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        dim = input_shape[self.axis]\n",
    "        if dim is None:\n",
    "            raise ValueError('Axis ' + str(self.axis) + ' of '\n",
    "                             'input tensor should have a defined dimension '\n",
    "                             'but the layer received an input with shape ' +\n",
    "                             str(input_shape) + '.')\n",
    "        if dim < self.groups:\n",
    "            raise ValueError('Number of groups (' + str(self.groups) + ') cannot be '\n",
    "                             'more than the number of channels (' +\n",
    "                             str(dim) + ').')\n",
    "        if dim % self.groups != 0:\n",
    "            raise ValueError('Number of groups (' + str(self.groups) + ') must be a '\n",
    "                             'multiple of the number of channels (' +\n",
    "                             str(dim) + ').')\n",
    "            \n",
    "        self.input_spec = InputSpec(ndim=len(input_shape),\n",
    "                                    axes={self.axis: dim})\n",
    "        shape = (dim,)\n",
    "\n",
    "        if self.scale:\n",
    "            self.gamma = self.add_weight(shape=shape,\n",
    "                                         name='gamma',\n",
    "                                         initializer=self.gamma_initializer,\n",
    "                                         regularizer=self.gamma_regularizer,\n",
    "                                         constraint=self.gamma_constraint)\n",
    "        else:\n",
    "            self.gamma = None\n",
    "\n",
    "        if self.center:\n",
    "            self.beta = self.add_weight(shape=shape,\n",
    "                                        name='beta',\n",
    "                                        initializer=self.beta_initializer,\n",
    "                                        regularizer=self.beta_regularizer,\n",
    "                                        constraint=self.beta_constraint)\n",
    "        else:\n",
    "            self.beta = None\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        input_shape = K.int_shape(inputs)\n",
    "        tensor_input_shape = K.shape(inputs)\n",
    "\n",
    "        # Prepare broadcasting shape.\n",
    "        reduction_axes = list(range(len(input_shape)))\n",
    "        del reduction_axes[self.axis]\n",
    "        broadcast_shape = [1] * len(input_shape)\n",
    "        broadcast_shape[self.axis] = input_shape[self.axis] // self.groups\n",
    "        broadcast_shape.insert(1, self.groups)\n",
    "\n",
    "        reshape_group_shape = K.shape(inputs)\n",
    "        group_axes = [reshape_group_shape[i] for i in range(len(input_shape))]\n",
    "        group_axes[self.axis] = input_shape[self.axis] // self.groups\n",
    "        group_axes.insert(1, self.groups)\n",
    "\n",
    "        # reshape inputs to new group shape\n",
    "        group_shape = [group_axes[0], self.groups] + group_axes[2:]\n",
    "        group_shape = K.stack(group_shape)\n",
    "        inputs = K.reshape(inputs, group_shape)\n",
    "\n",
    "        group_reduction_axes = list(range(len(group_axes)))\n",
    "        group_reduction_axes = group_reduction_axes[2:]\n",
    "\n",
    "        mean = K.mean(inputs, axis=group_reduction_axes, keepdims=True)\n",
    "        variance = K.var(inputs, axis=group_reduction_axes, keepdims=True)\n",
    "\n",
    "        inputs = (inputs - mean) / (K.sqrt(variance + self.epsilon))\n",
    "\n",
    "        # prepare broadcast shape\n",
    "        inputs = K.reshape(inputs, group_shape)\n",
    "        outputs = inputs\n",
    "\n",
    "        # In this case we must explicitly broadcast all parameters.\n",
    "        if self.scale:\n",
    "            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n",
    "            outputs = outputs * broadcast_gamma\n",
    "\n",
    "        if self.center:\n",
    "            broadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
    "            outputs = outputs + broadcast_beta\n",
    "\n",
    "        outputs = K.reshape(outputs, tensor_input_shape)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'groups': self.groups,\n",
    "            'axis': self.axis,\n",
    "            'epsilon': self.epsilon,\n",
    "            'center': self.center,\n",
    "            'scale': self.scale,\n",
    "            'beta_initializer': initializers.serialize(self.beta_initializer),\n",
    "            'gamma_initializer': initializers.serialize(self.gamma_initializer),\n",
    "            'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n",
    "            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n",
    "            'beta_constraint': constraints.serialize(self.beta_constraint),\n",
    "            'gamma_constraint': constraints.serialize(self.gamma_constraint)\n",
    "        }\n",
    "        base_config = super(GroupNorm, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "    \n",
    "class InstanceNormalization(Layer):\n",
    "    \"\"\"Instance normalization layer.\n",
    "    Normalize the activations of the previous layer at each step,\n",
    "    i.e. applies a transformation that maintains the mean activation\n",
    "    close to 0 and the activation standard deviation close to 1.\n",
    "    # Arguments\n",
    "        axis: Integer, the axis that should be normalized\n",
    "            (typically the features axis).\n",
    "            For instance, after a `Conv2D` layer with\n",
    "            `data_format=\"channels_first\"`,\n",
    "            set `axis=1` in `InstanceNormalization`.\n",
    "            Setting `axis=None` will normalize all values in each\n",
    "            instance of the batch.\n",
    "            Axis 0 is the batch dimension. `axis` cannot be set to 0 to avoid errors.\n",
    "        epsilon: Small float added to variance to avoid dividing by zero.\n",
    "        center: If True, add offset of `beta` to normalized tensor.\n",
    "            If False, `beta` is ignored.\n",
    "        scale: If True, multiply by `gamma`.\n",
    "            If False, `gamma` is not used.\n",
    "            When the next layer is linear (also e.g. `nn.relu`),\n",
    "            this can be disabled since the scaling\n",
    "            will be done by the next layer.\n",
    "        beta_initializer: Initializer for the beta weight.\n",
    "        gamma_initializer: Initializer for the gamma weight.\n",
    "        beta_regularizer: Optional regularizer for the beta weight.\n",
    "        gamma_regularizer: Optional regularizer for the gamma weight.\n",
    "        beta_constraint: Optional constraint for the beta weight.\n",
    "        gamma_constraint: Optional constraint for the gamma weight.\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a Sequential model.\n",
    "    # Output shape\n",
    "        Same shape as input.\n",
    "    # References\n",
    "        - [Layer Normalization](https://arxiv.org/abs/1607.06450)\n",
    "        - [Instance Normalization: The Missing Ingredient for Fast Stylization](\n",
    "        https://arxiv.org/abs/1607.08022)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 axis=None,\n",
    "                 epsilon=1e-3,\n",
    "                 center=True,\n",
    "                 scale=True,\n",
    "                 beta_initializer='zeros',\n",
    "                 gamma_initializer='ones',\n",
    "                 beta_regularizer=None,\n",
    "                 gamma_regularizer=None,\n",
    "                 beta_constraint=None,\n",
    "                 gamma_constraint=None,\n",
    "                 **kwargs):\n",
    "        super(InstanceNormalization, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.axis = axis\n",
    "        self.epsilon = epsilon\n",
    "        self.center = center\n",
    "        self.scale = scale\n",
    "        self.beta_initializer = initializers.get(beta_initializer)\n",
    "        self.gamma_initializer = initializers.get(gamma_initializer)\n",
    "        self.beta_regularizer = regularizers.get(beta_regularizer)\n",
    "        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
    "        self.beta_constraint = constraints.get(beta_constraint)\n",
    "        self.gamma_constraint = constraints.get(gamma_constraint)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        ndim = len(input_shape)\n",
    "        if self.axis == 0:\n",
    "            raise ValueError('Axis cannot be zero')\n",
    "\n",
    "        if (self.axis is not None) and (ndim == 2):\n",
    "            raise ValueError('Cannot specify axis for rank 1 tensor')\n",
    "\n",
    "        self.input_spec = InputSpec(ndim=ndim)\n",
    "\n",
    "        if self.axis is None:\n",
    "            shape = (1,)\n",
    "        else:\n",
    "            shape = (input_shape[self.axis],)\n",
    "\n",
    "        if self.scale:\n",
    "            self.gamma = self.add_weight(shape=shape,\n",
    "                                         name='gamma',\n",
    "                                         initializer=self.gamma_initializer,\n",
    "                                         regularizer=self.gamma_regularizer,\n",
    "                                         constraint=self.gamma_constraint)\n",
    "        else:\n",
    "            self.gamma = None\n",
    "        if self.center:\n",
    "            self.beta = self.add_weight(shape=shape,\n",
    "                                        name='beta',\n",
    "                                        initializer=self.beta_initializer,\n",
    "                                        regularizer=self.beta_regularizer,\n",
    "                                        constraint=self.beta_constraint)\n",
    "        else:\n",
    "            self.beta = None\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        input_shape = K.int_shape(inputs)\n",
    "        reduction_axes = list(range(0, len(input_shape)))\n",
    "\n",
    "        if self.axis is not None:\n",
    "            del reduction_axes[self.axis]\n",
    "\n",
    "        del reduction_axes[0]\n",
    "\n",
    "        mean = K.mean(inputs, reduction_axes, keepdims=True)\n",
    "        stddev = K.std(inputs, reduction_axes, keepdims=True) + self.epsilon\n",
    "        normed = (inputs - mean) / stddev\n",
    "\n",
    "        broadcast_shape = [1] * len(input_shape)\n",
    "        if self.axis is not None:\n",
    "            broadcast_shape[self.axis] = input_shape[self.axis]\n",
    "\n",
    "        if self.scale:\n",
    "            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n",
    "            normed = normed * broadcast_gamma\n",
    "        if self.center:\n",
    "            broadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
    "            normed = normed + broadcast_beta\n",
    "        return normed\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'axis': self.axis,\n",
    "            'epsilon': self.epsilon,\n",
    "            'center': self.center,\n",
    "            'scale': self.scale,\n",
    "            'beta_initializer': initializers.serialize(self.beta_initializer),\n",
    "            'gamma_initializer': initializers.serialize(self.gamma_initializer),\n",
    "            'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n",
    "            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n",
    "            'beta_constraint': constraints.serialize(self.beta_constraint),\n",
    "            'gamma_constraint': constraints.serialize(self.gamma_constraint)\n",
    "        }\n",
    "        base_config = super(InstanceNormalization, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "###    \n",
    "def normalization(x, method=\"bn\"):\n",
    "    if method is \"bn\":\n",
    "        x = KL.BatchNormalization(axis=-1)(x)\n",
    "    elif method is \"gn\":\n",
    "        x = GroupNorm(groups=32)(x)\n",
    "    else:\n",
    "        \"\"\"Do nothing\"\"\"\n",
    "        x = x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 455s 9ms/step - loss: 1.5806 - acc: 0.4141 - val_loss: 1.3265 - val_acc: 0.5157\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 455s 9ms/step - loss: 1.2363 - acc: 0.5546 - val_loss: 1.1192 - val_acc: 0.5994\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 451s 9ms/step - loss: 1.0832 - acc: 0.6123 - val_loss: 1.0717 - val_acc: 0.6158\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 524s 10ms/step - loss: 0.9787 - acc: 0.6527 - val_loss: 0.9001 - val_acc: 0.6843\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 543s 11ms/step - loss: 0.9004 - acc: 0.6808 - val_loss: 0.9068 - val_acc: 0.6851\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 546s 11ms/step - loss: 0.8361 - acc: 0.7066 - val_loss: 0.7999 - val_acc: 0.7192\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 537s 11ms/step - loss: 0.7766 - acc: 0.7295 - val_loss: 0.8046 - val_acc: 0.7165\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 546s 11ms/step - loss: 0.7254 - acc: 0.7476 - val_loss: 0.7680 - val_acc: 0.7272\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 579s 12ms/step - loss: 0.6793 - acc: 0.7641 - val_loss: 0.7368 - val_acc: 0.7396\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 573s 11ms/step - loss: 0.6382 - acc: 0.7801 - val_loss: 0.7840 - val_acc: 0.7301\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 581s 12ms/step - loss: 0.6062 - acc: 0.7917 - val_loss: 0.7160 - val_acc: 0.7528\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 572s 11ms/step - loss: 0.5727 - acc: 0.8018 - val_loss: 0.6630 - val_acc: 0.7700\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 531s 11ms/step - loss: 0.5427 - acc: 0.8125 - val_loss: 0.6688 - val_acc: 0.7657\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 565s 11ms/step - loss: 0.5161 - acc: 0.8214 - val_loss: 0.6140 - val_acc: 0.7832\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 574s 11ms/step - loss: 0.4916 - acc: 0.8306 - val_loss: 0.7051 - val_acc: 0.7523\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 537s 11ms/step - loss: 0.4652 - acc: 0.8383 - val_loss: 0.6842 - val_acc: 0.7643\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 538s 11ms/step - loss: 0.4420 - acc: 0.8459 - val_loss: 0.6446 - val_acc: 0.7766\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 484s 10ms/step - loss: 0.4198 - acc: 0.8559 - val_loss: 0.5949 - val_acc: 0.7959\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 534s 11ms/step - loss: 0.3997 - acc: 0.8623 - val_loss: 0.6009 - val_acc: 0.7924\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 587s 12ms/step - loss: 0.3784 - acc: 0.8704 - val_loss: 0.6354 - val_acc: 0.7790\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 632s 13ms/step - loss: 0.3584 - acc: 0.8771 - val_loss: 0.5974 - val_acc: 0.7999\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 662s 13ms/step - loss: 0.3401 - acc: 0.8836 - val_loss: 0.5963 - val_acc: 0.8004\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 575s 11ms/step - loss: 0.3202 - acc: 0.8916 - val_loss: 0.5901 - val_acc: 0.8010\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 510s 10ms/step - loss: 0.3035 - acc: 0.8963 - val_loss: 0.6148 - val_acc: 0.7952\n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 488s 10ms/step - loss: 0.2852 - acc: 0.9031 - val_loss: 0.6041 - val_acc: 0.8046\n",
      "Epoch 26/100\n",
      "50000/50000 [==============================] - 451s 9ms/step - loss: 0.2688 - acc: 0.9087 - val_loss: 0.6219 - val_acc: 0.7952\n",
      "Epoch 27/100\n",
      "50000/50000 [==============================] - 416s 8ms/step - loss: 0.2539 - acc: 0.9138 - val_loss: 0.6230 - val_acc: 0.8033\n",
      "Epoch 28/100\n",
      "50000/50000 [==============================] - 418s 8ms/step - loss: 0.2372 - acc: 0.9193 - val_loss: 0.6351 - val_acc: 0.7952\n",
      "Epoch 29/100\n",
      "13302/50000 [======>.......................] - ETA: 4:53 - loss: 0.2032 - acc: 0.9351"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e16d34c6413f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mrecords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnormal_method\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "records = defaultdict(dict)\n",
    "for normal_method in [\"gn\", \"bn\", None]:\n",
    "    K.clear_session()\n",
    "\n",
    "    inputs = KL.Input(shape=[32, 32, 3], name='inputs')\n",
    "    x = KL.Conv2D(filters=64, kernel_size=3, padding='same')(inputs)\n",
    "    x = normalization(x, method=normal_method)\n",
    "    x = KL.Activation(\"relu\")(x)\n",
    "    x = KL.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "    x = KL.Conv2D(filters=128, kernel_size=3, padding='same')(x)\n",
    "    x = normalization(x, method=normal_method)\n",
    "    x = KL.Activation(\"relu\")(x)\n",
    "    x = KL.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "    x = KL.Conv2D(filters=128, kernel_size=3, padding='same')(x)\n",
    "    x = normalization(x, method=normal_method)\n",
    "    x = KL.Activation(\"relu\")(x)\n",
    "    x = KL.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    out = KL.Dense(units=10, activation=\"softmax\", name=\"output\")(x)\n",
    "\n",
    "    model = keras.models.Model(inputs=[inputs], outputs=[out])\n",
    "\n",
    "    optim = keras.optimizers.Adam(lr=0.0001)\n",
    "    model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optim)\n",
    "\n",
    "    model.fit(x_train, y_train, batch_size=1, epochs=EPOCHS, validation_data=(x_valid, y_valid), shuffle=True)\n",
    "    \n",
    "    records[normal_method]['loss'] = model.history.history['loss']\n",
    "    records[normal_method]['val_loss'] = model.history.history['val_loss']\n",
    "    records[normal_method]['acc'] = model.history.history['acc']\n",
    "    records[normal_method]['val_acc'] = model.history.history['val_acc']\n",
    "    print(\"Method: %s Done\" % (normal_method))\n",
    "    \n",
    "for method in records.keys():\n",
    "    plt.plot(range(len(records[method]['loss'])), records[method]['loss'], '-', label=str(method))\n",
    "    plt.plot(range(len(records[method]['val_loss'])), records[method]['val_loss'], '--', label=str(method))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = defaultdict(dict)\n",
    "for normal_method in [\"gn\", \"bn\", None]:\n",
    "    K.clear_session()\n",
    "\n",
    "    inputs = KL.Input(shape=[32, 32, 3], name='inputs')\n",
    "    x = KL.Conv2D(filters=64, kernel_size=3, padding='same')(inputs)\n",
    "    x = normalization(x, method=normal_method)\n",
    "    x = KL.Activation(\"relu\")(x)\n",
    "    x = KL.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "    x = KL.Conv2D(filters=128, kernel_size=3, padding='same')(x)\n",
    "    x = normalization(x, method=normal_method)\n",
    "    x = KL.Activation(\"relu\")(x)\n",
    "    x = KL.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "    x = KL.Conv2D(filters=128, kernel_size=3, padding='same')(x)\n",
    "    x = normalization(x, method=normal_method)\n",
    "    x = KL.Activation(\"relu\")(x)\n",
    "    x = KL.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    out = KL.Dense(units=10, activation=\"softmax\", name=\"output\")(x)\n",
    "\n",
    "    model = keras.models.Model(inputs=[inputs], outputs=[out])\n",
    "\n",
    "    optim = keras.optimizers.Adam(lr=0.0001)\n",
    "    model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optim)\n",
    "\n",
    "    model.fit(x_train, y_train, batch_size=32, epochs=EPOCHS, validation_data=(x_valid, y_valid), shuffle=True)\n",
    "    \n",
    "    records[normal_method]['loss'] = model.history.history['loss']\n",
    "    records[normal_method]['val_loss'] = model.history.history['val_loss']\n",
    "    records[normal_method]['acc'] = model.history.history['acc']\n",
    "    records[normal_method]['val_acc'] = model.history.history['val_acc']\n",
    "    print(\"Method: %s Done\" % (normal_method))\n",
    "    \n",
    "for method in records.keys():\n",
    "    plt.plot(range(len(records[method]['loss'])), records[method]['loss'], '-', label=str(method))\n",
    "    plt.plot(range(len(records[method]['val_loss'])), records[method]['val_loss'], '--', label=str(method))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = defaultdict(dict)\n",
    "for normal_method in [\"gn\", \"bn\", None]:\n",
    "    K.clear_session()\n",
    "\n",
    "    inputs = KL.Input(shape=[32, 32, 3], name='inputs')\n",
    "    x = KL.Conv2D(filters=64, kernel_size=3, padding='same')(inputs)\n",
    "    x = normalization(x, method=normal_method)\n",
    "    x = KL.Activation(\"relu\")(x)\n",
    "    x = KL.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "    x = KL.Conv2D(filters=128, kernel_size=3, padding='same')(x)\n",
    "    x = normalization(x, method=normal_method)\n",
    "    x = KL.Activation(\"relu\")(x)\n",
    "    x = KL.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "    x = KL.Conv2D(filters=128, kernel_size=3, padding='same')(x)\n",
    "    x = normalization(x, method=normal_method)\n",
    "    x = KL.Activation(\"relu\")(x)\n",
    "    x = KL.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    out = KL.Dense(units=10, activation=\"softmax\", name=\"output\")(x)\n",
    "\n",
    "    model = keras.models.Model(inputs=[inputs], outputs=[out])\n",
    "\n",
    "    optim = keras.optimizers.Adam(lr=0.0001)\n",
    "    model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optim)\n",
    "\n",
    "    model.fit(x_train, y_train, batch_size=256, epochs=EPOCHS, validation_data=(x_valid, y_valid), shuffle=True)\n",
    "    \n",
    "    records[normal_method]['loss'] = model.history.history['loss']\n",
    "    records[normal_method]['val_loss'] = model.history.history['val_loss']\n",
    "    records[normal_method]['acc'] = model.history.history['acc']\n",
    "    records[normal_method]['val_acc'] = model.history.history['val_acc']\n",
    "    print(\"Method: %s Done\" % (normal_method))\n",
    "    \n",
    "for method in records.keys():\n",
    "    plt.plot(range(len(records[method]['loss'])), records[method]['loss'], '-', label=str(method))\n",
    "    plt.plot(range(len(records[method]['val_loss'])), records[method]['val_loss'], '--', label=str(method))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 20s 409us/step - loss: 1.7282 - acc: 0.3638 - val_loss: 1.5113 - val_acc: 0.4480\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "append() takes exactly one argument (0 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d916626fed7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: append() takes exactly one argument (0 given)"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "normal_method = \"gn\"\n",
    "\n",
    "inputs = KL.Input(shape=[32, 32, 3], name='inputs')\n",
    "x = KL.Conv2D(filters=64, kernel_size=3, padding='same')(inputs)\n",
    "x = normalization(x, method=normal_method)\n",
    "x = KL.Activation(\"relu\")(x)\n",
    "x = KL.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "x = KL.Conv2D(filters=128, kernel_size=3, padding='same')(x)\n",
    "x = normalization(x, method=normal_method)\n",
    "x = KL.Activation(\"relu\")(x)\n",
    "x = KL.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "x = KL.Conv2D(filters=128, kernel_size=3, padding='same')(x)\n",
    "x = normalization(x, method=normal_method)\n",
    "x = KL.Activation(\"relu\")(x)\n",
    "x = KL.GlobalAveragePooling2D()(x)\n",
    "\n",
    "out = KL.Dense(units=10, activation=\"softmax\", name=\"output\")(x)\n",
    "\n",
    "model = keras.models.Model(inputs=[inputs], outputs=[out])\n",
    "\n",
    "optim = keras.optimizers.Adam(lr=0.0001)\n",
    "model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optim)\n",
    "\n",
    "model.fit(x_train, y_train, epochs=EPOCHS, validation_data=(x_valid, y_valid), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 13s 268us/step - loss: 1.5244 - acc: 0.4651 - val_loss: 1.6642 - val_acc: 0.4034\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 14s 271us/step - loss: 1.2532 - acc: 0.5644 - val_loss: 1.3167 - val_acc: 0.5303\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 13s 265us/step - loss: 1.1440 - acc: 0.6084 - val_loss: 1.1772 - val_acc: 0.5744\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 13s 265us/step - loss: 1.0773 - acc: 0.6277 - val_loss: 1.2038 - val_acc: 0.5767\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 13s 253us/step - loss: 1.0231 - acc: 0.6463 - val_loss: 1.2296 - val_acc: 0.5587\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 13s 254us/step - loss: 0.9783 - acc: 0.6629 - val_loss: 1.0656 - val_acc: 0.6164\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 13s 262us/step - loss: 0.9420 - acc: 0.6770 - val_loss: 1.0917 - val_acc: 0.6187\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.9088 - acc: 0.6873 - val_loss: 1.0466 - val_acc: 0.6390\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 13s 256us/step - loss: 0.8821 - acc: 0.6965 - val_loss: 1.1274 - val_acc: 0.6082\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 13s 259us/step - loss: 0.8539 - acc: 0.7061 - val_loss: 1.1761 - val_acc: 0.5805\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 13s 263us/step - loss: 0.8307 - acc: 0.7127 - val_loss: 1.0265 - val_acc: 0.6400\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 13s 262us/step - loss: 0.8064 - acc: 0.7237 - val_loss: 1.3621 - val_acc: 0.5505\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 13s 256us/step - loss: 0.7860 - acc: 0.7307 - val_loss: 1.1890 - val_acc: 0.6066\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 13s 256us/step - loss: 0.7616 - acc: 0.7384 - val_loss: 1.0786 - val_acc: 0.6194\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 13s 253us/step - loss: 0.7463 - acc: 0.7431 - val_loss: 0.9208 - val_acc: 0.6789\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 12s 250us/step - loss: 0.7294 - acc: 0.7521 - val_loss: 0.8987 - val_acc: 0.6886\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 13s 251us/step - loss: 0.7106 - acc: 0.7568 - val_loss: 1.0816 - val_acc: 0.6311\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 12s 244us/step - loss: 0.6945 - acc: 0.7634 - val_loss: 0.9795 - val_acc: 0.6505\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 13s 254us/step - loss: 0.6801 - acc: 0.7685 - val_loss: 1.2200 - val_acc: 0.5944\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 0.6621 - acc: 0.7740 - val_loss: 0.9494 - val_acc: 0.6665\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 12s 250us/step - loss: 0.6528 - acc: 0.7782 - val_loss: 1.3389 - val_acc: 0.5628\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 13s 253us/step - loss: 0.6382 - acc: 0.7820 - val_loss: 0.8446 - val_acc: 0.7104\n",
      "Epoch 23/100\n",
      "48416/50000 [============================>.] - ETA: 0s - loss: 0.6243 - acc: 0.7872"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-dc4eb2bd442b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "normal_method = \"bn\"\n",
    "\n",
    "inputs = KL.Input(shape=[32, 32, 3], name='inputs')\n",
    "x = KL.Conv2D(filters=64, kernel_size=3, padding='same')(inputs)\n",
    "x = normalization(x, method=normal_method)\n",
    "x = KL.Activation(\"relu\")(x)\n",
    "x = KL.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "x = KL.Conv2D(filters=128, kernel_size=3, padding='same')(x)\n",
    "x = normalization(x, method=normal_method)\n",
    "x = KL.Activation(\"relu\")(x)\n",
    "x = KL.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "x = KL.Conv2D(filters=128, kernel_size=3, padding='same')(x)\n",
    "x = normalization(x, method=normal_method)\n",
    "x = KL.Activation(\"relu\")(x)\n",
    "x = KL.GlobalAveragePooling2D()(x)\n",
    "\n",
    "out = KL.Dense(units=10, activation=\"softmax\", name=\"output\")(x)\n",
    "\n",
    "model = keras.models.Model(inputs=[inputs], outputs=[out])\n",
    "\n",
    "optim = keras.optimizers.Adam(lr=0.0001)\n",
    "model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optim)\n",
    "\n",
    "model.fit(x_train, y_train, epochs=EPOCHS, validation_data=(x_valid, y_valid), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "normal_method = None\n",
    "\n",
    "inputs = KL.Input(shape=[32, 32, 3], name='inputs')\n",
    "x = KL.Conv2D(filters=64, kernel_size=3, padding='same')(inputs)\n",
    "x = normalization(x, method=normal_method)\n",
    "x = KL.Activation(\"relu\")(x)\n",
    "x = KL.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "x = KL.Conv2D(filters=128, kernel_size=3, padding='same')(x)\n",
    "x = normalization(x, method=normal_method)\n",
    "x = KL.Activation(\"relu\")(x)\n",
    "x = KL.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "x = KL.Conv2D(filters=128, kernel_size=3, padding='same')(x)\n",
    "x = normalization(x, method=normal_method)\n",
    "x = KL.Activation(\"relu\")(x)\n",
    "x = KL.GlobalAveragePooling2D()(x)\n",
    "\n",
    "out = KL.Dense(units=10, activation=\"softmax\", name=\"output\")(x)\n",
    "\n",
    "model = keras.models.Model(inputs=[inputs], outputs=[out])\n",
    "\n",
    "optim = keras.optimizers.Adam(lr=0.0001)\n",
    "model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optim)\n",
    "\n",
    "model.fit(x_train, y_train, epochs=EPOCHS, validation_data=(x_valid, y_valid), shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {
    "6b1438b3075f49289cfcf03a4fce2ccb": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "ab7848b490e5454e9a42f439a6ef6b31": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "e0e3b0d66b1e47c4ade6c276bacc5c55": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
