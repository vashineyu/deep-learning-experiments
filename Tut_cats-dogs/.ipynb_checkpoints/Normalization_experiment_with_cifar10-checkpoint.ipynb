{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from time import sleep\n",
    "from tqdm import tqdm # if use notebook\n",
    "\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Event\n",
    "import queue\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check path and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train, d_valid= cifar10.load_data()\n",
    "x_train, y_train = d_train\n",
    "x_valid, y_valid = d_valid\n",
    "\n",
    "x_train = x_train / 255.\n",
    "x_valid = x_valid / 255.\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_valid = keras.utils.to_categorical(y_valid, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers as KL\n",
    "from keras.layers import Layer, InputSpec\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras import backend as K\n",
    "\n",
    "class GroupNorm(Layer):\n",
    "    '''Group normalization layer\n",
    "    Group Normalization divides the channels into groups and computes within each group\n",
    "    the mean and variance for normalization. GN's computation is independent of batch sizes,\n",
    "    and its accuracy is stable in a wide range of batch sizes\n",
    "    # Arguments\n",
    "        groups: Integer, the number of groups for Group Normalization.\n",
    "        axis: Integer, the axis that should be normalized\n",
    "            (typically the features axis).\n",
    "            For instance, after a `Conv2D` layer with\n",
    "            `data_format=\"channels_first\"`,\n",
    "            set `axis=1` in `BatchNormalization`.\n",
    "        epsilon: Small float added to variance to avoid dividing by zero.\n",
    "        center: If True, add offset of `beta` to normalized tensor.\n",
    "            If False, `beta` is ignored.\n",
    "        scale: If True, multiply by `gamma`.\n",
    "            If False, `gamma` is not used.\n",
    "            When the next layer is linear (also e.g. `nn.relu`),\n",
    "            this can be disabled since the scaling\n",
    "            will be done by the next layer.\n",
    "        beta_initializer: Initializer for the beta weight.\n",
    "        gamma_initializer: Initializer for the gamma weight.\n",
    "        beta_regularizer: Optional regularizer for the beta weight.\n",
    "        gamma_regularizer: Optional regularizer for the gamma weight.\n",
    "        beta_constraint: Optional constraint for the beta weight.\n",
    "        gamma_constraint: Optional constraint for the gamma weight.\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "    # Output shape\n",
    "        Same shape as input.\n",
    "    # References\n",
    "        - [Group Normalization](https://arxiv.org/abs/1803.08494)\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 groups=32,\n",
    "                 axis=-1,\n",
    "                 epsilon=1e-8,\n",
    "                 center=True,\n",
    "                 scale=True,\n",
    "                 beta_initializer='zeros',\n",
    "                 gamma_initializer='ones',\n",
    "                 beta_regularizer=None,\n",
    "                 gamma_regularizer=None,\n",
    "                 beta_constraint=None,\n",
    "                 gamma_constraint=None,\n",
    "                 **kwargs):\n",
    "        super(GroupNorm, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.groups = groups\n",
    "        self.axis = axis\n",
    "        self.epsilon = epsilon\n",
    "        self.center = center\n",
    "        self.scale = scale\n",
    "        self.beta_initializer = initializers.get(beta_initializer)\n",
    "        self.gamma_initializer = initializers.get(gamma_initializer)\n",
    "        self.beta_regularizer = regularizers.get(beta_regularizer)\n",
    "        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
    "        self.beta_constraint = constraints.get(beta_constraint)\n",
    "        self.gamma_constraint = constraints.get(gamma_constraint)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        dim = input_shape[self.axis]\n",
    "        if dim is None:\n",
    "            raise ValueError('Axis ' + str(self.axis) + ' of '\n",
    "                             'input tensor should have a defined dimension '\n",
    "                             'but the layer received an input with shape ' +\n",
    "                             str(input_shape) + '.')\n",
    "        if dim < self.groups:\n",
    "            raise ValueError('Number of groups (' + str(self.groups) + ') cannot be '\n",
    "                             'more than the number of channels (' +\n",
    "                             str(dim) + ').')\n",
    "        if dim % self.groups != 0:\n",
    "            raise ValueError('Number of groups (' + str(self.groups) + ') must be a '\n",
    "                             'multiple of the number of channels (' +\n",
    "                             str(dim) + ').')\n",
    "            \n",
    "        self.input_spec = InputSpec(ndim=len(input_shape),\n",
    "                                    axes={self.axis: dim})\n",
    "        shape = (dim,)\n",
    "\n",
    "        if self.scale:\n",
    "            self.gamma = self.add_weight(shape=shape,\n",
    "                                         name='gamma',\n",
    "                                         initializer=self.gamma_initializer,\n",
    "                                         regularizer=self.gamma_regularizer,\n",
    "                                         constraint=self.gamma_constraint)\n",
    "        else:\n",
    "            self.gamma = None\n",
    "\n",
    "        if self.center:\n",
    "            self.beta = self.add_weight(shape=shape,\n",
    "                                        name='beta',\n",
    "                                        initializer=self.beta_initializer,\n",
    "                                        regularizer=self.beta_regularizer,\n",
    "                                        constraint=self.beta_constraint)\n",
    "        else:\n",
    "            self.beta = None\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        input_shape = K.int_shape(inputs)\n",
    "        tensor_input_shape = K.shape(inputs)\n",
    "\n",
    "        # Prepare broadcasting shape.\n",
    "        reduction_axes = list(range(len(input_shape)))\n",
    "        del reduction_axes[self.axis]\n",
    "        broadcast_shape = [1] * len(input_shape)\n",
    "        broadcast_shape[self.axis] = input_shape[self.axis] // self.groups\n",
    "        broadcast_shape.insert(1, self.groups)\n",
    "\n",
    "        reshape_group_shape = K.shape(inputs)\n",
    "        group_axes = [reshape_group_shape[i] for i in range(len(input_shape))]\n",
    "        group_axes[self.axis] = input_shape[self.axis] // self.groups\n",
    "        group_axes.insert(1, self.groups)\n",
    "\n",
    "        # reshape inputs to new group shape\n",
    "        group_shape = [group_axes[0], self.groups] + group_axes[2:]\n",
    "        group_shape = K.stack(group_shape)\n",
    "        inputs = K.reshape(inputs, group_shape)\n",
    "\n",
    "        group_reduction_axes = list(range(len(group_axes)))\n",
    "        group_reduction_axes = group_reduction_axes[2:]\n",
    "\n",
    "        mean = K.mean(inputs, axis=group_reduction_axes, keepdims=True)\n",
    "        variance = K.var(inputs, axis=group_reduction_axes, keepdims=True)\n",
    "\n",
    "        inputs = (inputs - mean) / (K.sqrt(variance + self.epsilon))\n",
    "\n",
    "        # prepare broadcast shape\n",
    "        inputs = K.reshape(inputs, group_shape)\n",
    "        outputs = inputs\n",
    "\n",
    "        # In this case we must explicitly broadcast all parameters.\n",
    "        if self.scale:\n",
    "            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n",
    "            outputs = outputs * broadcast_gamma\n",
    "\n",
    "        if self.center:\n",
    "            broadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
    "            outputs = outputs + broadcast_beta\n",
    "\n",
    "        outputs = K.reshape(outputs, tensor_input_shape)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'groups': self.groups,\n",
    "            'axis': self.axis,\n",
    "            'epsilon': self.epsilon,\n",
    "            'center': self.center,\n",
    "            'scale': self.scale,\n",
    "            'beta_initializer': initializers.serialize(self.beta_initializer),\n",
    "            'gamma_initializer': initializers.serialize(self.gamma_initializer),\n",
    "            'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n",
    "            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n",
    "            'beta_constraint': constraints.serialize(self.beta_constraint),\n",
    "            'gamma_constraint': constraints.serialize(self.gamma_constraint)\n",
    "        }\n",
    "        base_config = super(GroupNorm, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "    \n",
    "def normalization(x, method=\"bn\"):\n",
    "    if method is \"bn\":\n",
    "        x = KL.BatchNormalization(axis=-1)(x)\n",
    "    elif method is \"gn\":\n",
    "        x = GroupNorm(groups=16)(x)\n",
    "    else:\n",
    "        \"\"\"Do nothing\"\"\"\n",
    "        x = x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n",
      "50000/50000 [==============================] - 25s 496us/step - loss: 1.7038 - acc: 0.3735 - val_loss: 1.4883 - val_acc: 0.4629\n",
      "Epoch 2/25\n",
      "50000/50000 [==============================] - 19s 380us/step - loss: 1.4224 - acc: 0.4885 - val_loss: 1.3351 - val_acc: 0.5153\n",
      "Epoch 3/25\n",
      "50000/50000 [==============================] - 19s 377us/step - loss: 1.3038 - acc: 0.5356 - val_loss: 1.2606 - val_acc: 0.5409\n",
      "Epoch 4/25\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 1.2264 - acc: 0.5651 - val_loss: 1.2169 - val_acc: 0.5581\n",
      "Epoch 5/25\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 1.1686 - acc: 0.5844 - val_loss: 1.2168 - val_acc: 0.5651\n",
      "Epoch 6/25\n",
      "50000/50000 [==============================] - 19s 372us/step - loss: 1.1210 - acc: 0.6039 - val_loss: 1.0939 - val_acc: 0.6122\n",
      "Epoch 7/25\n",
      "50000/50000 [==============================] - 19s 377us/step - loss: 1.0814 - acc: 0.6193 - val_loss: 1.0582 - val_acc: 0.6280\n",
      "Epoch 8/25\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 1.0445 - acc: 0.6313 - val_loss: 1.0583 - val_acc: 0.6269\n",
      "Epoch 9/25\n",
      "50000/50000 [==============================] - 18s 364us/step - loss: 1.0141 - acc: 0.6448 - val_loss: 1.1120 - val_acc: 0.6090\n",
      "Epoch 10/25\n",
      "50000/50000 [==============================] - 19s 372us/step - loss: 0.9907 - acc: 0.6505 - val_loss: 0.9834 - val_acc: 0.6485\n",
      "Epoch 11/25\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.9682 - acc: 0.6600 - val_loss: 0.9874 - val_acc: 0.6515\n",
      "Epoch 12/25\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.9424 - acc: 0.6701 - val_loss: 1.0400 - val_acc: 0.6260\n",
      "Epoch 13/25\n",
      "50000/50000 [==============================] - 19s 378us/step - loss: 0.9196 - acc: 0.6779 - val_loss: 0.9400 - val_acc: 0.6715\n",
      "Epoch 14/25\n",
      "50000/50000 [==============================] - 20s 393us/step - loss: 0.9021 - acc: 0.6833 - val_loss: 0.8752 - val_acc: 0.6968\n",
      "Epoch 15/25\n",
      "50000/50000 [==============================] - 19s 380us/step - loss: 0.8840 - acc: 0.6911 - val_loss: 0.8895 - val_acc: 0.6915\n",
      "Epoch 16/25\n",
      "50000/50000 [==============================] - 18s 360us/step - loss: 0.8683 - acc: 0.6964 - val_loss: 0.9077 - val_acc: 0.6834\n",
      "Epoch 17/25\n",
      "50000/50000 [==============================] - 22s 439us/step - loss: 0.8467 - acc: 0.7033 - val_loss: 0.9006 - val_acc: 0.6746\n",
      "Epoch 18/25\n",
      "50000/50000 [==============================] - 21s 420us/step - loss: 0.8340 - acc: 0.7087 - val_loss: 0.8458 - val_acc: 0.7083\n",
      "Epoch 19/25\n",
      "50000/50000 [==============================] - 20s 400us/step - loss: 0.8200 - acc: 0.7123 - val_loss: 0.8587 - val_acc: 0.6984\n",
      "Epoch 20/25\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.8021 - acc: 0.7203 - val_loss: 0.8067 - val_acc: 0.7225\n",
      "Epoch 21/25\n",
      "50000/50000 [==============================] - 20s 406us/step - loss: 0.7873 - acc: 0.7276 - val_loss: 0.8067 - val_acc: 0.7151\n",
      "Epoch 22/25\n",
      "50000/50000 [==============================] - 19s 375us/step - loss: 0.7780 - acc: 0.7283 - val_loss: 0.8754 - val_acc: 0.6925\n",
      "Epoch 23/25\n",
      "50000/50000 [==============================] - 19s 375us/step - loss: 0.7627 - acc: 0.7346 - val_loss: 0.8331 - val_acc: 0.7030\n",
      "Epoch 24/25\n",
      "50000/50000 [==============================] - 20s 397us/step - loss: 0.7501 - acc: 0.7375 - val_loss: 0.8314 - val_acc: 0.7096\n",
      "Epoch 25/25\n",
      "50000/50000 [==============================] - 20s 401us/step - loss: 0.7436 - acc: 0.7411 - val_loss: 0.8178 - val_acc: 0.7054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7d17aaa8d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "normal_method = \"gn\"\n",
    "\n",
    "inputs = KL.Input(shape=[32, 32, 3], name='inputs')\n",
    "x = KL.Conv2D(filters=64, kernel_size=3, padding='same')(inputs)\n",
    "x = normalization(x, method=normal_method)\n",
    "x = KL.Activation(\"relu\")(x)\n",
    "x = KL.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "x = KL.Conv2D(filters=128, kernel_size=3, padding='same')(x)\n",
    "x = normalization(x, method=normal_method)\n",
    "x = KL.Activation(\"relu\")(x)\n",
    "x = KL.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "x = KL.Conv2D(filters=128, kernel_size=3, padding='same')(x)\n",
    "x = normalization(x, method=normal_method)\n",
    "x = KL.Activation(\"relu\")(x)\n",
    "x = KL.GlobalAveragePooling2D()(x)\n",
    "\n",
    "out = KL.Dense(units=10, activation=\"softmax\", name=\"output\")(x)\n",
    "\n",
    "model = keras.models.Model(inputs=[inputs], outputs=[out])\n",
    "\n",
    "optim = keras.optimizers.Adam(lr=0.0001)\n",
    "model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optim)\n",
    "\n",
    "model.fit(x_train, y_train, epochs=25, validation_data=(x_valid, y_valid), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n",
      "50000/50000 [==============================] - 16s 328us/step - loss: 1.5328 - acc: 0.4639 - val_loss: 1.3422 - val_acc: 0.5169\n",
      "Epoch 2/25\n",
      "50000/50000 [==============================] - 17s 331us/step - loss: 1.2586 - acc: 0.5634 - val_loss: 1.2961 - val_acc: 0.5372\n",
      "Epoch 3/25\n",
      "50000/50000 [==============================] - 17s 332us/step - loss: 1.1513 - acc: 0.6036 - val_loss: 1.3860 - val_acc: 0.5053\n",
      "Epoch 4/25\n",
      "50000/50000 [==============================] - 15s 307us/step - loss: 1.0790 - acc: 0.6289 - val_loss: 1.2904 - val_acc: 0.5376\n",
      "Epoch 5/25\n",
      "50000/50000 [==============================] - 14s 282us/step - loss: 1.0229 - acc: 0.6458 - val_loss: 1.5936 - val_acc: 0.4464\n",
      "Epoch 6/25\n",
      "50000/50000 [==============================] - 14s 287us/step - loss: 0.9808 - acc: 0.6609 - val_loss: 1.4047 - val_acc: 0.5224\n",
      "Epoch 7/25\n",
      "50000/50000 [==============================] - 15s 306us/step - loss: 0.9440 - acc: 0.6753 - val_loss: 0.9795 - val_acc: 0.6508\n",
      "Epoch 8/25\n",
      "50000/50000 [==============================] - 15s 309us/step - loss: 0.9066 - acc: 0.6878 - val_loss: 1.3975 - val_acc: 0.5420\n",
      "Epoch 9/25\n",
      "50000/50000 [==============================] - 15s 304us/step - loss: 0.8796 - acc: 0.6969 - val_loss: 1.0174 - val_acc: 0.6402\n",
      "Epoch 10/25\n",
      "50000/50000 [==============================] - 15s 300us/step - loss: 0.8511 - acc: 0.7083 - val_loss: 1.1234 - val_acc: 0.6195\n",
      "Epoch 11/25\n",
      "50000/50000 [==============================] - 16s 321us/step - loss: 0.8289 - acc: 0.7150 - val_loss: 1.1005 - val_acc: 0.6096\n",
      "Epoch 12/25\n",
      "50000/50000 [==============================] - 16s 311us/step - loss: 0.8026 - acc: 0.7250 - val_loss: 0.9304 - val_acc: 0.6723\n",
      "Epoch 13/25\n",
      "50000/50000 [==============================] - 16s 327us/step - loss: 0.7811 - acc: 0.7329 - val_loss: 0.9199 - val_acc: 0.6782\n",
      "Epoch 14/25\n",
      "50000/50000 [==============================] - 14s 280us/step - loss: 0.7600 - acc: 0.7385 - val_loss: 1.2099 - val_acc: 0.5994\n",
      "Epoch 15/25\n",
      "50000/50000 [==============================] - 14s 285us/step - loss: 0.7462 - acc: 0.7428 - val_loss: 0.9128 - val_acc: 0.6692\n",
      "Epoch 16/25\n",
      "50000/50000 [==============================] - 14s 288us/step - loss: 0.7249 - acc: 0.7525 - val_loss: 0.9633 - val_acc: 0.6763\n",
      "Epoch 17/25\n",
      "50000/50000 [==============================] - 15s 290us/step - loss: 0.7111 - acc: 0.7576 - val_loss: 1.0566 - val_acc: 0.6463\n",
      "Epoch 18/25\n",
      "50000/50000 [==============================] - 14s 288us/step - loss: 0.6934 - acc: 0.7629 - val_loss: 1.2087 - val_acc: 0.6095\n",
      "Epoch 19/25\n",
      "50000/50000 [==============================] - 14s 283us/step - loss: 0.6791 - acc: 0.7673 - val_loss: 1.4758 - val_acc: 0.5450\n",
      "Epoch 20/25\n",
      "50000/50000 [==============================] - 14s 281us/step - loss: 0.6690 - acc: 0.7708 - val_loss: 1.0089 - val_acc: 0.6700\n",
      "Epoch 21/25\n",
      "50000/50000 [==============================] - 14s 290us/step - loss: 0.6510 - acc: 0.7796 - val_loss: 0.9850 - val_acc: 0.6597\n",
      "Epoch 22/25\n",
      "50000/50000 [==============================] - 14s 290us/step - loss: 0.6390 - acc: 0.7807 - val_loss: 0.8188 - val_acc: 0.7166\n",
      "Epoch 23/25\n",
      "50000/50000 [==============================] - 15s 302us/step - loss: 0.6282 - acc: 0.7858 - val_loss: 1.0090 - val_acc: 0.6593\n",
      "Epoch 24/25\n",
      "50000/50000 [==============================] - 15s 304us/step - loss: 0.6122 - acc: 0.7910 - val_loss: 1.0109 - val_acc: 0.6497\n",
      "Epoch 25/25\n",
      "50000/50000 [==============================] - 15s 296us/step - loss: 0.6050 - acc: 0.7957 - val_loss: 1.1307 - val_acc: 0.6343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7d12b914e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "normal_method = \"bn\"\n",
    "\n",
    "inputs = KL.Input(shape=[32, 32, 3], name='inputs')\n",
    "x = KL.Conv2D(filters=64, kernel_size=3, padding='same')(inputs)\n",
    "x = normalization(x, method=normal_method)\n",
    "x = KL.Activation(\"relu\")(x)\n",
    "x = KL.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "x = KL.Conv2D(filters=128, kernel_size=3, padding='same')(x)\n",
    "x = normalization(x, method=normal_method)\n",
    "x = KL.Activation(\"relu\")(x)\n",
    "x = KL.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "x = KL.Conv2D(filters=128, kernel_size=3, padding='same')(x)\n",
    "x = normalization(x, method=normal_method)\n",
    "x = KL.Activation(\"relu\")(x)\n",
    "x = KL.GlobalAveragePooling2D()(x)\n",
    "\n",
    "out = KL.Dense(units=10, activation=\"softmax\", name=\"output\")(x)\n",
    "\n",
    "model = keras.models.Model(inputs=[inputs], outputs=[out])\n",
    "\n",
    "optim = keras.optimizers.Adam(lr=0.0001)\n",
    "model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optim)\n",
    "\n",
    "model.fit(x_train, y_train, epochs=25, validation_data=(x_valid, y_valid), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n",
      "50000/50000 [==============================] - 11s 222us/step - loss: 1.9340 - acc: 0.2793 - val_loss: 1.7716 - val_acc: 0.3563\n",
      "Epoch 2/25\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 1.7259 - acc: 0.3551 - val_loss: 1.6724 - val_acc: 0.3820\n",
      "Epoch 3/25\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 1.6424 - acc: 0.3944 - val_loss: 1.6015 - val_acc: 0.4051\n",
      "Epoch 4/25\n",
      "50000/50000 [==============================] - 10s 200us/step - loss: 1.5895 - acc: 0.4226 - val_loss: 1.5377 - val_acc: 0.4416\n",
      "Epoch 5/25\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 1.5449 - acc: 0.4414 - val_loss: 1.5244 - val_acc: 0.4492\n",
      "Epoch 6/25\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 1.5025 - acc: 0.4590 - val_loss: 1.4919 - val_acc: 0.4536\n",
      "Epoch 7/25\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 1.4685 - acc: 0.4733 - val_loss: 1.4392 - val_acc: 0.4829\n",
      "Epoch 8/25\n",
      "50000/50000 [==============================] - 11s 222us/step - loss: 1.4361 - acc: 0.4880 - val_loss: 1.4261 - val_acc: 0.4959\n",
      "Epoch 9/25\n",
      "50000/50000 [==============================] - 11s 211us/step - loss: 1.4092 - acc: 0.4935 - val_loss: 1.3855 - val_acc: 0.5021\n",
      "Epoch 10/25\n",
      "50000/50000 [==============================] - 10s 200us/step - loss: 1.3834 - acc: 0.5066 - val_loss: 1.3591 - val_acc: 0.5199\n",
      "Epoch 11/25\n",
      "50000/50000 [==============================] - 10s 194us/step - loss: 1.3619 - acc: 0.5127 - val_loss: 1.3294 - val_acc: 0.5342\n",
      "Epoch 12/25\n",
      "50000/50000 [==============================] - 10s 198us/step - loss: 1.3427 - acc: 0.5231 - val_loss: 1.3257 - val_acc: 0.5363\n",
      "Epoch 13/25\n",
      "50000/50000 [==============================] - 11s 211us/step - loss: 1.3211 - acc: 0.5322 - val_loss: 1.3093 - val_acc: 0.5387\n",
      "Epoch 14/25\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 1.3038 - acc: 0.5376 - val_loss: 1.2870 - val_acc: 0.5455\n",
      "Epoch 15/25\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 1.2868 - acc: 0.5434 - val_loss: 1.2943 - val_acc: 0.5415\n",
      "Epoch 16/25\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 1.2706 - acc: 0.5501 - val_loss: 1.2804 - val_acc: 0.5436\n",
      "Epoch 17/25\n",
      "50000/50000 [==============================] - 11s 210us/step - loss: 1.2530 - acc: 0.5562 - val_loss: 1.2457 - val_acc: 0.5586\n",
      "Epoch 18/25\n",
      "50000/50000 [==============================] - 11s 212us/step - loss: 1.2391 - acc: 0.5614 - val_loss: 1.2362 - val_acc: 0.5618\n",
      "Epoch 19/25\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 1.2223 - acc: 0.5686 - val_loss: 1.2026 - val_acc: 0.5770\n",
      "Epoch 20/25\n",
      "50000/50000 [==============================] - 10s 200us/step - loss: 1.2093 - acc: 0.5746 - val_loss: 1.2144 - val_acc: 0.5708\n",
      "Epoch 21/25\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 1.1981 - acc: 0.5769 - val_loss: 1.2131 - val_acc: 0.5751\n",
      "Epoch 22/25\n",
      "50000/50000 [==============================] - 11s 213us/step - loss: 1.1820 - acc: 0.5848 - val_loss: 1.2339 - val_acc: 0.5601\n",
      "Epoch 23/25\n",
      "50000/50000 [==============================] - 9s 187us/step - loss: 1.1716 - acc: 0.5874 - val_loss: 1.1676 - val_acc: 0.5921\n",
      "Epoch 24/25\n",
      "50000/50000 [==============================] - 10s 192us/step - loss: 1.1609 - acc: 0.5923 - val_loss: 1.1691 - val_acc: 0.5912\n",
      "Epoch 25/25\n",
      "50000/50000 [==============================] - 10s 192us/step - loss: 1.1461 - acc: 0.5982 - val_loss: 1.1304 - val_acc: 0.6027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7bd0792da0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "normal_method = None\n",
    "\n",
    "inputs = KL.Input(shape=[32, 32, 3], name='inputs')\n",
    "x = KL.Conv2D(filters=64, kernel_size=3, padding='same')(inputs)\n",
    "x = normalization(x, method=normal_method)\n",
    "x = KL.Activation(\"relu\")(x)\n",
    "x = KL.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "x = KL.Conv2D(filters=128, kernel_size=3, padding='same')(x)\n",
    "x = normalization(x, method=normal_method)\n",
    "x = KL.Activation(\"relu\")(x)\n",
    "x = KL.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "x = KL.Conv2D(filters=128, kernel_size=3, padding='same')(x)\n",
    "x = normalization(x, method=normal_method)\n",
    "x = KL.Activation(\"relu\")(x)\n",
    "x = KL.GlobalAveragePooling2D()(x)\n",
    "\n",
    "out = KL.Dense(units=10, activation=\"softmax\", name=\"output\")(x)\n",
    "\n",
    "model = keras.models.Model(inputs=[inputs], outputs=[out])\n",
    "\n",
    "optim = keras.optimizers.Adam(lr=0.0001)\n",
    "model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optim)\n",
    "\n",
    "model.fit(x_train, y_train, epochs=25, validation_data=(x_valid, y_valid), shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {
    "6b1438b3075f49289cfcf03a4fce2ccb": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "ab7848b490e5454e9a42f439a6ef6b31": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "e0e3b0d66b1e47c4ade6c276bacc5c55": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
