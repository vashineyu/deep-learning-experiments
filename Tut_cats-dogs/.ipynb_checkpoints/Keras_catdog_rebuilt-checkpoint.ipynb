{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter setting and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=32, do_augment=True, dq_size=6, epochs=100, gpu_id=5, image_dir='/data/seanyu/cat_dog/dataset/', image_size=(256, 256, 3), lr=0.0001, model_file_name='model.h5', n_batch=100, n_classes=2, n_threads=4, save_dir='./result', train_ratio=0.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from time import sleep\n",
    "from tqdm import tqdm # if use notebook\n",
    "\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Event\n",
    "import queue\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import random\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gpu_id', default=5)\n",
    "parser.add_argument('--image_dir', default=\"/data/seanyu/cat_dog/dataset/\")\n",
    "parser.add_argument('--save_dir', default='./result')\n",
    "parser.add_argument('--batch_size', default=32, type=int)\n",
    "parser.add_argument('--do_augment', default=True, type = bool)\n",
    "parser.add_argument('--epochs', default=100, type=int)\n",
    "parser.add_argument('--lr', default=1e-4, type=float)\n",
    "parser.add_argument('--image_size', default=(256,256,3), type = int)\n",
    "parser.add_argument('--n_classes', default=2, type = int)\n",
    "parser.add_argument('--n_batch', default=100, type = int)\n",
    "parser.add_argument('--train_ratio', default=0.9, type = float)\n",
    "parser.add_argument('--model_file_name', default = 'model.h5')\n",
    "parser.add_argument('--n_threads', default = 4, type = int)\n",
    "parser.add_argument('--dq_size', default = 6, type = int)\n",
    "\n",
    "FLAGS = parser.parse_args([])\n",
    "print(FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS.gpu_id = \"7\"\n",
    "FLAGS.image_dir = \"/data/seanyu/cat_dog/dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check path and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(FLAGS.gpu_id)\n",
    "import tensorflow as tf\n",
    "\n",
    "if not os.path.exists(FLAGS.save_dir):\n",
    "    os.makedirs(FLAGS.save_dir)\n",
    "\n",
    "model_dir = FLAGS.save_dir + '/model'\n",
    "\n",
    "\"\"\"  Get data \"\"\"\n",
    "d_train = FLAGS.image_dir + '/train/'\n",
    "d_test = FLAGS.image_dir + '/test1/'\n",
    "\n",
    "image_train_list = glob.glob(d_train + '*.jpg')\n",
    "image_test_list = glob.glob(d_test + '*.jpg')\n",
    "\n",
    "df_train = pd.DataFrame({'img_path': image_train_list})\n",
    "df_test = pd.DataFrame({'img_path': image_test_list})\n",
    "\n",
    "df_train['cate'] = df_train.img_path.apply(os.path.basename)\n",
    "df_train['cate'] = [i.split(\".\")[0] for i in list(df_train.cate)]\n",
    "df_train.cate = df_train.cate.replace({'dog': 0, 'cat': 1})\n",
    "\n",
    "nb_epoch = FLAGS.epochs\n",
    "\n",
    "df_train_0, df_val_0 = train_test_split(df_train[df_train['cate'] == 0], test_size = 1-FLAGS.train_ratio)\n",
    "df_train_1, df_val_1 = train_test_split(df_train[df_train['cate'] == 1], test_size = 1-FLAGS.train_ratio)\n",
    "\n",
    "df_val = pd.concat((df_val_0, df_val_1)).reset_index(drop = True)\n",
    "\n",
    "del df_val_0, df_val_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import imgaug as ia\n",
    "    from imgaug import augmenters as iaa\n",
    "except:\n",
    "    print(\"Import Error, Please make sure you have imgaug\")\n",
    "        \n",
    "try:\n",
    "    import sys\n",
    "    sys.path.append(\"/mnt/deep-learning/usr/seanyu/common_tools/\")\n",
    "    from customized_imgaug_func import keypoint_func, img_channelswap\n",
    "except:\n",
    "    print(\"Warning, if you used customized imgaug function\")\n",
    "    \n",
    "class Augmentation_Setup(object):  \n",
    "    sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "    lesstimes = lambda aug: iaa.Sometimes(0.2, aug)\n",
    "    \n",
    "    augmentation = iaa.Sequential([\n",
    "        iaa.Fliplr(0.5, name=\"FlipLR\"),\n",
    "        iaa.Flipud(0.5, name=\"FlipUD\"),\n",
    "        iaa.OneOf([iaa.Affine(rotate = 90),\n",
    "                   iaa.Affine(rotate = 180),\n",
    "                   iaa.Affine(rotate = 270)]),\n",
    "        sometimes(iaa.Affine(\n",
    "                    scale = (0.8,1.2),\n",
    "                    translate_percent = (-0.2, 0.2),\n",
    "                    rotate = (-15, 15),\n",
    "                    mode = 'wrap'\n",
    "                    ))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetDataset():\n",
    "    def __init__(self, df_list, class_id, n_classes, f_input_preproc, image_size=(256,256,3), onehot=True, augmentation=None):\n",
    "        \n",
    "        self.df_list = df_list\n",
    "        self.class_id = class_id\n",
    "        self.n_classes = n_classes\n",
    "        self.preproc = f_input_preproc\n",
    "        self.image_size = image_size\n",
    "        self.onehot = onehot\n",
    "        self.aug = augmentation\n",
    "        \n",
    "        ## Init ##\n",
    "        self.df_list = self.df_list.sample(frac=1.).reset_index(drop=True)\n",
    "        self.current_index = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img = self.load_image(img_path=self.df_list.iloc[self.current_index]['img_path'], image_size=self.image_size)\n",
    "        \n",
    "        if self.aug is not None:\n",
    "            img = self.aug.augment_image(img)\n",
    "            \n",
    "        img = img.astype(np.float32)\n",
    "        \n",
    "        if self.preproc is not None:\n",
    "            img = self.preproc(img)\n",
    "        \n",
    "        label = self.class_id\n",
    "        if self.onehot:\n",
    "             label = tf.keras.utils.to_categorical(label, num_classes=self.n_classes)\n",
    "        \n",
    "        self.current_index = (self.current_index + 1) % len(self.df_list)\n",
    "        return img, label\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.__getitem__(idx=self.current_index)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_image(img_path, image_size):\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (image_size[0], image_size[1]))\n",
    "        return img\n",
    "    \n",
    "class Customized_dataloader():\n",
    "    \"\"\"\n",
    "    1. Compose multiple generators together\n",
    "    2. Make this composed generator into multi-processing function\n",
    "    \"\"\"\n",
    "    def __init__(self, list_dataset, batch_size_per_dataset=16, queue_size=128, num_workers=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - list_dataset: put generator object as list [gen1, gen2, ...]\n",
    "            - batch_size_per_dataset: bz for each generator (total_batch_size/n_class)\n",
    "            - queue_size: queue size\n",
    "            - num_workers: start n workers to get data\n",
    "        \n",
    "        Action: Call with next\n",
    "        \"\"\"\n",
    "        self.list_dataset = list_dataset\n",
    "        self.batch_size_per_dataset = batch_size_per_dataset\n",
    "        self.sample_queue = mp.Queue(maxsize = queue_size)\n",
    "        \n",
    "        self.jobs = num_workers\n",
    "        self.events = list()\n",
    "        self.workers = list()\n",
    "        for i in range(num_workers):\n",
    "            event = Event()\n",
    "            work = mp.Process(target = enqueue, args = (self.sample_queue, event, self.compose_data))\n",
    "            work.daemon = True\n",
    "            work.start()\n",
    "            self.events.append(event)\n",
    "            self.workers.append(work)\n",
    "        print(\"workers ready\")\n",
    "        \n",
    "    def __next__(self):\n",
    "        return self.sample_queue.get()\n",
    "    \n",
    "    def compose_data(self):\n",
    "        while True:\n",
    "            imgs, labels = [], []\n",
    "            for z in range(self.batch_size_per_dataset):\n",
    "                data = [next(i) for i in self.list_dataset]\n",
    "                img, label = zip(*data)\n",
    "                imgs.append(np.array(img))\n",
    "                labels.append(np.array(label))\n",
    "            yield np.concatenate(imgs), np.concatenate(labels)\n",
    "    \n",
    "    def stop_worker(self):\n",
    "        for t in self.events:\n",
    "            t.set()\n",
    "        for i, t in enumerate(self.workers):\n",
    "            t.join(timeout = 1)\n",
    "        print(\"all_worker_stop\")\n",
    "\n",
    "# ----- #\n",
    "def enqueue(queue, stop, gen_func):\n",
    "    gen = gen_func()\n",
    "    while True:\n",
    "        if stop.is_set():\n",
    "            return\n",
    "        queue.put(next(gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(img):\n",
    "    #return (img - img.min()) / (img.max() - img.min())\n",
    "    return img / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_RESNET_PREPROC = False\n",
    "dog_train = GetDataset(df_list=df_train[df_train['cate'] == 0],\n",
    "                       class_id=0, n_classes=2,\n",
    "                       f_input_preproc=preproc if not USE_RESNET_PREPROC else tf.keras.applications.resnet50.preprocess_input,\n",
    "                       augmentation=Augmentation_Setup.augmentation, \n",
    "                       onehot= True, \n",
    "                       image_size=(256,256,3))\n",
    "\n",
    "cat_train = GetDataset(df_list=df_train[df_train['cate'] == 1], \n",
    "                       class_id=1, n_classes=2, \n",
    "                       f_input_preproc=preproc if not USE_RESNET_PREPROC else tf.keras.applications.resnet50.preprocess_input,\n",
    "                       augmentation=Augmentation_Setup.augmentation, \n",
    "                       onehot= True, \n",
    "                       image_size=(256,256,3))\n",
    "\n",
    "dog_valid = GetDataset(df_list=df_val[df_val['cate'] == 0], \n",
    "                       class_id=0, n_classes=2,\n",
    "                       f_input_preproc=preproc if not USE_RESNET_PREPROC else tf.keras.applications.resnet50.preprocess_input,\n",
    "                       augmentation=None, \n",
    "                       onehot= True, \n",
    "                       image_size=(256,256,3))\n",
    "\n",
    "cat_valid = GetDataset(df_list=df_val[df_val['cate'] == 1], \n",
    "                       class_id=1, n_classes=2, \n",
    "                       f_input_preproc=preproc if not USE_RESNET_PREPROC else tf.keras.applications.resnet50.preprocess_input,\n",
    "                       augmentation=None, \n",
    "                       onehot= True, \n",
    "                       image_size=(256,256,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workers ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_worker_stop\n",
      "(320, 256, 256, 3)\n",
      "(320, 2)\n",
      "[160. 160.]\n"
     ]
    }
   ],
   "source": [
    "valid_gen = Customized_dataloader([dog_valid, cat_valid], batch_size_per_dataset=FLAGS.batch_size//2, num_workers=2, queue_size=10)\n",
    "x_val, y_val = [], []\n",
    "for _ in tqdm(range(10)):\n",
    "    a,b = next(valid_gen)\n",
    "    x_val.append(a)\n",
    "    y_val.append(b)\n",
    "x_val = np.concatenate(x_val)\n",
    "y_val = np.concatenate(y_val)\n",
    "valid_gen.stop_worker()\n",
    "\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "print(y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Use keras, official resnet\\nimport keras\\nclass Build_FunctionalModel():\\n    def __init__(self, input_shape, classes, backbone=\\'resnet50\\', use_pretrain=False):\\n        self.input_layer = keras.layers.Input(shape=input_shape, name=\"input\")\\n        graph_pool = keras.applications.ResNet50(input_tensor=self.input_layer, include_top=False)\\n        gap = keras.layers.GlobalAveragePooling2D()(graph_pool.output)\\n        self.logit = keras.layers.Dense(units=classes, name=\"logit\")(gap)\\n        self.out = keras.layers.Activation(\"softmax\", name=\"output\")(self.logit)\\n\\n    def build(self):\\n        return keras.models.Model(inputs=self.input_layer, outputs=self.out)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.utils import get_file\n",
    "sys.path.append(\"/mnt/deep-learning/usr/seanyu/lab_mldl_tools/models/\")\n",
    "\n",
    "\n",
    "\n",
    "TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/'\\\n",
    "                         'releases/download/v0.2/'\\\n",
    "                         'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "\"\"\"\n",
    "# use tf.keras rebulit\n",
    "from tf_resnet.model import set_custom_objects, resnet_graph\n",
    "class Build_FunctionalModel():\n",
    "    def __init__(self, input_shape, classes, backbone='resnet50', use_pretrain=False):\n",
    "        input_layer = tf.keras.layers.Input(shape=input_shape, name=\"input\")\n",
    "        self.pretrain_modules, stage_layers = resnet_graph(input_tensor=input_layer, \n",
    "                                                           architecture=backbone, \n",
    "                                                           train_bn=True, norm_use='gn')\n",
    "        if use_pretrain:\n",
    "            weight_path = get_file('resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5', \n",
    "                                   TF_WEIGHTS_PATH_NO_TOP, \n",
    "                                   cache_subdir=\"models\", \n",
    "                                   md5_hash='a268eb855778b3df3c7506639542a6af')\n",
    "            self.pretrain_modules.load_weights(weight_path, by_name=True)\n",
    "\n",
    "        self.out = tf.keras.layers.Dense(units=classes, name=\"output\", activation=\"softmax\")(self.pretrain_modules.output)\n",
    "    \n",
    "    def build(self):\n",
    "        return tf.keras.Model(inputs=[self.pretrain_modules.input], outputs=[self.out])\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Use keras, rebuilt resnet\n",
    "import keras\n",
    "from keras_resnet_rebuilt.model import set_custom_objects, resnet_graph\n",
    "\n",
    "\"\"\"\n",
    "class Build_FunctionalModel():\n",
    "    def __init__(self, input_shape, classes, backbone='resnet50', use_pretrain=False):\n",
    "        input_layer = keras.layers.Input(shape=input_shape, name=\"input\")\n",
    "        self.pretrain_modules, stage_layers = resnet_graph(input_tensor=input_layer, \n",
    "                                                           input_norm=False,\n",
    "                                                           architecture=backbone, \n",
    "                                                           train_bn=True, norm_use='gn')\n",
    "        if use_pretrain:\n",
    "            weight_path = get_file('resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5', \n",
    "                                   TF_WEIGHTS_PATH_NO_TOP, \n",
    "                                   cache_subdir=\"models\", \n",
    "                                   md5_hash='a268eb855778b3df3c7506639542a6af')\n",
    "            self.pretrain_modules.load_weights(weight_path, by_name=True)\n",
    "        \n",
    "        logit = keras.layers.Dense(units=classes, name=\"logit\")(self.pretrain_modules.output)\n",
    "        self.out = keras.layers.Activation(\"softmax\", name=\"output\")(logit)\n",
    "\n",
    "        #self.out = keras.layers.Dense(units=classes, name=\"output\", activation=\"softmax\")(self.pretrain_modules.output)\n",
    "    \n",
    "    def build(self):\n",
    "        return keras.Model(inputs=[self.pretrain_modules.input], outputs=[self.out])\n",
    "\"\"\"\n",
    "\n",
    "input_layer = keras.layers.Input(shape=x_val.shape[1:], name=\"input\")\n",
    "pertrain_module , _ =  resnet_graph(input_tensor=input_layer, \n",
    "                                    input_norm=False,\n",
    "                                    architecture=\"resnet50\", \n",
    "                                    train_bn=True, \n",
    "                                    norm_use='gn')\n",
    "out = keras.layers.Dense(units=2, activation=\"softmax\", name=\"output\")(pertrain_module.output)\n",
    "model = keras.models.Model(inputs=[input_layer], outputs=[out])\n",
    "optim = keras.optimizers.SGD(lr=FLAGS.lr, nesterov=True, clipvalue=5, momentum=0.95)\n",
    "model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optim)\n",
    "\n",
    "\"\"\"\n",
    "# Use tf.keras, official resnet\n",
    "class Build_FunctionalModel():\n",
    "    def __init__(self, input_shape, classes, backbone='resnet50', use_pretrain=False):\n",
    "        \n",
    "        self.input_layer = tf.keras.layers.Input(shape=input_shape, name=\"input\")\n",
    "        graph_pool = tf.keras.applications.ResNet50(input_tensor=self.input_layer, include_top=False)\n",
    "        gap = tf.keras.layers.GlobalAveragePooling2D()(graph_pool.output)\n",
    "        self.logit = tf.keras.layers.Dense(units=classes, name=\"logit\")(gap)\n",
    "        self.out = tf.keras.layers.Activation(\"softmax\", name=\"output\")(self.logit)\n",
    "        \n",
    "        \n",
    "        #self.graph_pool = tf.keras.applications.resnet50.ResNet50(input_shape=input_shape, include_top=False)#weights='imagenet')\n",
    "        #gap = tf.keras.layers.GlobalAveragePooling2D()(self.graph_pool.output)\n",
    "        #self.out = tf.keras.layers.Dense(units=classes, name=\"output\", activation=\"softmax\")(gap)\n",
    "    def build(self):\n",
    "        #return tf.keras.models.Model(inputs=self.graph_pool.input, outputs=self.out)\n",
    "        return tf.keras.models.Model(inputs=self.input_layer, outputs=self.out)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# Use keras, official resnet\n",
    "import keras\n",
    "class Build_FunctionalModel():\n",
    "    def __init__(self, input_shape, classes, backbone='resnet50', use_pretrain=False):\n",
    "        self.input_layer = keras.layers.Input(shape=input_shape, name=\"input\")\n",
    "        graph_pool = keras.applications.ResNet50(input_tensor=self.input_layer, include_top=False)\n",
    "        gap = keras.layers.GlobalAveragePooling2D()(graph_pool.output)\n",
    "        self.logit = keras.layers.Dense(units=classes, name=\"logit\")(gap)\n",
    "        self.out = keras.layers.Activation(\"softmax\", name=\"output\")(self.logit)\n",
    "\n",
    "    def build(self):\n",
    "        return keras.models.Model(inputs=self.input_layer, outputs=self.out)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Build_FunctionalModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d3da4114c914>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBuild_FunctionalModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"resnet50\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_pretrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Build_FunctionalModel' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "pretrain_model = tf.keras.applications.resnet50.ResNet50(include_top=False, input_shape=x_val.shape[1:], weights='imagenet')\n",
    "gap = tf.keras.layers.GlobalAveragePooling2D()(pretrain_model.output)\n",
    "model_output = tf.keras.layers.Dense(units=2, activation='softmax', name='output')(gap)\n",
    "model = tf.keras.models.Model(inputs = [pretrain_model.input], outputs = [model_output])\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model = Build_FunctionalModel(input_shape=[256, 256, 3], classes=2, backbone=\"resnet50\", use_pretrain=False)\n",
    "model = model.build()\n",
    "\n",
    "#optim = tf.keras.optimizers.Adam(lr=FLAGS.lr)\n",
    "optim = keras.optimizers.SGD(lr=FLAGS.lr, nesterov=True, clipvalue=5, momentum=0.95)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              metrics=[\"accuracy\"], \n",
    "              optimizer=optim)\n",
    "\n",
    "\"\"\"\n",
    "model.compile(loss=\"categorical_crossentropy\", \n",
    "              metrics=[\"accuracy\"], \n",
    "              optimizer=keras.optimizers.Adam(lr=FLAGS.lr))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsys.path.append(\"/home/seanyu/research/Keras-Group-Normalization/\")\\n\\nimport resnet_groupnorm as resnet\\nimport keras\\nimport keras.backend as K\\n\\nK.clear_session()\\n\\nimg_dim = (256, 256, 3)\\npretrain_modules =resnet.ResNet50(img_dim, weight_decay=0, include_top=False)\\ngap = keras.layers.GlobalAveragePooling2D()(pretrain_modules.output)\\nout = keras.layers.Dense(units=2, activation=\\'softmax\\', name=\\'output\\')(gap)\\nmodel = keras.models.Model(inputs=[pretrain_modules.input], outputs=[out])\\n\\noptim = keras.optimizers.Adam(lr=FLAGS.lr, amsgrad=True)\\nmodel.compile(loss=\\'categorical_crossentropy\\', \\n              metrics=[\"accuracy\"], \\n              optimizer=optim)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "sys.path.append(\"/home/seanyu/research/Keras-Group-Normalization/\")\n",
    "\n",
    "import resnet_groupnorm as resnet\n",
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "img_dim = (256, 256, 3)\n",
    "pretrain_modules =resnet.ResNet50(img_dim, weight_decay=0, include_top=False)\n",
    "gap = keras.layers.GlobalAveragePooling2D()(pretrain_modules.output)\n",
    "out = keras.layers.Dense(units=2, activation='softmax', name='output')(gap)\n",
    "model = keras.models.Model(inputs=[pretrain_modules.input], outputs=[out])\n",
    "\n",
    "optim = keras.optimizers.Adam(lr=FLAGS.lr, amsgrad=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              metrics=[\"accuracy\"], \n",
    "              optimizer=optim)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 262, 262, 3)  0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 128, 128, 64) 9472        zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gn_conv1 (GroupNorm)            (None, 128, 128, 64) 128         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 64) 0           gn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 64, 64, 64)   4160        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "gn_2a_branch2a (GroupNorm)      (None, 64, 64, 64)   128         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 64)   0           gn_2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 64, 64, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "gn_2a_branch2b (GroupNorm)      (None, 64, 64, 64)   128         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 64)   0           gn_2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 64, 64, 256)  16640       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 64, 64, 256)  16640       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "gn_2a_branch2c (GroupNorm)      (None, 64, 64, 256)  512         res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "gn_2a_branch1 (GroupNorm)       (None, 64, 64, 256)  512         res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64, 64, 256)  0           gn_2a_branch2c[0][0]             \n",
      "                                                                 gn_2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_out (Activation)          (None, 64, 64, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 64, 64, 64)   16448       res2a_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gn_2b_branch2a (GroupNorm)      (None, 64, 64, 64)   128         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 64, 64, 64)   0           gn_2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 64, 64, 64)   36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "gn_2b_branch2b (GroupNorm)      (None, 64, 64, 64)   128         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 64, 64, 64)   0           gn_2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 64, 64, 256)  16640       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "gn_2b_branch2c (GroupNorm)      (None, 64, 64, 256)  512         res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 64, 64, 256)  0           gn_2b_branch2c[0][0]             \n",
      "                                                                 res2a_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res2b_out (Activation)          (None, 64, 64, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 64, 64, 64)   16448       res2b_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gn_2c_branch2a (GroupNorm)      (None, 64, 64, 64)   128         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 64, 64, 64)   0           gn_2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 64, 64, 64)   36928       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "gn_2c_branch2b (GroupNorm)      (None, 64, 64, 64)   128         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 64, 64, 64)   0           gn_2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 64, 64, 256)  16640       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "gn_2c_branch2c (GroupNorm)      (None, 64, 64, 256)  512         res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 64, 64, 256)  0           gn_2c_branch2c[0][0]             \n",
      "                                                                 res2b_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res2c_out (Activation)          (None, 64, 64, 256)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 32, 32, 128)  32896       res2c_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gn_3a_branch2a (GroupNorm)      (None, 32, 32, 128)  256         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 128)  0           gn_3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 32, 32, 128)  147584      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "gn_3a_branch2b (GroupNorm)      (None, 32, 32, 128)  256         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 128)  0           gn_3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 32, 32, 512)  66048       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 32, 32, 512)  131584      res2c_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gn_3a_branch2c (GroupNorm)      (None, 32, 32, 512)  1024        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "gn_3a_branch1 (GroupNorm)       (None, 32, 32, 512)  1024        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 32, 32, 512)  0           gn_3a_branch2c[0][0]             \n",
      "                                                                 gn_3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_out (Activation)          (None, 32, 32, 512)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 32, 32, 128)  65664       res3a_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gn_3b_branch2a (GroupNorm)      (None, 32, 32, 128)  256         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 128)  0           gn_3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 32, 32, 128)  147584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gn_3b_branch2b (GroupNorm)      (None, 32, 32, 128)  256         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 128)  0           gn_3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 32, 32, 512)  66048       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gn_3b_branch2c (GroupNorm)      (None, 32, 32, 512)  1024        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 32, 32, 512)  0           gn_3b_branch2c[0][0]             \n",
      "                                                                 res3a_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res3b_out (Activation)          (None, 32, 32, 512)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 32, 32, 128)  65664       res3b_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gn_3c_branch2a (GroupNorm)      (None, 32, 32, 128)  256         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 128)  0           gn_3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 32, 32, 128)  147584      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gn_3c_branch2b (GroupNorm)      (None, 32, 32, 128)  256         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 128)  0           gn_3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 32, 32, 512)  66048       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gn_3c_branch2c (GroupNorm)      (None, 32, 32, 512)  1024        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 32, 32, 512)  0           gn_3c_branch2c[0][0]             \n",
      "                                                                 res3b_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res3c_out (Activation)          (None, 32, 32, 512)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 32, 32, 128)  65664       res3c_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gn_3d_branch2a (GroupNorm)      (None, 32, 32, 128)  256         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 32, 32, 128)  0           gn_3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 32, 32, 128)  147584      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gn_3d_branch2b (GroupNorm)      (None, 32, 32, 128)  256         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 32, 32, 128)  0           gn_3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 32, 32, 512)  66048       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gn_3d_branch2c (GroupNorm)      (None, 32, 32, 512)  1024        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 32, 32, 512)  0           gn_3d_branch2c[0][0]             \n",
      "                                                                 res3c_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res3d_out (Activation)          (None, 32, 32, 512)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 16, 16, 256)  131328      res3d_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gn_4a_branch2a (GroupNorm)      (None, 16, 16, 256)  512         res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 256)  0           gn_4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gn_4a_branch2b (GroupNorm)      (None, 16, 16, 256)  512         res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 256)  0           gn_4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 16, 16, 1024) 525312      res3d_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gn_4a_branch2c (GroupNorm)      (None, 16, 16, 1024) 2048        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "gn_4a_branch1 (GroupNorm)       (None, 16, 16, 1024) 2048        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 16, 16, 1024) 0           gn_4a_branch2c[0][0]             \n",
      "                                                                 gn_4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_out (Activation)          (None, 16, 16, 1024) 0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 16, 16, 256)  262400      res4a_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gn_4b_branch2a (GroupNorm)      (None, 16, 16, 256)  512         res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 256)  0           gn_4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gn_4b_branch2b (GroupNorm)      (None, 16, 16, 256)  512         res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 16, 16, 256)  0           gn_4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gn_4b_branch2c (GroupNorm)      (None, 16, 16, 1024) 2048        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 16, 16, 1024) 0           gn_4b_branch2c[0][0]             \n",
      "                                                                 res4a_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res4b_out (Activation)          (None, 16, 16, 1024) 0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 16, 16, 256)  262400      res4b_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gn_4c_branch2a (GroupNorm)      (None, 16, 16, 256)  512         res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 16, 16, 256)  0           gn_4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gn_4c_branch2b (GroupNorm)      (None, 16, 16, 256)  512         res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 16, 16, 256)  0           gn_4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gn_4c_branch2c (GroupNorm)      (None, 16, 16, 1024) 2048        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 16, 16, 1024) 0           gn_4c_branch2c[0][0]             \n",
      "                                                                 res4b_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res4c_out (Activation)          (None, 16, 16, 1024) 0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 16, 16, 256)  262400      res4c_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gn_4d_branch2a (GroupNorm)      (None, 16, 16, 256)  512         res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 256)  0           gn_4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gn_4d_branch2b (GroupNorm)      (None, 16, 16, 256)  512         res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 256)  0           gn_4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gn_4d_branch2c (GroupNorm)      (None, 16, 16, 1024) 2048        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 16, 16, 1024) 0           gn_4d_branch2c[0][0]             \n",
      "                                                                 res4c_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res4d_out (Activation)          (None, 16, 16, 1024) 0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 16, 16, 256)  262400      res4d_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gn_4e_branch2a (GroupNorm)      (None, 16, 16, 256)  512         res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 256)  0           gn_4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gn_4e_branch2b (GroupNorm)      (None, 16, 16, 256)  512         res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16, 16, 256)  0           gn_4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gn_4e_branch2c (GroupNorm)      (None, 16, 16, 1024) 2048        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 16, 16, 1024) 0           gn_4e_branch2c[0][0]             \n",
      "                                                                 res4d_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res4e_out (Activation)          (None, 16, 16, 1024) 0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 16, 16, 256)  262400      res4e_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gn_4f_branch2a (GroupNorm)      (None, 16, 16, 256)  512         res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 16, 16, 256)  0           gn_4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gn_4f_branch2b (GroupNorm)      (None, 16, 16, 256)  512         res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 16, 16, 256)  0           gn_4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gn_4f_branch2c (GroupNorm)      (None, 16, 16, 1024) 2048        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 16, 16, 1024) 0           gn_4f_branch2c[0][0]             \n",
      "                                                                 res4e_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res4f_out (Activation)          (None, 16, 16, 1024) 0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 8, 8, 512)    524800      res4f_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gn_5a_branch2a (GroupNorm)      (None, 8, 8, 512)    1024        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 8, 8, 512)    0           gn_5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 8, 8, 512)    2359808     activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gn_5a_branch2b (GroupNorm)      (None, 8, 8, 512)    1024        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 8, 8, 512)    0           gn_5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 8, 8, 2048)   1050624     activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 8, 8, 2048)   2099200     res4f_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gn_5a_branch2c (GroupNorm)      (None, 8, 8, 2048)   4096        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "gn_5a_branch1 (GroupNorm)       (None, 8, 8, 2048)   4096        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 8, 8, 2048)   0           gn_5a_branch2c[0][0]             \n",
      "                                                                 gn_5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_out (Activation)          (None, 8, 8, 2048)   0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 8, 8, 512)    1049088     res5a_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gn_5b_branch2a (GroupNorm)      (None, 8, 8, 512)    1024        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 8, 8, 512)    0           gn_5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 8, 8, 512)    2359808     activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gn_5b_branch2b (GroupNorm)      (None, 8, 8, 512)    1024        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 8, 8, 512)    0           gn_5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 8, 8, 2048)   1050624     activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gn_5b_branch2c (GroupNorm)      (None, 8, 8, 2048)   4096        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 8, 8, 2048)   0           gn_5b_branch2c[0][0]             \n",
      "                                                                 res5a_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res5b_out (Activation)          (None, 8, 8, 2048)   0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 8, 8, 512)    1049088     res5b_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gn_5c_branch2a (GroupNorm)      (None, 8, 8, 512)    1024        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 8, 8, 512)    0           gn_5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 8, 8, 512)    2359808     activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gn_5c_branch2b (GroupNorm)      (None, 8, 8, 512)    1024        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 8, 8, 512)    0           gn_5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 8, 8, 2048)   1050624     activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gn_5c_branch2c (GroupNorm)      (None, 8, 8, 2048)   4096        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 8, 8, 2048)   0           gn_5c_branch2c[0][0]             \n",
      "                                                                 res5b_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res5c_out (Activation)          (None, 8, 8, 2048)   0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           res5c_out[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            4098        avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 23,538,690\n",
      "Trainable params: 23,538,690\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note Area ###\n",
    "#### tf.keras lab-resnet\n",
    "\n",
    "\n",
    "#### tf.keras official-resnet\n",
    "* loss use tf.keras.losses.categorical_crossentropy / tf.keras.losses.binary_crossentropy will FAIL\n",
    "* loss use tf.losses.softmax_cross_entropy\n",
    "* loss use 'categorical_crossentropy' will PASS and SOSO\n",
    "\n",
    "#### keras\n",
    "1. PASS and GOOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workers ready\n"
     ]
    }
   ],
   "source": [
    "train_gen = Customized_dataloader([dog_train, cat_train], \n",
    "                                  batch_size_per_dataset=FLAGS.batch_size//2, \n",
    "                                  num_workers=4, queue_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "cb_list = [tf.keras.callbacks.ReduceLROnPlateau(factor=0.5,\n",
    "                                                patience=4,\n",
    "                                                min_lr=1e-12),\n",
    "           tf.keras.callbacks.EarlyStopping(min_delta = 1e-4, \n",
    "                                            patience= 50)\n",
    "          ]\n",
    "\n",
    "model.fit_generator(train_gen,\n",
    "                    epochs=FLAGS.epochs,\n",
    "                    steps_per_epoch=FLAGS.n_batch, \n",
    "                    validation_data=(x_val, y_val),\n",
    "                    #callbacks=cb_list\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.50964289e-05, 9.99954939e-01],\n",
       "       [2.30941696e-05, 9.99976873e-01],\n",
       "       [7.73848296e-05, 9.99922633e-01],\n",
       "       [4.45182231e-05, 9.99955535e-01],\n",
       "       [8.25511452e-05, 9.99917388e-01],\n",
       "       [4.22269914e-05, 9.99957800e-01],\n",
       "       [6.74393377e-05, 9.99932528e-01],\n",
       "       [2.00847469e-04, 9.99799192e-01],\n",
       "       [7.70057610e-04, 9.99229908e-01],\n",
       "       [3.24789326e-05, 9.99967575e-01],\n",
       "       [2.38693174e-05, 9.99976158e-01],\n",
       "       [7.09431843e-05, 9.99929070e-01],\n",
       "       [1.17315831e-05, 9.99988317e-01],\n",
       "       [2.69481738e-04, 9.99730527e-01],\n",
       "       [1.48543668e-05, 9.99985099e-01],\n",
       "       [1.80779520e-04, 9.99819219e-01],\n",
       "       [1.66606376e-04, 9.99833345e-01],\n",
       "       [6.43297244e-05, 9.99935627e-01],\n",
       "       [1.22542015e-05, 9.99987721e-01],\n",
       "       [1.44993683e-04, 9.99855042e-01],\n",
       "       [1.36246366e-04, 9.99863744e-01],\n",
       "       [2.26747761e-05, 9.99977350e-01],\n",
       "       [1.81648807e-04, 9.99818385e-01],\n",
       "       [2.33482842e-05, 9.99976635e-01],\n",
       "       [1.91981435e-05, 9.99980807e-01],\n",
       "       [1.04859209e-05, 9.99989510e-01],\n",
       "       [4.70311625e-06, 9.99995351e-01],\n",
       "       [2.33056817e-05, 9.99976635e-01],\n",
       "       [8.54892132e-05, 9.99914527e-01],\n",
       "       [7.75258741e-06, 9.99992251e-01],\n",
       "       [4.08416890e-05, 9.99959111e-01],\n",
       "       [6.46053290e-04, 9.99353945e-01],\n",
       "       [2.31633894e-05, 9.99976873e-01],\n",
       "       [2.42474969e-04, 9.99757469e-01],\n",
       "       [5.50231707e-05, 9.99944925e-01],\n",
       "       [1.28716626e-03, 9.98712897e-01],\n",
       "       [4.50964289e-05, 9.99954939e-01],\n",
       "       [2.30941696e-05, 9.99976873e-01],\n",
       "       [7.73848296e-05, 9.99922633e-01],\n",
       "       [4.45182231e-05, 9.99955535e-01],\n",
       "       [8.25511452e-05, 9.99917388e-01],\n",
       "       [4.22269914e-05, 9.99957800e-01],\n",
       "       [6.74393377e-05, 9.99932528e-01],\n",
       "       [2.00847469e-04, 9.99799192e-01],\n",
       "       [7.70057610e-04, 9.99229908e-01],\n",
       "       [3.24789326e-05, 9.99967575e-01],\n",
       "       [2.38693174e-05, 9.99976158e-01],\n",
       "       [7.09431843e-05, 9.99929070e-01],\n",
       "       [1.17315831e-05, 9.99988317e-01],\n",
       "       [2.69481738e-04, 9.99730527e-01],\n",
       "       [1.48543668e-05, 9.99985099e-01],\n",
       "       [1.80779520e-04, 9.99819219e-01],\n",
       "       [1.66606376e-04, 9.99833345e-01],\n",
       "       [6.43297244e-05, 9.99935627e-01],\n",
       "       [1.22542015e-05, 9.99987721e-01],\n",
       "       [1.44993683e-04, 9.99855042e-01],\n",
       "       [1.36246366e-04, 9.99863744e-01],\n",
       "       [2.26747761e-05, 9.99977350e-01],\n",
       "       [1.81648807e-04, 9.99818385e-01],\n",
       "       [2.33482842e-05, 9.99976635e-01],\n",
       "       [1.91981435e-05, 9.99980807e-01],\n",
       "       [1.04859209e-05, 9.99989510e-01],\n",
       "       [4.70311625e-06, 9.99995351e-01],\n",
       "       [2.33056817e-05, 9.99976635e-01],\n",
       "       [8.54892132e-05, 9.99914527e-01],\n",
       "       [7.75258741e-06, 9.99992251e-01],\n",
       "       [4.08416890e-05, 9.99959111e-01],\n",
       "       [6.46053290e-04, 9.99353945e-01],\n",
       "       [2.31633894e-05, 9.99976873e-01],\n",
       "       [2.42474969e-04, 9.99757469e-01],\n",
       "       [5.50231707e-05, 9.99944925e-01],\n",
       "       [1.28716626e-03, 9.98712897e-01],\n",
       "       [6.96493153e-05, 9.99930382e-01],\n",
       "       [1.38127973e-04, 9.99861836e-01],\n",
       "       [1.25121087e-05, 9.99987483e-01],\n",
       "       [6.20494247e-05, 9.99937892e-01],\n",
       "       [7.48213934e-05, 9.99925137e-01],\n",
       "       [4.05633837e-05, 9.99959469e-01],\n",
       "       [7.95790402e-06, 9.99992013e-01],\n",
       "       [4.30906075e-04, 9.99569118e-01],\n",
       "       [2.08438418e-04, 9.99791563e-01],\n",
       "       [3.69781665e-05, 9.99963045e-01],\n",
       "       [5.18464003e-05, 9.99948144e-01],\n",
       "       [1.18503885e-04, 9.99881506e-01],\n",
       "       [2.81259097e-04, 9.99718726e-01],\n",
       "       [4.40023614e-05, 9.99956012e-01],\n",
       "       [3.34244469e-05, 9.99966621e-01],\n",
       "       [1.55177859e-05, 9.99984503e-01],\n",
       "       [4.61617077e-04, 9.99538422e-01],\n",
       "       [1.79317738e-06, 9.99998212e-01],\n",
       "       [6.12670789e-04, 9.99387264e-01],\n",
       "       [5.40294743e-04, 9.99459684e-01],\n",
       "       [1.43702327e-05, 9.99985576e-01],\n",
       "       [1.91467116e-05, 9.99980807e-01],\n",
       "       [3.56150958e-05, 9.99964356e-01],\n",
       "       [3.80493060e-04, 9.99619484e-01],\n",
       "       [3.78703960e-04, 9.99621272e-01],\n",
       "       [8.10676866e-05, 9.99918938e-01],\n",
       "       [2.41051137e-04, 9.99758899e-01],\n",
       "       [6.25280780e-04, 9.99374688e-01],\n",
       "       [1.87537429e-04, 9.99812543e-01],\n",
       "       [4.85926466e-05, 9.99951363e-01],\n",
       "       [1.29175387e-04, 9.99870777e-01],\n",
       "       [1.14008835e-04, 9.99886036e-01],\n",
       "       [3.26798530e-04, 9.99673247e-01],\n",
       "       [2.88255706e-05, 9.99971151e-01],\n",
       "       [9.10291328e-06, 9.99990940e-01],\n",
       "       [6.16907491e-04, 9.99383092e-01],\n",
       "       [6.96493153e-05, 9.99930382e-01],\n",
       "       [1.38127973e-04, 9.99861836e-01],\n",
       "       [1.25121087e-05, 9.99987483e-01],\n",
       "       [6.20494247e-05, 9.99937892e-01],\n",
       "       [7.48213934e-05, 9.99925137e-01],\n",
       "       [4.05633837e-05, 9.99959469e-01],\n",
       "       [7.95790402e-06, 9.99992013e-01],\n",
       "       [4.30906075e-04, 9.99569118e-01],\n",
       "       [2.08438418e-04, 9.99791563e-01],\n",
       "       [3.69781665e-05, 9.99963045e-01],\n",
       "       [5.18464003e-05, 9.99948144e-01],\n",
       "       [1.18503885e-04, 9.99881506e-01],\n",
       "       [2.81259097e-04, 9.99718726e-01],\n",
       "       [4.40023614e-05, 9.99956012e-01],\n",
       "       [3.34244469e-05, 9.99966621e-01],\n",
       "       [1.55177859e-05, 9.99984503e-01],\n",
       "       [4.61617077e-04, 9.99538422e-01],\n",
       "       [1.79317738e-06, 9.99998212e-01],\n",
       "       [6.12670789e-04, 9.99387264e-01],\n",
       "       [5.40294743e-04, 9.99459684e-01],\n",
       "       [1.43702327e-05, 9.99985576e-01],\n",
       "       [1.91467116e-05, 9.99980807e-01],\n",
       "       [3.56150958e-05, 9.99964356e-01],\n",
       "       [3.80493060e-04, 9.99619484e-01],\n",
       "       [3.78703960e-04, 9.99621272e-01],\n",
       "       [8.10676866e-05, 9.99918938e-01],\n",
       "       [2.41051137e-04, 9.99758899e-01],\n",
       "       [6.25280780e-04, 9.99374688e-01],\n",
       "       [1.87537429e-04, 9.99812543e-01],\n",
       "       [4.85926466e-05, 9.99951363e-01],\n",
       "       [1.29175387e-04, 9.99870777e-01],\n",
       "       [1.14008835e-04, 9.99886036e-01],\n",
       "       [3.26798530e-04, 9.99673247e-01],\n",
       "       [2.88255706e-05, 9.99971151e-01],\n",
       "       [9.10291328e-06, 9.99990940e-01],\n",
       "       [6.16907491e-04, 9.99383092e-01],\n",
       "       [2.62621979e-05, 9.99973774e-01],\n",
       "       [2.70369317e-04, 9.99729574e-01],\n",
       "       [2.04390511e-04, 9.99795616e-01],\n",
       "       [3.66072018e-05, 9.99963403e-01],\n",
       "       [4.10925786e-05, 9.99958873e-01],\n",
       "       [3.73750227e-05, 9.99962568e-01],\n",
       "       [1.79370225e-04, 9.99820650e-01],\n",
       "       [7.61295451e-05, 9.99923825e-01],\n",
       "       [1.10967194e-04, 9.99889016e-01],\n",
       "       [4.50728694e-05, 9.99954939e-01],\n",
       "       [7.61148040e-05, 9.99923825e-01],\n",
       "       [9.92825371e-04, 9.99007165e-01],\n",
       "       [1.39161522e-04, 9.99860764e-01],\n",
       "       [1.17614538e-04, 9.99882340e-01],\n",
       "       [2.49866189e-05, 9.99974966e-01],\n",
       "       [3.61123712e-05, 9.99963880e-01],\n",
       "       [6.32660522e-05, 9.99936700e-01],\n",
       "       [3.33906559e-04, 9.99666095e-01],\n",
       "       [8.25178504e-05, 9.99917507e-01],\n",
       "       [3.29812960e-04, 9.99670148e-01],\n",
       "       [4.19863454e-06, 9.99995828e-01],\n",
       "       [1.21722704e-04, 9.99878287e-01],\n",
       "       [1.58004204e-05, 9.99984145e-01],\n",
       "       [8.56183295e-04, 9.99143839e-01],\n",
       "       [2.99048625e-05, 9.99970078e-01],\n",
       "       [3.36600078e-06, 9.99996662e-01],\n",
       "       [1.53180445e-05, 9.99984741e-01],\n",
       "       [8.87708957e-05, 9.99911189e-01],\n",
       "       [1.06510190e-04, 9.99893427e-01],\n",
       "       [5.57409476e-05, 9.99944210e-01],\n",
       "       [4.61488235e-05, 9.99953866e-01],\n",
       "       [1.50030273e-05, 9.99984980e-01],\n",
       "       [2.00897048e-05, 9.99979854e-01],\n",
       "       [2.30860660e-05, 9.99976873e-01],\n",
       "       [1.18615399e-05, 9.99988079e-01],\n",
       "       [7.51135012e-05, 9.99924898e-01],\n",
       "       [2.62621979e-05, 9.99973774e-01],\n",
       "       [2.70369317e-04, 9.99729574e-01],\n",
       "       [2.04390511e-04, 9.99795616e-01],\n",
       "       [3.66072018e-05, 9.99963403e-01],\n",
       "       [4.10925786e-05, 9.99958873e-01],\n",
       "       [3.73750227e-05, 9.99962568e-01],\n",
       "       [1.79370225e-04, 9.99820650e-01],\n",
       "       [7.61295451e-05, 9.99923825e-01],\n",
       "       [1.10967194e-04, 9.99889016e-01],\n",
       "       [4.50728694e-05, 9.99954939e-01],\n",
       "       [7.61148040e-05, 9.99923825e-01],\n",
       "       [9.92825371e-04, 9.99007165e-01],\n",
       "       [1.39161522e-04, 9.99860764e-01],\n",
       "       [1.17614538e-04, 9.99882340e-01],\n",
       "       [2.49866189e-05, 9.99974966e-01],\n",
       "       [3.61123712e-05, 9.99963880e-01],\n",
       "       [6.32660522e-05, 9.99936700e-01],\n",
       "       [3.33906559e-04, 9.99666095e-01],\n",
       "       [8.25178504e-05, 9.99917507e-01],\n",
       "       [3.29812960e-04, 9.99670148e-01],\n",
       "       [4.19863454e-06, 9.99995828e-01],\n",
       "       [1.21722704e-04, 9.99878287e-01],\n",
       "       [1.58004204e-05, 9.99984145e-01],\n",
       "       [8.56183295e-04, 9.99143839e-01],\n",
       "       [2.99048625e-05, 9.99970078e-01],\n",
       "       [3.36600078e-06, 9.99996662e-01],\n",
       "       [1.53180445e-05, 9.99984741e-01],\n",
       "       [8.87708957e-05, 9.99911189e-01],\n",
       "       [1.06510190e-04, 9.99893427e-01],\n",
       "       [5.57409476e-05, 9.99944210e-01],\n",
       "       [4.61488235e-05, 9.99953866e-01],\n",
       "       [1.50030273e-05, 9.99984980e-01],\n",
       "       [2.00897048e-05, 9.99979854e-01],\n",
       "       [2.30860660e-05, 9.99976873e-01],\n",
       "       [1.18615399e-05, 9.99988079e-01],\n",
       "       [7.51135012e-05, 9.99924898e-01],\n",
       "       [6.42141895e-05, 9.99935746e-01],\n",
       "       [3.08447015e-05, 9.99969125e-01],\n",
       "       [5.35537001e-05, 9.99946475e-01],\n",
       "       [1.15168823e-05, 9.99988437e-01],\n",
       "       [2.75025704e-05, 9.99972463e-01],\n",
       "       [2.36737214e-05, 9.99976277e-01],\n",
       "       [1.59619885e-04, 9.99840379e-01],\n",
       "       [6.11920477e-05, 9.99938846e-01],\n",
       "       [2.21715472e-05, 9.99977827e-01],\n",
       "       [2.17010340e-04, 9.99782979e-01],\n",
       "       [3.02164524e-04, 9.99697804e-01],\n",
       "       [6.41942897e-05, 9.99935746e-01],\n",
       "       [6.75834817e-05, 9.99932408e-01],\n",
       "       [2.53538392e-03, 9.97464657e-01],\n",
       "       [2.20979608e-04, 9.99779046e-01],\n",
       "       [6.84842307e-05, 9.99931455e-01],\n",
       "       [2.35601201e-05, 9.99976397e-01],\n",
       "       [7.70417173e-05, 9.99922991e-01],\n",
       "       [2.83949630e-04, 9.99715984e-01],\n",
       "       [3.87600739e-05, 9.99961257e-01],\n",
       "       [2.49754032e-03, 9.97502506e-01],\n",
       "       [2.12639261e-05, 9.99978781e-01],\n",
       "       [1.16226925e-04, 9.99883771e-01],\n",
       "       [1.30971079e-04, 9.99868989e-01],\n",
       "       [3.41083396e-05, 9.99965906e-01],\n",
       "       [3.17847957e-06, 9.99996781e-01],\n",
       "       [4.73605287e-05, 9.99952674e-01],\n",
       "       [2.63131624e-05, 9.99973655e-01],\n",
       "       [5.09733181e-05, 9.99948978e-01],\n",
       "       [3.77775505e-05, 9.99962211e-01],\n",
       "       [3.35800723e-05, 9.99966383e-01],\n",
       "       [4.78555703e-05, 9.99952197e-01],\n",
       "       [9.94510119e-05, 9.99900579e-01],\n",
       "       [2.63346774e-05, 9.99973655e-01],\n",
       "       [6.78595607e-05, 9.99932170e-01],\n",
       "       [5.34911196e-05, 9.99946475e-01],\n",
       "       [6.42141895e-05, 9.99935746e-01],\n",
       "       [3.08447015e-05, 9.99969125e-01],\n",
       "       [5.35537001e-05, 9.99946475e-01],\n",
       "       [1.15168823e-05, 9.99988437e-01],\n",
       "       [2.75025704e-05, 9.99972463e-01],\n",
       "       [2.36737214e-05, 9.99976277e-01],\n",
       "       [1.59619885e-04, 9.99840379e-01],\n",
       "       [6.11920477e-05, 9.99938846e-01],\n",
       "       [2.21715472e-05, 9.99977827e-01],\n",
       "       [2.17010340e-04, 9.99782979e-01],\n",
       "       [3.02164524e-04, 9.99697804e-01],\n",
       "       [6.41942897e-05, 9.99935746e-01],\n",
       "       [6.75834817e-05, 9.99932408e-01],\n",
       "       [2.53538392e-03, 9.97464657e-01],\n",
       "       [2.20979608e-04, 9.99779046e-01],\n",
       "       [6.84842307e-05, 9.99931455e-01],\n",
       "       [2.35601201e-05, 9.99976397e-01],\n",
       "       [7.70417173e-05, 9.99922991e-01],\n",
       "       [2.83949630e-04, 9.99715984e-01],\n",
       "       [3.87600739e-05, 9.99961257e-01],\n",
       "       [2.49754032e-03, 9.97502506e-01],\n",
       "       [2.12639261e-05, 9.99978781e-01],\n",
       "       [1.16226925e-04, 9.99883771e-01],\n",
       "       [1.30971079e-04, 9.99868989e-01],\n",
       "       [3.41083396e-05, 9.99965906e-01],\n",
       "       [3.17847957e-06, 9.99996781e-01],\n",
       "       [4.73605287e-05, 9.99952674e-01],\n",
       "       [2.63131624e-05, 9.99973655e-01],\n",
       "       [5.09733181e-05, 9.99948978e-01],\n",
       "       [3.77775505e-05, 9.99962211e-01],\n",
       "       [3.35800723e-05, 9.99966383e-01],\n",
       "       [4.78555703e-05, 9.99952197e-01],\n",
       "       [9.94510119e-05, 9.99900579e-01],\n",
       "       [2.63346774e-05, 9.99973655e-01],\n",
       "       [6.78595607e-05, 9.99932170e-01],\n",
       "       [5.34911196e-05, 9.99946475e-01],\n",
       "       [8.21527647e-06, 9.99991775e-01],\n",
       "       [1.80081726e-04, 9.99819934e-01],\n",
       "       [2.69941006e-06, 9.99997258e-01],\n",
       "       [6.15765166e-04, 9.99384165e-01],\n",
       "       [2.92980385e-05, 9.99970675e-01],\n",
       "       [2.69316311e-04, 9.99730647e-01],\n",
       "       [1.82965978e-05, 9.99981761e-01],\n",
       "       [2.45098327e-03, 9.97548997e-01],\n",
       "       [2.12606345e-03, 9.97873902e-01],\n",
       "       [3.24870166e-06, 9.99996781e-01],\n",
       "       [4.52531567e-05, 9.99954700e-01],\n",
       "       [2.92163109e-04, 9.99707758e-01],\n",
       "       [3.83695442e-05, 9.99961615e-01],\n",
       "       [9.41779763e-06, 9.99990582e-01],\n",
       "       [1.35677110e-04, 9.99864340e-01],\n",
       "       [1.61241088e-03, 9.98387575e-01],\n",
       "       [6.67934873e-05, 9.99933243e-01],\n",
       "       [5.50645345e-05, 9.99944925e-01],\n",
       "       [9.95363553e-06, 9.99990106e-01],\n",
       "       [1.01908641e-04, 9.99898076e-01],\n",
       "       [8.38074266e-06, 9.99991655e-01],\n",
       "       [2.86063823e-05, 9.99971390e-01],\n",
       "       [3.78296827e-05, 9.99962211e-01],\n",
       "       [3.42206302e-04, 9.99657750e-01],\n",
       "       [4.25216684e-04, 9.99574840e-01],\n",
       "       [2.11703045e-05, 9.99978781e-01],\n",
       "       [3.66953500e-05, 9.99963284e-01],\n",
       "       [1.64705398e-06, 9.99998331e-01],\n",
       "       [3.05346548e-05, 9.99969482e-01],\n",
       "       [4.11844230e-05, 9.99958873e-01],\n",
       "       [5.11414371e-04, 9.99488592e-01],\n",
       "       [4.92187064e-06, 9.99995112e-01],\n",
       "       [5.92830591e-04, 9.99407172e-01],\n",
       "       [1.54042995e-04, 9.99845982e-01],\n",
       "       [2.23707648e-05, 9.99977589e-01],\n",
       "       [3.89671368e-05, 9.99961019e-01],\n",
       "       [8.21527647e-06, 9.99991775e-01],\n",
       "       [1.80081726e-04, 9.99819934e-01],\n",
       "       [2.69941006e-06, 9.99997258e-01],\n",
       "       [6.15765166e-04, 9.99384165e-01],\n",
       "       [2.92980385e-05, 9.99970675e-01],\n",
       "       [2.69316311e-04, 9.99730647e-01],\n",
       "       [1.82965978e-05, 9.99981761e-01],\n",
       "       [2.45098327e-03, 9.97548997e-01],\n",
       "       [2.12606345e-03, 9.97873902e-01],\n",
       "       [3.24870166e-06, 9.99996781e-01],\n",
       "       [4.52531567e-05, 9.99954700e-01],\n",
       "       [2.92163109e-04, 9.99707758e-01],\n",
       "       [3.83695442e-05, 9.99961615e-01],\n",
       "       [9.41779763e-06, 9.99990582e-01],\n",
       "       [1.35677110e-04, 9.99864340e-01],\n",
       "       [1.61241088e-03, 9.98387575e-01],\n",
       "       [6.67934873e-05, 9.99933243e-01],\n",
       "       [5.50645345e-05, 9.99944925e-01],\n",
       "       [9.95363553e-06, 9.99990106e-01],\n",
       "       [1.01908641e-04, 9.99898076e-01],\n",
       "       [8.38074266e-06, 9.99991655e-01],\n",
       "       [2.86063823e-05, 9.99971390e-01],\n",
       "       [3.78296827e-05, 9.99962211e-01],\n",
       "       [3.42206302e-04, 9.99657750e-01],\n",
       "       [4.25216684e-04, 9.99574840e-01],\n",
       "       [2.11703045e-05, 9.99978781e-01],\n",
       "       [3.66953500e-05, 9.99963284e-01],\n",
       "       [1.64705398e-06, 9.99998331e-01],\n",
       "       [3.05347094e-05, 9.99969482e-01],\n",
       "       [4.11845031e-05, 9.99958873e-01],\n",
       "       [5.11415361e-04, 9.99488592e-01],\n",
       "       [4.92188019e-06, 9.99995112e-01],\n",
       "       [5.92831406e-04, 9.99407172e-01],\n",
       "       [1.54043300e-04, 9.99845982e-01],\n",
       "       [2.23707866e-05, 9.99977589e-01],\n",
       "       [3.89671732e-05, 9.99961019e-01]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {
    "6b1438b3075f49289cfcf03a4fce2ccb": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "ab7848b490e5454e9a42f439a6ef6b31": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "e0e3b0d66b1e47c4ade6c276bacc5c55": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
